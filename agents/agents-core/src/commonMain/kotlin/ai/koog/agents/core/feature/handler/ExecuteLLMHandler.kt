@file:OptIn(ExperimentalUuidApi::class)

package ai.koog.agents.core.feature.handler

import ai.koog.agents.core.tools.ToolDescriptor
import ai.koog.prompt.dsl.Prompt
import ai.koog.prompt.llm.LLModel
import ai.koog.prompt.message.Message
import kotlin.uuid.ExperimentalUuidApi
import kotlin.uuid.Uuid

/**
 * A handler responsible for managing the execution flow of a Large Language Model (LLM) call.
 * It allows customization of logic to be executed before and after the LLM is called.
 */
public class ExecuteLLMHandler {

    /**
     * A handler that is invoked before making a call to the Language Learning Model (LLM).
     *
     * This handler enables customization or preprocessing steps to be applied prior to querying the model.
     * It accepts the prompt, a list of tools, the model, and a session UUID as inputs, allowing
     * users to define specific logic or modifications to these inputs before the call is made.
     */
    public var beforeLLMCallHandler: BeforeLLMCallHandler =
        BeforeLLMCallHandler { prompt, tools, model, sessionUuid -> }

    /**
     * A handler that is invoked after a call to a language model (LLM) is executed.
     *
     * This variable represents a custom implementation of the `AfterLLMCallHandler` functional interface,
     * allowing post-processing or custom logic to be performed once the LLM has returned a response.
     *
     * The handler receives various pieces of information about the LLM call, including the original prompt,
     * the tools used, the model invoked, the responses returned by the model, and a unique session identifier.
     *
     * Customize this handler to implement specific behavior required immediately after LLM processing.
     */
    public var afterLLMCallHandler: AfterLLMCallHandler =
        AfterLLMCallHandler { prompt, tools, model, response, sessionUuid -> }

    public var
}

/**
 * A functional interface implemented to handle logic that occurs before invoking a large language model (LLM).
 * It allows preprocessing steps or validation based on the provided prompt, available tools, targeted LLM model,
 * and a unique session identifier.
 *
 * This can be particularly useful for custom input manipulation, logging, validation, or applying
 * configurations to the LLM request based on external context.
 */
public fun interface BeforeLLMCallHandler {
    /**
     * Handles a language model interaction by processing the given prompt, tools, model, and session identifier.
     *
     * @param sessionId The unique identifier for the session to track the interaction.
     * @param prompt The prompt containing messages, parameters, and a unique identifier for the interaction.
     * @param tools The list of tool descriptors available for the language model to utilize during the interaction.
     * @param model The language model instance used to process the prompt and generate responses.
     */
    public suspend fun handle(sessionId: String, prompt: Prompt, tools: List<ToolDescriptor>, model: LLModel)
}

/**
 * Represents a functional interface for handling operations or logic that should occur after a call
 * to a large language model (LLM) is made. The implementation of this interface provides a mechanism
 * to perform custom logic or processing based on the provided inputs, such as the prompt, tools,
 * model, and generated responses.
 */
public fun interface AfterLLMCallHandler {
    /**
     * Handles the post-processing of a prompt and its associated data after a language model call.
     *
     * @param prompt The prompt containing a list of messages and related language model settings.
     * @param tools A list of tool descriptors specifying the tools available for use.
     * @param model The language learning model that was used for generating responses.
     * @param responses A list of response messages generated by the language model.
     * @param sessionUuid The unique identifier for the session during which the processing takes place.
     */
    public suspend fun handle(sessionId: String, prompt: Prompt, tools: List<ToolDescriptor>, model: LLModel, responses: List<Message.Response>)
}
