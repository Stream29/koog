package ai.koog.agents.core.feature

import ai.koog.agents.core.tools.ToolDescriptor
import ai.koog.prompt.dsl.Prompt
import ai.koog.prompt.executor.model.LLMChoice
import ai.koog.prompt.executor.model.PromptExecutor
import ai.koog.prompt.llm.LLModel
import ai.koog.prompt.message.Message
import io.github.oshai.kotlinlogging.KotlinLogging
import kotlinx.coroutines.flow.Flow

/**
 * A wrapper around [ai.koog.prompt.executor.model.PromptExecutor] that allows for adding internal functionality to the executor
 * to catch and log events related to LLM calls.
 *
 * @property executor The [ai.koog.prompt.executor.model.PromptExecutor] to wrap.
 * @property pipeline The [AIAgentPipeline] associated with the executor.
 */
public class PromptExecutorProxy(
    private val executor: PromptExecutor,
    private val pipeline: AIAgentPipeline,
    private val sessionId: String,
) : PromptExecutor {

    private companion object {
        private val logger = KotlinLogging.logger {  }
    }

    override suspend fun execute(prompt: Prompt, model: LLModel, tools: List<ToolDescriptor>): List<Message.Response> {
        logger.debug { "Executing LLM call (prompt: $prompt, tools: [${tools.joinToString { it.name }}])" }
        pipeline.onBeforeLLMCall(sessionId, prompt, model, tools)

        val responses = executor.execute(prompt, model, tools)

        logger.debug { "Finished LLM call with responses: [${responses.joinToString { "${it.role}: ${it.content}" } }]" }
        pipeline.onAfterLLMCall(sessionId, prompt, model, tools, responses)

        return responses
    }

    override suspend fun executeStreaming(prompt: Prompt, model: LLModel): Flow<String> {
        logger.debug { "Executing LLM streaming call (prompt: $prompt)" }
        val stream = executor.executeStreaming(prompt, model)

        return stream
    }

    override suspend fun executeMultipleChoices(prompt: Prompt, model: LLModel, tools: List<ToolDescriptor>): List<LLMChoice> {
        logger.debug { "Executing LLM call prompt: $prompt with tools: [${tools.joinToString { it.name }}]" }

        val responses = executor.executeMultipleChoices(prompt, model, tools)

        val messageBuilder = StringBuilder()
            .appendLine("Finished LLM call with LLM Choice response:")

        responses.forEachIndexed { index, response ->
            messageBuilder.appendLine("- Response #${index}")
            response.forEach { message ->
                messageBuilder.appendLine("  -- [${message.role}] ${message.content}")
            }
        }

        logger.debug { "Finished LLM call with responses: $messageBuilder" }

        return responses
    }
}
