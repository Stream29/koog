{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Koog is a Kotlin-based framework designed to build and run AI agents entirely in idiomatic Kotlin. It lets you create agents that can interact with tools, handle complex workflows, and communicate with users.</p> <p>The framework supports the following types of agents:</p> <ul> <li>Single-run agents with minimal configuration that process a single input and provide a response.   An agent of this type operates within a single cycle of tool-calling to complete its task and provide a response.</li> <li>Complex workflow agents with advanced capabilities that support custom strategies and configurations.</li> </ul>"},{"location":"#key-features","title":"Key features","text":"<p>Key features of Koog include:</p> <ul> <li>Pure Kotlin implementation: Build AI agents entirely in natural and idiomatic Kotlin.</li> <li>MCP integration: Connect to Model Control Protocol for enhanced model management.</li> <li>Embedding capabilities: Use vector embeddings for semantic search and knowledge retrieval.</li> <li>Custom tool creation: Extend your agents with tools that access external systems and APIs.</li> <li>Ready-to-use components: Speed up development with pre-built solutions for common AI engineering challenges.</li> <li>Intelligent history compression: Optimize token usage while maintaining conversation context using various pre-built strategies.</li> <li>Powerful Streaming API: Process responses in real-time with streaming support and parallel tool calls.</li> <li>Persistent agent memory: Enable knowledge retention across sessions and even different agents.</li> <li>Comprehensive tracing: Debug and monitor agent execution with detailed and configurable tracing.</li> <li>Flexible graph workflows: Design complex agent behaviors using intuitive graph-based workflows.</li> <li>Modular feature system: Customize agent capabilities through a composable architecture.</li> <li>Scalable architecture: Handle workloads from simple chatbots to enterprise applications.</li> <li>Multiplatform: Run agents on JVM, JS, WasmJS targets with Kotlin Multiplatform.</li> </ul>"},{"location":"#available-llm-providers-and-platforms","title":"Available LLM providers and platforms","text":"<p>The LLM providers and platforms whose LLMs you can use to power your agent capabilities:</p> <ul> <li>Google</li> <li>OpenAI</li> <li>Anthropic</li> <li>OpenRouter</li> <li>Ollama</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>To use Koog, you need to include all necessary dependencies in your build configuration.</p>"},{"location":"#gradle","title":"Gradle","text":""},{"location":"#gradle-kotlin-dsl","title":"Gradle (Kotlin DSL)","text":"<ol> <li> <p>Add dependencies to the <code>build.gradle.kts</code> file:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:LATEST_VERSION\")\n}\n</code></pre> </li> <li> <p>Make sure that you have <code>mavenCentral()</code> in the list of repositories.</p> </li> </ol>"},{"location":"#gradle-groovy","title":"Gradle (Groovy)","text":"<ol> <li> <p>Add dependencies to the <code>build.gradle</code> file:</p> <pre><code>dependencies {\n    implementation 'ai.koog:koog-agents:LATEST_VERSION'\n}\n</code></pre> </li> <li> <p>Make sure that you have <code>mavenCentral()</code> in the list of repositories.</p> </li> </ol>"},{"location":"#maven","title":"Maven","text":"<ol> <li> <p>Add dependencies to the <code>pom.xml</code> file:</p> <pre><code>&lt;dependency&gt;\n    &lt;groupId&gt;ai.koog&lt;/groupId&gt;\n    &lt;artifactId&gt;koog-agents-jvm&lt;/artifactId&gt;\n    &lt;version&gt;LATEST_VERSION&lt;/version&gt;\n&lt;/dependency&gt;\n</code></pre> </li> <li> <p>Make sure that you have <code>mavenCentral</code> in the list of repositories.</p> </li> </ol>"},{"location":"#quickstart-example","title":"Quickstart example","text":"<p>To help you get started with AI agents, here is a quick example of a single-run agent:</p> <p>Note</p> <p>Before you run the example, assign a corresponding API key as an environment variable. For details, see Getting started.</p> <pre><code>fun main() {\n    runBlocking {\n        val apiKey = System.getenv(\"OPENAI_API_KEY\") // or Anthropic, Google, OpenRouter, etc.\n\n        val agent = AIAgent(\n            executor = simpleOpenAIExecutor(apiKey), // or Anthropic, Google, OpenRouter, etc.\n            systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n            llmModel = OpenAIModels.Chat.GPT4o\n        )\n\n        val result = agent.run(\"Hello! How can you help me?\")\n        println(result)\n    }\n}\n</code></pre> <p>For more details, see Getting started.</p>"},{"location":"agent-events/","title":"Agent events","text":"<p>Agent events are actions or interactions that occur as part of an agent workflow. They include:</p> <ul> <li>Agent lifecycle events</li> <li>Strategy events</li> <li>Node events</li> <li>LLM call events</li> <li>Tool call events</li> </ul>"},{"location":"agent-events/#event-handlers","title":"Event handlers","text":"<p>You can monitor and respond to specific events during the agent workflow by using event handlers for logging, testing, debugging, and extending agent behavior.</p> <p>The EventHandler feature lets you hook into various agent events. It serves as an event delegation mechanism that:</p> <ul> <li>Manages the lifecycle of AI agent operations.</li> <li>Provides hooks for monitoring and responding to different stages of the workflow.</li> <li>Enables error handling and recovery.</li> <li>Facilitates tool invocation tracking and result processing.</li> </ul>"},{"location":"agent-events/#installation-and-configuration","title":"Installation and configuration","text":"<p>The EventHandler feature integrates with the agent workflow through the <code>EventHandler</code> class, which provides a way to register callbacks for different agent events, and can be installed as a feature in the agent configuration. For details, see API reference.</p> <p>To install the feature and configure event handlers for the agent, do the following:</p> <pre><code>handleEvents {\n    // Handle tool calls\n    onToolCall { eventContext -&gt;\n        println(\"Tool called: ${eventContext.tool} with args ${eventContext.toolArgs}\")\n    }\n    // Handle event triggered when the agent completes its execution\n    onAgentFinished { eventContext -&gt;\n        println(\"Agent finished with result: ${eventContext.result}\")\n    }\n\n    // Other event handlers\n}\n</code></pre> <p>For more details about event handler configuration, see API reference.</p> <p>You can also set up event handlers using the <code>handleEvents</code> extension function when creating an agent. This function also installs the event handler feature and configures event handlers for the agent. Here is an example:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n){\n    handleEvents {\n        // Handle tool calls\n        onToolCall { eventContext -&gt;\n            println(\"Tool called: ${eventContext.tool} with args ${eventContext.toolArgs}\")\n        }\n        // Handle event triggered when the agent completes its execution\n        onAgentFinished { eventContext -&gt;\n            println(\"Agent finished with result: ${eventContext.result}\")\n        }\n\n        // Other event handlers\n    }\n}\n</code></pre>"},{"location":"agent-memory/","title":"Memory","text":""},{"location":"agent-memory/#feature-overview","title":"Feature overview","text":"<p>The AgentMemory feature is a component of the Koog framework that lets AI agents store, retrieve, and use information across conversations.</p>"},{"location":"agent-memory/#purpose","title":"Purpose","text":"<p>The AgentMemory Feature addresses the challenge of maintaining context in AI agent interactions by:</p> <ul> <li>Storing important facts extracted from conversations.</li> <li>Organizing information by concepts, subjects, and scopes.</li> <li>Retrieving relevant information when needed in future interactions.</li> <li>Enabling personalization based on user preferences and history.</li> </ul>"},{"location":"agent-memory/#architecture","title":"Architecture","text":"<p>The AgentMemory feature is built on a hierarchical structure. The elements of the structure are listed and explained in the sections below.</p>"},{"location":"agent-memory/#facts","title":"Facts","text":"<p>Facts are individual pieces of information stored in the memory.  Facts represent actual stored information. There are two types of facts:</p> <ul> <li>SingleFact: a single value associated with a concept. For example, an IDE user's current preferred theme:</li> </ul> <pre><code>// Storing favorite IDE theme (single value)\nval themeFact = SingleFact(\n    concept = Concept(\n        \"ide-theme\", \n        \"User's preferred IDE theme\", \n        factType = FactType.SINGLE),\n    value = \"Dark Theme\",\n    timestamp = DefaultTimeProvider.getCurrentTimestamp()\n)\n</code></pre> <ul> <li>MultipleFacts: multiple values associated with a concept. For example, all languages that a user knows:</li> </ul> <pre><code>// Storing programming languages (multiple values)\nval languagesFact = MultipleFacts(\n    concept = Concept(\n        \"programming-languages\",\n        \"Languages the user knows\",\n        factType = FactType.MULTIPLE\n    ),\n    values = listOf(\"Kotlin\", \"Java\", \"Python\"),\n    timestamp = DefaultTimeProvider.getCurrentTimestamp()\n)\n</code></pre>"},{"location":"agent-memory/#concepts","title":"Concepts","text":"<p>Concepts are categories of information with associated metadata.</p> <ul> <li>Keyword: unique identifier for the concept.</li> <li>Description: detailed explanation of what the concept represents.</li> <li>FactType: whether the concept stores single or multiple facts (<code>FactType.SINGLE</code> or <code>FactType.MULTIPLE</code>).</li> </ul>"},{"location":"agent-memory/#subjects","title":"Subjects","text":"<p>Subjects are entities that facts can be associated with.</p> <p>Common examples of subjects include:</p> <ul> <li>User: Personal preferences and settings</li> <li>Environment: Information related to the environment of the application</li> </ul> <p>There is a predefined <code>MemorySubject.Everything</code> that you may use as a default subject for all facts. In addition, you can define your own custom memory subjects by extending the <code>MemorySubject</code> abstract class:</p> <pre><code>object MemorySubjects {\n    /**\n     * Information specific to the local machine environment\n     * Examples: Installed tools, SDKs, OS configuration, available commands\n     */\n    @Serializable\n    data object Machine : MemorySubject() {\n        override val name: String = \"machine\"\n        override val promptDescription: String =\n            \"Technical environment (installed tools, package managers, packages, SDKs, OS, etc.)\"\n        override val priorityLevel: Int = 1\n    }\n\n    /**\n     * Information specific to the user\n     * Examples: Conversation preferences, issue history, contact information\n     */\n    @Serializable\n    data object User : MemorySubject() {\n        override val name: String = \"user\"\n        override val promptDescription: String =\n            \"User information (conversation preferences, issue history, contact details, etc.)\"\n        override val priorityLevel: Int = 1\n    }\n}\n</code></pre>"},{"location":"agent-memory/#scopes","title":"Scopes","text":"<p>Memory scopes are contexts in which facts are relevant:</p> <ul> <li>Agent: specific to an agent.</li> <li>Feature: specific to a feature.</li> <li>Product: specific to a product.</li> <li>CrossProduct: relevant across multiple products.</li> </ul>"},{"location":"agent-memory/#configuration-and-initialization","title":"Configuration and initialization","text":"<p>The feature integrates with the agent pipeline through the <code>AgentMemory</code> class, which provides methods for saving and loading facts, and can be installed as a feature in the agent configuration.</p>"},{"location":"agent-memory/#configuration","title":"Configuration","text":"<p>The <code>AgentMemory.Config</code> class is the configuration class for the AgentMemory feature.</p> <pre><code>class Config(\n    var memoryProvider: AgentMemoryProvider = NoMemory,\n    var scopesProfile: MemoryScopesProfile = MemoryScopesProfile(),\n\n    var agentName: String,\n    var featureName: String,\n    var organizationName: String,\n    var productName: String\n) : FeatureConfig()\n</code></pre>"},{"location":"agent-memory/#installation","title":"Installation","text":"<p>To install the AgentMemory feature in an agent, follow the pattern provided in the code sample below.</p> <pre><code>val agent = AIAgent(\n    executor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    install(AgentMemory) {\n        memoryProvider = memoryProvider\n        agentName = \"your-agent-name\"\n        featureName = \"your-feature-name\"\n        organizationName = \"your-organization-name\"\n        productName = \"your-product-name\"\n    }\n}\n</code></pre>"},{"location":"agent-memory/#examples-and-quickstarts","title":"Examples and quickstarts","text":""},{"location":"agent-memory/#basic-usage","title":"Basic usage","text":"<p>The following code snippets demonstrate the basic setup of a memory storage and how facts are saved to and loaded from the memory.</p> <ol> <li>Set up memory storage</li> </ol> <pre><code>// Create a memory provider\nval memoryProvider = LocalFileMemoryProvider(\n    config = LocalMemoryConfig(\"customer-support-memory\"),\n    storage = SimpleStorage(JVMFileSystemProvider.ReadWrite),\n    fs = JVMFileSystemProvider.ReadWrite,\n    root = Path(\"path/to/memory/root\")\n)\n</code></pre> <ol> <li>Store a fact in the memory</li> </ol> <pre><code>memoryProvider.save(\n    fact = SingleFact(\n        concept = Concept(\"greeting\", \"User's name\", FactType.SINGLE),\n        value = \"John\",\n        timestamp = DefaultTimeProvider.getCurrentTimestamp()\n    ),\n    subject = MemorySubjects.User,\n    scope = MemoryScope.Product(\"my-app\"),\n)\n</code></pre> <ol> <li>Retrieve the fact</li> </ol> <pre><code>// Get the stored information\nval greeting = memoryProvider.load(\n    concept = Concept(\"greeting\", \"User's name\", FactType.SINGLE),\n    subject = MemorySubjects.User,\n    scope = MemoryScope.Product(\"my-app\")\n)\nif (greeting.size &gt; 1) {\n    println(\"Memories found: ${greeting.joinToString(\", \")}\")\n} else {\n    println(\"Information not found. First time here?\")\n}\n</code></pre>"},{"location":"agent-memory/#using-memory-nodes","title":"Using memory nodes","text":"<p>The AgentMemory feature provides the following predefined memory nodes that can be used in agent strategies:</p> <ul> <li>nodeLoadAllFactsFromMemory: loads all facts about the subject from the memory for a given concept.</li> <li>nodeLoadFromMemory: loads specific facts from the memory for a given concept.</li> <li>nodeSaveToMemory: saves a fact to the memory.</li> <li>nodeSaveToMemoryAutoDetectFacts: automatically detects and extracts facts from the chat history and saves them to the memory. Uses the LLM to identify concepts.</li> </ul> <p>Here is an example of how nodes can be implemented in an agent strategy:</p> <pre><code>val strategy = strategy(\"example-agent\") {\n    // Node to automatically detect and save facts\n    val detectFacts by nodeSaveToMemoryAutoDetectFacts&lt;Unit&gt;(\n        subjects = listOf(MemorySubjects.User, MemorySubjects.Machine)\n    )\n\n    // Node to load specific facts\n    val loadPreferences by node&lt;Unit, Unit&gt; {\n        withMemory {\n            loadFactsToAgent(\n                concept = Concept(\"user-preference\", \"User's preferred programming language\", FactType.SINGLE),\n                subjects = listOf(MemorySubjects.User)\n            )\n        }\n    }\n\n    // Connect nodes in the strategy\n    edge(nodeStart forwardTo detectFacts)\n    edge(detectFacts forwardTo loadPreferences)\n    edge(loadPreferences forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"agent-memory/#making-memory-secure","title":"Making memory secure","text":"<p>You can use encryption to make sure that sensitive information is protected inside an encrypted storage used by the memory provider.</p> <pre><code>// Simple encrypted storage setup\nval secureStorage = EncryptedStorage(\n    fs = JVMFileSystemProvider.ReadWrite,\n    encryption = Aes256GCMEncryptor(\"your-secret-key\")\n)\n</code></pre>"},{"location":"agent-memory/#example-remembering-user-preferences","title":"Example: Remembering user preferences","text":"<p>Here is an example of how AgentMemory is used in a real-world scenario to remember a user's preference, specifically the user's favorite programming language.</p> <pre><code>memoryProvider.save(\n    fact = SingleFact(\n        concept = Concept(\"preferred-language\", \"What programming language is preferred by the user?\", FactType.SINGLE),\n        value = \"Kotlin\",\n        timestamp = DefaultTimeProvider.getCurrentTimestamp()\n    ),\n    subject = MemorySubjects.User,\n    scope = MemoryScope.Product(\"my-app\")\n)\n</code></pre>"},{"location":"agent-memory/#advanced-usage","title":"Advanced usage","text":""},{"location":"agent-memory/#custom-nodes-with-memory","title":"Custom nodes with memory","text":"<p>You can also use the memory from the <code>withMemory</code> clause inside any node. The ready-to-use <code>loadFactsToAgent</code> and <code>saveFactsFromHistory</code> higher level abstractions save facts to the history, load facts from it, and update the LLM chat:</p> <pre><code>val loadProjectInfo by node&lt;Unit, Unit&gt; {\n    withMemory {\n        loadFactsToAgent(Concept(\"preferred-language\", \"What programming language is preferred by the user?\", FactType.SINGLE))\n    }\n}\n\nval saveProjectInfo by node&lt;Unit, Unit&gt; {\n    withMemory {\n        saveFactsFromHistory(Concept(\"preferred-language\", \"What programming language is preferred by the user?\", FactType.SINGLE),\n            subject = MemorySubjects.User,\n            scope = MemoryScope.Product(\"my-app\")\n        )\n    }\n}\n</code></pre>"},{"location":"agent-memory/#automatic-fact-detection","title":"Automatic fact detection","text":"<p>You can also ask the LLM to detect all the facts from the agent's history using the <code>nodeSaveToMemoryAutoDetectFacts</code> method:</p> <pre><code>val saveAutoDetect by nodeSaveToMemoryAutoDetectFacts&lt;Unit&gt;(\n    subjects = listOf(MemorySubjects.User, MemorySubjects.Machine)\n)\n</code></pre> <p>In the example above, the LLM would search for the user-related facts and project-related facts, determine the concepts, and save them into the memory.</p>"},{"location":"agent-memory/#best-practices","title":"Best practices","text":"<ol> <li> <p>Start Simple</p> <ul> <li>Begin with basic storage without encryption</li> <li>Use single facts before moving to multiple facts</li> </ul> </li> <li> <p>Organize Well</p> <ul> <li>Use clear concept names</li> <li>Add helpful descriptions</li> <li>Keep related information under the same subject</li> </ul> </li> <li> <p>Handle Errors </p> </li> </ol> <p><pre><code> try {\n     memoryProvider.save(fact, subject)\n } catch (e: Exception) {\n     println(\"Oops! Couldn't save: ${e.message}\")\n }\n</code></pre> </p> <p>For more details on error handling, see Error handling and edge cases.</p>"},{"location":"agent-memory/#error-handling-and-edge-cases","title":"Error handling and edge cases","text":"<p>The AgentMemory feature includes several mechanisms to handle edge cases:</p> <ol> <li> <p>NoMemory provider: a default implementation that doesn't store anything, used when no memory provider is    specified.</p> </li> <li> <p>Subject specificity handling: when loading facts, the feature prioritizes facts from more specific subjects    based on their defined <code>priorityLevel</code>.</p> </li> <li> <p>Scope filtering: facts can be filtered by scope to ensure only relevant information is loaded.</p> </li> <li> <p>Timestamp tracking: facts are stored with timestamps to track when they were created.</p> </li> <li> <p>Fact type handling: the feature supports both single facts and multiple facts, with appropriate handling for each type.</p> </li> </ol>"},{"location":"agent-memory/#api-documentation","title":"API documentation","text":"<p>For a complete API reference related to the AgentMemory feature, see the reference documentation for the agents-features-memory module.</p> <p>API documentation for specific packages:</p> <ul> <li>ai.koog.agents.local.memory.feature: includes the <code>AgentMemory</code> class and the core implementation of the   AI agents memory feature.</li> <li>ai.koog.agents.local.memory.feature.nodes: includes predefined memory-related nodes that can be used in   subgraphs.</li> <li>ai.koog.agents.local.memory.config: provides definitions of memory scopes used for memory operations.</li> <li>ai.koog.agents.local.memory.model: includes definitions of the core data structures and interfaces   that enable agents to store, organize, and retrieve information across different contexts and time periods.</li> <li>ai.koog.agents.local.memory.feature.history: provides the history compression strategy for retrieving and   incorporating factual knowledge about specific concepts from past session activity or stored memory.</li> <li>ai.koog.agents.local.memory.providers: provides the core interface that defines the fundamental operation for storing and retrieving knowledge in a structured, context-aware manner and its implementations.</li> <li>ai.koog.agents.local.memory.storage: provides the core interface and specific implementations for file operations across different platforms and storage backends.</li> </ul>"},{"location":"agent-memory/#faq-and-troubleshooting","title":"FAQ and troubleshooting","text":""},{"location":"agent-memory/#how-do-i-implement-a-custom-memory-provider","title":"How do I implement a custom memory provider?","text":"<p>To implement a custom memory provider, create a class that implements the <code>AgentMemoryProvider</code> interface:</p> <pre><code>class MyCustomMemoryProvider : AgentMemoryProvider {\n    override suspend fun save(fact: Fact, subject: MemorySubject, scope: MemoryScope) {\n        // Implementation for saving facts\n    }\n\n    override suspend fun load(concept: Concept, subject: MemorySubject, scope: MemoryScope): List&lt;Fact&gt; {\n        // Implementation for loading facts by concept\n    }\n\n    override suspend fun loadAll(subject: MemorySubject, scope: MemoryScope): List&lt;Fact&gt; {\n        // Implementation for loading all facts\n    }\n\n    override suspend fun loadByDescription(\n        description: String,\n        subject: MemorySubject,\n        scope: MemoryScope\n    ): List&lt;Fact&gt; {\n        // Implementation for loading facts by description\n    }\n}\n</code></pre>"},{"location":"agent-memory/#how-are-facts-prioritized-when-loading-from-multiple-subjects","title":"How are facts prioritized when loading from multiple subjects?","text":"<p>Facts are prioritized based on subject specificity. When loading facts, if the same concept has facts from multiple subjects, the fact from the most specific subject will be used.</p>"},{"location":"agent-memory/#can-i-store-multiple-values-for-the-same-concept","title":"Can I store multiple values for the same concept?","text":"<p>Yes, by using the <code>MultipleFacts</code> type. When defining a concept, set its <code>factType</code> to <code>FactType.MULTIPLE</code>:</p> <pre><code>val concept = Concept(\n    keyword = \"user-skills\",\n    description = \"Programming languages the user is skilled in\",\n    factType = FactType.MULTIPLE\n)\n</code></pre> <p>This lets you store multiple values for the concept, which is retrieved as a list.</p>"},{"location":"agent-persistency/","title":"Agent Persistency","text":"<p>Agent Persistency is a feature that provides checkpoint functionality for AI agents in the Koog framework. It lets you save and restore the state of an agent at specific points during execution, enabling capabilities such as:</p> <ul> <li>Resuming agent execution from a specific point</li> <li>Rolling back to previous states</li> <li>Persisting agent state across sessions</li> </ul>"},{"location":"agent-persistency/#key-concepts","title":"Key concepts","text":""},{"location":"agent-persistency/#checkpoints","title":"Checkpoints","text":"<p>A checkpoint captures the complete state of an agent at a specific point in its execution, including:</p> <ul> <li>Message history (all interactions between user, system, assistant, and tools)</li> <li>Current node being executed</li> <li>Input data for the current node</li> <li>Timestamp of creation</li> </ul> <p>Checkpoints are identified by unique IDs and are associated with a specific agent.</p>"},{"location":"agent-persistency/#prerequisites","title":"Prerequisites","text":"<p>The Agent Persistency feature requires that all nodes in your agent's strategy have unique names. This is enforced when the feature is installed:</p> <pre><code>require(ctx.strategy.metadata.uniqueNames) {\n    \"Checkpoint feature requires unique node names in the strategy metadata\"\n}\n</code></pre> <p>Make sure to set unique names for nodes in your graph.</p>"},{"location":"agent-persistency/#installation","title":"Installation","text":"<p>To use the Agent Persistency feature, add it to your agent's configuration:</p> <pre><code>val agent = AIAgent(\n    executor = executor,\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    install(Persistency) {\n        // Use in-memory storage for snapshots\n        storage = InMemoryPersistencyStorageProvider(\"in-memory-storage\")\n        // Enable automatic persistency\n        enableAutomaticPersistency = true\n    }\n}\n</code></pre>"},{"location":"agent-persistency/#configuration-options","title":"Configuration options","text":"<p>The Agent Persistency feature has two main configuration options:</p> <ul> <li>Storage provider: the provider used to save and retrieve checkpoints.</li> <li>Continuous persistence: automatic creation of checkpoints after each node is run.</li> </ul>"},{"location":"agent-persistency/#storage-provider","title":"Storage provider","text":"<p>Set the storage provider that will be used to save and retrieve checkpoints:</p> <pre><code>install(Persistency) {\n    storage = InMemoryPersistencyStorageProvider(\"in-memory-storage\")\n}\n</code></pre> <p>The framework includes the following built-in providers:</p> <ul> <li><code>InMemoryPersistencyStorageProvider</code>: stores checkpoints in memory (lost when the application restarts).</li> <li><code>FilePersistencyStorageProvider</code>: persists checkpoints to the file system.</li> <li><code>NoPersistencyStorageProvider</code>: a no-op implementation that does not store checkpoints. This is the default provider.</li> </ul> <p>You can also implement custom storage providers by implementing the <code>PersistencyStorageProvider</code> interface. For more information, see Custom storage providers.</p>"},{"location":"agent-persistency/#continuous-persistence","title":"Continuous persistence","text":"<p>Continuous persistence means that a checkpoint is automatically created after each node is run. To activate continuous persistence, use the code below:</p> <pre><code>install(Persistency) {\n    enableAutomaticPersistency = true\n}\n</code></pre> <p>When activated, the agent will automatically create a checkpoint after each node is executed, allowing for fine-grained recovery.</p>"},{"location":"agent-persistency/#basic-usage","title":"Basic usage","text":""},{"location":"agent-persistency/#creating-a-checkpoint","title":"Creating a checkpoint","text":"<p>To learn how to create a checkpoint at a specific point in your agent's execution, see the code sample below:</p> <pre><code>suspend fun example(context: AIAgentContextBase) {\n    // Create a checkpoint with the current state\n    val checkpoint = context.persistency().createCheckpoint(\n        agentContext = context,\n        nodeId = \"current-node-id\",\n        lastInput = inputData,\n        lastInputType = inputType,\n        checkpointId = context.runId,\n    )\n\n    // The checkpoint ID can be stored for later use\n    val checkpointId = checkpoint?.checkpointId\n}\n</code></pre>"},{"location":"agent-persistency/#restoring-from-a-checkpoint","title":"Restoring from a checkpoint","text":"<p>To restore the state of an agent from a specific checkpoint, follow the code sample below:</p> <pre><code>suspend fun example(context: AIAgentContextBase, checkpointId: String) {\n    // Roll back to a specific checkpoint\n    context.persistency().rollbackToCheckpoint(checkpointId, context)\n\n    // Or roll back to the latest checkpoint\n    context.persistency().rollbackToLatestCheckpoint(context)\n}\n</code></pre>"},{"location":"agent-persistency/#using-extension-functions","title":"Using extension functions","text":"<p>The Agent Persistency feature provides convenient extension functions for working with checkpoints:</p> <pre><code>suspend fun example(context: AIAgentContextBase) {\n    // Access the checkpoint feature\n    val checkpointFeature = context.persistency()\n\n    // Or perform an action with the checkpoint feature\n    context.withPersistency(context) { ctx -&gt;\n        // 'this' is the checkpoint feature\n        createCheckpoint(\n            agentContext = ctx,\n            nodeId = \"current-node-id\",\n            lastInput = inputData,\n            lastInputType = inputType,\n            checkpointId = ctx.runId,\n        )\n    }\n}\n</code></pre>"},{"location":"agent-persistency/#advanced-usage","title":"Advanced usage","text":""},{"location":"agent-persistency/#custom-storage-providers","title":"Custom storage providers","text":"<p>You can implement custom storage providers by implementing the <code>PersistencyStorageProvider</code> interface:</p> <pre><code>class MyCustomStorageProvider : PersistencyStorageProvider {\n    override suspend fun getCheckpoints(agentId: String): List&lt;AgentCheckpointData&gt; {\n        // Implementation\n    }\n\n    override suspend fun saveCheckpoint(agentCheckpointData: AgentCheckpointData) {\n        // Implementation\n    }\n\n    override suspend fun getLatestCheckpoint(agentId: String): AgentCheckpointData? {\n        // Implementation\n    }\n}\n</code></pre> <p>To use your custom provider in the feature configuration, set it as the storage when configuring the Agent Persistency feature in your agent.</p> <pre><code>install(Persistency) {\n    storage = MyCustomStorageProvider()\n}\n</code></pre>"},{"location":"agent-persistency/#setting-execution-points","title":"Setting execution points","text":"<p>For advanced control, you can directly set the execution point of an agent:</p> <pre><code>fun example(context: AIAgentContextBase) {\n    context.persistency().setExecutionPoint(\n        agentContext = context,\n        nodeId = \"target-node-id\",\n        messageHistory = customMessageHistory,\n        input = customInput\n    )\n}\n</code></pre> <p>This allows for more fine-grained control over the agent's state beyond just restoring from checkpoints.</p>"},{"location":"annotation-based-tools/","title":"Annotation-based tools","text":"<p>Annotation-based tools provide a declarative way to expose functions as tools for large language models (LLMs). By using annotations, you can transform any function into a tool that LLMs can understand and use.</p> <p>This approach is useful when you need to expose existing functionality to LLMs without implementing tool descriptions manually.</p> <p>Note</p> <p>Annotation-based tools are JVM-only and not available for other platforms. For multiplatform support, use the class-based tool API.</p>"},{"location":"annotation-based-tools/#key-annotations","title":"Key annotations","text":"<p>To start using annotation-based tools in your project, you need to understand the following key annotations:</p> Annotation Description <code>@Tool</code> Marks functions that should be exposed as tools to LLMs. <code>@LLMDescription</code> Provides descriptive information about your tools and their components."},{"location":"annotation-based-tools/#tool-annotation","title":"@Tool annotation","text":"<p>The <code>@Tool</code> annotation is used to mark functions that should be exposed as tools to LLMs. The functions annotated with <code>@Tool</code> are collected by reflection from objects that implement the <code>ToolSet</code> interface. For details, see Implement the ToolSet interface.</p>"},{"location":"annotation-based-tools/#definition","title":"Definition","text":"<pre><code>@Target(AnnotationTarget.FUNCTION)\npublic annotation class Tool(val customName: String = \"\")\n</code></pre>"},{"location":"annotation-based-tools/#parameters","title":"Parameters","text":"Name Required Description <code>customName</code> No Specifies a custom name for the tool. If not provided, the name of the function is used."},{"location":"annotation-based-tools/#usage","title":"Usage","text":"<p>To mark a function as a tool, apply the <code>@Tool</code> annotation to this function in a class that implements the <code>ToolSet</code> interface:</p> <pre><code>class MyToolSet : ToolSet {\n    @Tool\n    fun myTool(): String {\n        // Tool implementation\n        return \"Result\"\n    }\n\n    @Tool(customName = \"customToolName\")\n    fun anotherTool(): String {\n        // Tool implementation\n        return \"Result\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#llmdescription-annotation","title":"@LLMDescription annotation","text":"<p>The <code>@LLMDescription</code> annotation provides descriptive information about code elements (classes, functions, parameters, and so on) to LLMs. This helps LLMs understand the purpose and usage of these elements.</p>"},{"location":"annotation-based-tools/#definition_1","title":"Definition","text":"<pre><code>@Target(\n    AnnotationTarget.PROPERTY,\n    AnnotationTarget.CLASS,\n    AnnotationTarget.PROPERTY,\n    AnnotationTarget.TYPE,\n    AnnotationTarget.VALUE_PARAMETER,\n    AnnotationTarget.FUNCTION\n)\npublic annotation class LLMDescription(val description: String)\n</code></pre>"},{"location":"annotation-based-tools/#parameters_1","title":"Parameters","text":"Name Required Description <code>description</code> Yes A string that describes the annotated element."},{"location":"annotation-based-tools/#usage_1","title":"Usage","text":"<p>The <code>@LLMDescription</code> annotation can be applied at various levels. For example:</p> <ul> <li>Function level:</li> </ul> <pre><code>@Tool\n@LLMDescription(\"Performs a specific operation and returns the result\")\nfun myTool(): String {\n    // Function implementation\n    return \"Result\"\n}\n</code></pre> <ul> <li>Parameter level:</li> </ul> <pre><code>@Tool\n@LLMDescription(\"Processes input data\")\nfun processTool(\n    @LLMDescription(\"The input data to process\")\n    input: String,\n\n    @LLMDescription(\"Optional configuration parameters\")\n    config: String = \"\"\n): String {\n    // Function implementation\n    return \"Processed: $input with config: $config\"\n}\n</code></pre>"},{"location":"annotation-based-tools/#creating-a-tool","title":"Creating a tool","text":""},{"location":"annotation-based-tools/#1-implement-the-toolset-interface","title":"1. Implement the ToolSet interface","text":"<p>Create a class that implements the <code>ToolSet</code> interface. This interface marks your class as a container for tools.</p> <pre><code>class MyFirstToolSet : ToolSet {\n    // Tools will go here\n}\n</code></pre>"},{"location":"annotation-based-tools/#2-add-tool-functions","title":"2. Add tool functions","text":"<p>Add functions to your class and annotate them with <code>@Tool</code> to expose them as tools:</p> <pre><code>class MyFirstToolSet : ToolSet {\n    @Tool\n    fun getWeather(location: String): String {\n        // In a real implementation, you would call a weather API\n        return \"The weather in $location is sunny and 72\u00b0F\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#3-add-descriptions","title":"3. Add descriptions","text":"<p>Add <code>@LLMDescription</code> annotations to provide context for the LLM:</p> <pre><code>@LLMDescription(\"Tools for getting weather information\")\nclass MyFirstToolSet : ToolSet {\n    @Tool\n    @LLMDescription(\"Get the current weather for a location\")\n    fun getWeather(\n        @LLMDescription(\"The city and state/country\")\n        location: String\n    ): String {\n        // In a real implementation, you would call a weather API\n        return \"The weather in $location is sunny and 72\u00b0F\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#4-use-your-tools-with-an-agent","title":"4. Use your tools with an agent","text":"<p>Now you can use your tools with an agent:</p> <pre><code>fun main() {\n    runBlocking {\n        // Create your tool set\n        val weatherTools = MyFirstToolSet()\n\n        // Create an agent with your tools\n\n        val agent = AIAgent(\n            executor = simpleOpenAIExecutor(apiToken),\n            systemPrompt = \"Provide weather information for a given location.\",\n            llmModel = OpenAIModels.Chat.GPT4o,\n            toolRegistry = ToolRegistry {\n                tools(weatherTools)\n            }\n        )\n\n        // The agent can now use your weather tools\n        agent.run(\"What's the weather like in New York?\")\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#usage-examples","title":"Usage examples","text":"<p>Here are some real-world examples of tool annotations.</p>"},{"location":"annotation-based-tools/#basic-example-switch-controller","title":"Basic example: Switch controller","text":"<p>This example shows a simple tool set for controlling a switch:</p> <pre><code>@LLMDescription(\"Tools for controlling a switch\")\nclass SwitchTools(val switch: Switch) : ToolSet {\n    @Tool\n    @LLMDescription(\"Switches the state of the switch\")\n    fun switch(\n        @LLMDescription(\"The state to set (true for on, false for off)\")\n        state: Boolean\n    ): String {\n        switch.switch(state)\n        return \"Switched to ${if (state) \"on\" else \"off\"}\"\n    }\n\n    @Tool\n    @LLMDescription(\"Returns the current state of the switch\")\n    fun switchState(): String {\n        return \"Switch is ${if (switch.isOn()) \"on\" else \"off\"}\"\n    }\n}\n</code></pre> <p>When an LLM needs to control a switch, it can understand the following information from the provided description:</p> <ul> <li>The purpose and functionality of the tools.</li> <li>The required parameters for using the tools.</li> <li>The acceptable values for each parameter.</li> <li>The expected return values upon execution.</li> </ul>"},{"location":"annotation-based-tools/#advanced-example-diagnostic-tools","title":"Advanced example: Diagnostic tools","text":"<p>This example shows a more complex tool set for device diagnostics:</p> <pre><code>@LLMDescription(\"Tools for performing diagnostics and troubleshooting on devices\")\nclass DiagnosticToolSet : ToolSet {\n    @Tool\n    @LLMDescription(\"Run diagnostic on a device to check its status and identify any issues\")\n    fun runDiagnostic(\n        @LLMDescription(\"The ID of the device to diagnose\")\n        deviceId: String,\n\n        @LLMDescription(\"Additional information for the diagnostic (optional)\")\n        additionalInfo: String = \"\"\n    ): String {\n        // Implementation\n        return \"Diagnostic results for device $deviceId\"\n    }\n\n    @Tool\n    @LLMDescription(\"Analyze an error code to determine its meaning and possible solutions\")\n    fun analyzeError(\n        @LLMDescription(\"The error code to analyze (e.g., 'E1001')\")\n        errorCode: String\n    ): String {\n        // Implementation\n        return \"Analysis of error code $errorCode\"\n    }\n}\n</code></pre>"},{"location":"annotation-based-tools/#best-practices","title":"Best practices","text":"<ul> <li>Provide clear descriptions: write clear, concise descriptions that explain the purpose and behavior of tools, parameters, and return values.</li> <li>Describe all parameters: add <code>@LLMDescription</code> to all parameters to help LLMs understand what each parameter is for.</li> <li>Use consistent naming: use consistent naming conventions for tools and parameters to make them more intuitive.</li> <li>Group related tools: group related tools in the same <code>ToolSet</code> implementation and provide a class-level description.</li> <li>Return informative results: make sure tool return values provide clear information about the result of the operation.</li> <li>Handle errors gracefully: include error handling in your tools and return informative error messages.</li> <li>Document default values: when parameters have default values, document this in the description.</li> <li>Keep tools focused: Each tool should perform a specific, well-defined task rather than trying to do too many things.</li> </ul>"},{"location":"annotation-based-tools/#troubleshooting-common-issues","title":"Troubleshooting common issues","text":"<p>When working with tool annotations, you might encounter some common issues.</p>"},{"location":"annotation-based-tools/#tools-not-being-recognized","title":"Tools not being recognized","text":"<p>If the agent does not recognize your tools, check the following:</p> <ul> <li>Your class implements the <code>ToolSet</code> interface.</li> <li>All tool functions are annotated with <code>@Tool</code>.</li> <li>Tool functions have appropriate return types (<code>String</code> is recommended for simplicity).</li> <li>Your tools are properly registered with the agent.</li> </ul>"},{"location":"annotation-based-tools/#unclear-tool-descriptions","title":"Unclear tool descriptions","text":"<p>If the LLM does not use your tools correctly or misunderstands their purpose, try the following:</p> <ul> <li>Improve your <code>@LLMDescription</code> annotations to be more specific and clear.</li> <li>Include examples in your descriptions if appropriate.</li> <li>Specify parameter constraints in the descriptions (for example, <code>\"Must be a positive number\"</code>).</li> <li>Use consistent terminology throughout your descriptions.</li> </ul>"},{"location":"annotation-based-tools/#parameter-type-issues","title":"Parameter type issues","text":"<p>If the LLM provides incorrect parameter types, try the following:</p> <ul> <li>Use simple parameter types when possible (<code>String</code>, <code>Boolean</code>, <code>Int</code>).</li> <li>Clearly describe the expected format in the parameter description.</li> <li>For complex types, consider using <code>String</code> parameters with a specific format and parse them in your tool.</li> <li>Include examples of valid inputs in your parameter descriptions.</li> </ul>"},{"location":"annotation-based-tools/#performance-issues","title":"Performance issues","text":"<p>If your tools cause performance problems, try the following:</p> <ul> <li>Keep tool implementations lightweight.</li> <li>For resource-intensive operations, consider implementing asynchronous processing.</li> <li>Cache results when appropriate.</li> <li>Log tool usage to identify bottlenecks.</li> </ul>"},{"location":"built-in-tools/","title":"Built-in tools","text":"<p>The Koog framework provides built-in tools that handle common scenarios of agent-user interaction.</p> <p>The following built-in tools are available:</p> Tool Name Description SayToUser <code>__say_to_user__</code> Lets the agent send a message to the user. It prints the agent message to the console with the <code>Agent says:</code> prefix. AskUser <code>__ask_user__</code> Lets the agent ask the user for input. It prints the agent message to the console and waits for user response. ExitTool <code>__exit__</code> Lets the agent finish the conversation and terminate the session. ReadFileTool <code>__read_file__</code> Reads text file with optional line range selection. Returns formatted content with metadata using 0-based line indexing."},{"location":"built-in-tools/#registering-built-in-tools","title":"Registering built-in tools","text":"<p>Like any other tool, a built-in tool must be added to the tool registry to become available for an agent. Here is an example:</p> <pre><code>// Create a tool registry with all built-in tools\nval toolRegistry = ToolRegistry {\n    tool(SayToUser)\n    tool(AskUser)\n    tool(ExitTool)\n    tool(ReadFileTool(JVMFileSystemProvider.ReadOnly))\n}\n\n// Pass the registry when creating an agent\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(apiToken),\n    systemPrompt = \"You are a helpful assistant.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry\n)\n</code></pre> <p>You can create a comprehensive set of capabilities for your agent by combining built-in tools and custom tools within the same registry. To learn more about custom tools, see Annotation-based tools and Class-based tools.</p>"},{"location":"class-based-tools/","title":"Class-based tools","text":"<p>This section explains the API designed for scenarios that require enhanced flexibility and customized behavior. With this approach, you have full control over a tool, including its parameters, metadata, execution logic, and how it is registered and invoked.</p> <p>This level of control is ideal for creating sophisticated tools that extend basic use cases, enabling seamless integration into agent sessions and workflows.</p> <p>This page describes how to implement a tool, manage tools through registries, call them, and use within node-based agent architectures.</p> <p>Note</p> <p>The API is multiplatform. This lets you use the same tools across different platforms.</p>"},{"location":"class-based-tools/#tool-implementation","title":"Tool implementation","text":"<p>The Koog framework provides the following approaches for implementing tools:</p> <ul> <li>Using the base class <code>Tool</code> for all tools. You should use this class when you need to return non-text results or require complete control over the tool behavior.</li> <li>Using the <code>SimpleTool</code> class that extends the base <code>Tool</code> class and simplifies the creation of tools that return text results. You should use this approach for scenarios where the    tool only needs to return a text.</li> </ul> <p>Both approaches use the same core components but differ in implementation and the results they return.</p>"},{"location":"class-based-tools/#tool-class","title":"Tool class","text":"<p>The <code>Tool&lt;Args, Result&gt;</code> abstract class is the base class for creating tools in Koog. It lets you create tools that accept specific argument types (<code>Args</code>) and return results of various types (<code>Result</code>).</p> <p>Each tool consists of the following components:</p> Component Description <code>Args</code> The serializable data class that defines arguments required for the tool. This class must implement the <code>ToolArgs</code> interface. For tools that do not require arguments, you can use the built-in <code>ToolArgs.Empty</code> implementation. <code>Result</code> The type of result that the tool returns. This must implement the <code>ToolResult</code> interface, which can be <code>ToolResult.Text</code>, <code>ToolResult.Boolean</code>, <code>ToolResult.Number</code>, or a custom implementation of <code>ToolResult.JSONSerializable</code>. <code>argsSerializer</code> The overridden variable that defines how the arguments for the tool are deserialized. See also argsSerializer. <code>descriptor</code> The overridden variable that specifies tool metadata:- <code>name</code>- <code>description</code>- <code>requiredParameters</code> (empty by default)- <code>optionalParameters</code> (empty by default)See also descriptor. <code>execute()</code> The function that implements the logic of the tool. It takes arguments of type <code>Args</code> and returns a result of type <code>Result</code>. See also execute(). <p>Tip</p> <p>Ensure your tools have clear descriptions and well-defined parameter names to make it easier for the LLM to understand and use them properly.</p>"},{"location":"class-based-tools/#usage-example","title":"Usage example","text":"<p>Here is an example of a custom tool implementation using the <code>Tool</code> class that returns a numeric result:</p> <pre><code>// Implement a simple calculator tool that adds two digits\nobject CalculatorTool : Tool&lt;CalculatorTool.Args, ToolResult.Number&gt;() {\n\n    // Arguments for the calculator tool\n    @Serializable\n    data class Args(\n        val digit1: Int,\n        val digit2: Int\n    ) : ToolArgs {\n        init {\n            require(digit1 in 0..9) { \"digit1 must be a single digit (0-9)\" }\n            require(digit2 in 0..9) { \"digit2 must be a single digit (0-9)\" }\n        }\n    }\n\n    // Serializer for the Args class\n    override val argsSerializer = Args.serializer()\n\n    // Tool descriptor\n    override val descriptor: ToolDescriptor = ToolDescriptor(\n        name = \"calculator\",\n        description = \"A simple calculator that can add two digits (0-9).\",\n        requiredParameters = listOf(\n            ToolParameterDescriptor(\n                name = \"digit1\",\n                description = \"The first digit to add (0-9)\",\n                type = ToolParameterType.Integer\n            ),\n            ToolParameterDescriptor(\n                name = \"digit2\",\n                description = \"The second digit to add (0-9)\",\n                type = ToolParameterType.Integer\n            )\n        )\n    )\n\n    // Function to add two digits\n    override suspend fun execute(args: Args): ToolResult.Number {\n        val sum = args.digit1 + args.digit2\n        return ToolResult.Number(sum)\n    }\n}\n</code></pre> <p>After implementing your tool, you need to add it to a tool registry and then use it with an agent. For details, see Tool registry.</p> <p>For more details, see API reference.</p>"},{"location":"class-based-tools/#simpletool-class","title":"SimpleTool class","text":"<p>The <code>SimpleTool&lt;Args&gt;</code> abstract class extends <code>Tool&lt;Args, ToolResult.Text&gt;</code> and simplifies the creation of tools that return text results.</p> <p>Each simple tool consists of the following components:</p> Component Description <code>Args</code> The serializable data class that defines arguments required for the custom tool. <code>argsSerializer</code> The overridden variable that defines how the arguments for the tool are serialized. See also argsSerializer. <code>descriptor</code> The overridden variable that specifies tool metadata:- <code>name</code>- <code>description</code>- <code>requiredParameters</code> (empty by default) - <code>optionalParameters</code> (empty by default) See also descriptor. <code>doExecute()</code> The overridden function that describes the main action performed by the tool. It takes arguments of type <code>Args</code> and returns a <code>String</code>. See also doExecute(). <p>Tip</p> <p>Ensure your tools have clear descriptions and well-defined parameter names to make it easier for the LLM to understand and use them properly.</p>"},{"location":"class-based-tools/#usage-example_1","title":"Usage example","text":"<p>Here is an example of a custom tool implementation using <code>SimpleTool</code>:</p> <pre><code>// Create a tool that casts a string expression to a double value\nobject CastToDoubleTool : SimpleTool&lt;CastToDoubleTool.Args&gt;() {\n    // Define tool arguments\n    @Serializable\n    data class Args(val expression: String, val comment: String) : ToolArgs\n\n    // Serializer for the Args class\n    override val argsSerializer = Args.serializer()\n\n    // Tool descriptor\n    override val descriptor = ToolDescriptor(\n        name = \"cast_to_double\",\n        description = \"casts the passed expression to double or returns 0.0 if the expression is not castable\",\n        requiredParameters = listOf(\n            ToolParameterDescriptor(\n                name = \"expression\", description = \"An expression to case to double\", type = ToolParameterType.String\n            )\n        ),\n        optionalParameters = listOf(\n            ToolParameterDescriptor(\n                name = \"comment\",\n                description = \"A comment on how to process the expression\",\n                type = ToolParameterType.String\n            )\n        )\n    )\n\n    // Function that executes the tool with the provided arguments\n    override suspend fun doExecute(args: Args): String {\n        return \"Result: ${castToDouble(args.expression)}, \" + \"the comment was: ${args.comment}\"\n    }\n\n    // Function to cast a string expression to a double value\n    private fun castToDouble(expression: String): Double {\n        return expression.toDoubleOrNull() ?: 0.0\n    }\n}\n</code></pre> <p>After implementing your tool, you need to add it to a tool registry and then use it with an agent. For details, see Tool registry.</p>"},{"location":"complex-workflow-agents/","title":"Complex workflow agents","text":"<p>In addition to single-run agents, the <code>AIAgent</code> class lets you build agents that handle complex workflows by defining  custom strategies, tools, configurations, and custom input/output types.</p> <p>The process of creating and configuring such an agent typically includes the following steps:</p> <ol> <li>Provide a prompt executor to communicate with the LLM.</li> <li>Define a strategy that controls the agent workflow.</li> <li>Configure agent behavior.</li> <li>Implement tools for the agent to use.</li> <li>Add optional features like event handling, memory, or tracing.</li> <li>Run the agent with user input.</li> </ol>"},{"location":"complex-workflow-agents/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, see Overview.</li> </ul> <p>Tip</p> <p>Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.</p>"},{"location":"complex-workflow-agents/#creating-a-single-run-agent","title":"Creating a single-run agent","text":""},{"location":"complex-workflow-agents/#1-add-dependencies","title":"1. Add dependencies","text":"<p>To use the <code>AIAgent</code> functionality, include all necessary dependencies in your build configuration:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:VERSION\")\n}\n</code></pre> <p>For all available installation methods, see Installation.</p>"},{"location":"complex-workflow-agents/#2-provide-a-prompt-executor","title":"2. Provide a prompt executor","text":"<p>Prompt executors manage and run prompts. You can choose a prompt executor based on the LLM provider you plan to use. Also, you can create a custom prompt executor using one of the available LLM clients. To learn more, see Prompt executors.</p> <p>For example, to provide the OpenAI prompt executor, you need to call the <code>simpleOpenAIExecutor</code> function and provide it with the API key required for authentication with the OpenAI service:</p> <pre><code>val promptExecutor = simpleOpenAIExecutor(token)\n</code></pre> <p>To create a prompt executor that works with multiple LLM providers, do the following:</p> <ol> <li>Configure clients for the required LLM providers with the corresponding API keys. For example:</li> </ol> <pre><code>val openAIClient = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\nval anthropicClient = AnthropicLLMClient(System.getenv(\"ANTHROPIC_KEY\"))\nval googleClient = GoogleLLMClient(System.getenv(\"GOOGLE_KEY\"))\n</code></pre> <ol> <li>Pass the configured clients to the <code>DefaultMultiLLMPromptExecutor</code> class constructor to create a prompt executor with multiple LLM providers:</li> </ol> <pre><code>val multiExecutor = DefaultMultiLLMPromptExecutor(openAIClient, anthropicClient, googleClient)\n</code></pre>"},{"location":"complex-workflow-agents/#3-define-a-strategy","title":"3. Define a strategy","text":"<p>A strategy defines the workflow of your agent by using nodes and edges. It can have arbitrary input and output types,  which can be specified in <code>strategy</code> function generic parameters. These will be input/output types of the <code>AIAgent</code> as well. Default type for both input and output is <code>String</code>.</p> <p>Tip</p> <p>To learn more about strategies, see Custom strategy graphs</p>"},{"location":"complex-workflow-agents/#31-understand-nodes-and-edges","title":"3.1. Understand nodes and edges","text":"<p>Nodes and edges are the building blocks of the strategy.</p> <p>Nodes represent processing steps in your agent strategy.</p> <pre><code>val processNode by node&lt;InputType, OutputType&gt; { input -&gt;\n    // Process the input and return an output\n    // You can use llm.writeSession to interact with the LLM\n    // You can call tools using callTool, callToolRaw, etc.\n    transformedOutput\n}\n</code></pre> <p>Tip</p> <p>There are also pre-defined nodes that you can use in your agent strategy. To learn more, see Predefined nodes and components.</p> <p>Edges define the connections between nodes.</p> <pre><code>// Basic edge\nedge(sourceNode forwardTo targetNode)\n\n// Edge with condition\nedge(sourceNode forwardTo targetNode onCondition { output -&gt;\n    // Return true to follow this edge, false to skip it\n    output.contains(\"specific text\")\n})\n\n// Edge with transformation\nedge(sourceNode forwardTo targetNode transformed { output -&gt;\n    // Transform the output before passing it to the target node\n    \"Modified: $output\"\n})\n\n// Combined condition and transformation\nedge(sourceNode forwardTo targetNode onCondition { it.isNotEmpty() } transformed { it.uppercase() })\n</code></pre>"},{"location":"complex-workflow-agents/#32-implement-the-strategy","title":"3.2. Implement the strategy","text":"<p>To implement the agent strategy, call the <code>strategy</code> function and define nodes and edges. For example:</p> <pre><code>val agentStrategy = strategy(\"Simple calculator\") {\n    // Define nodes for the strategy\n    val nodeSendInput by nodeLLMRequest()\n    val nodeExecuteTool by nodeExecuteTool()\n    val nodeSendToolResult by nodeLLMSendToolResult()\n\n    // Define edges between nodes\n    // Start -&gt; Send input\n    edge(nodeStart forwardTo nodeSendInput)\n\n    // Send input -&gt; Finish\n    edge(\n        (nodeSendInput forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n\n    // Send input -&gt; Execute tool\n    edge(\n        (nodeSendInput forwardTo nodeExecuteTool)\n                onToolCall { true }\n    )\n\n    // Execute tool -&gt; Send the tool result\n    edge(nodeExecuteTool forwardTo nodeSendToolResult)\n\n    // Send the tool result -&gt; finish\n    edge(\n        (nodeSendToolResult forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n}\n</code></pre> <p>Tip</p> <p>The <code>strategy</code> function lets you define multiple subgraphs, each containing its own set of nodes and edges. This approach offers more flexibility and functionality compared to using simplified strategy builders. To learn more about subgraphs, see Subgraphs.</p>"},{"location":"complex-workflow-agents/#4-configure-the-agent","title":"4. Configure the agent","text":"<p>Define agent behavior with a configuration:</p> <pre><code>val agentConfig = AIAgentConfig.withSystemPrompt(\n    prompt = \"\"\"\n        You are a simple calculator assistant.\n        You can add two numbers together using the calculator tool.\n        When the user provides input, extract the numbers they want to add.\n        The input might be in various formats like \"add 5 and 7\", \"5 + 7\", or just \"5 7\".\n        Extract the two numbers and use the calculator tool to add them.\n        Always respond with a clear, friendly message showing the calculation and result.\n        \"\"\".trimIndent()\n)\n</code></pre> <p>For more advanced configuration, you can specify which LLM the agent will use and set the maximum number of iterations the agent can perform to respond:</p> <pre><code>val agentConfig = AIAgentConfig(\n    prompt = Prompt.build(\"simple-calculator\") {\n        system(\n            \"\"\"\n                You are a simple calculator assistant.\n                You can add two numbers together using the calculator tool.\n                When the user provides input, extract the numbers they want to add.\n                The input might be in various formats like \"add 5 and 7\", \"5 + 7\", or just \"5 7\".\n                Extract the two numbers and use the calculator tool to add them.\n                Always respond with a clear, friendly message showing the calculation and result.\n                \"\"\".trimIndent()\n        )\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 10\n)\n</code></pre>"},{"location":"complex-workflow-agents/#5-implement-tools-and-set-up-a-tool-registry","title":"5. Implement tools and set up a tool registry","text":"<p>Tools let your agent perform specific tasks. To make a tool available for the agent, add it to a tool registry. For example:</p> <pre><code>// Implement a simple calculator tool that can add two numbers\n@LLMDescription(\"Tools for performing basic arithmetic operations\")\nclass CalculatorTools : ToolSet {\n    @Tool\n    @LLMDescription(\"Add two numbers together and return their sum\")\n    fun add(\n        @LLMDescription(\"First number to add (integer value)\")\n        num1: Int,\n\n        @LLMDescription(\"Second number to add (integer value)\")\n        num2: Int\n    ): String {\n        val sum = num1 + num2\n        return \"The sum of $num1 and $num2 is: $sum\"\n    }\n}\n\n// Add the tool to the tool registry\nval toolRegistry = ToolRegistry {\n    tools(CalculatorTools())\n}\n</code></pre> <p>To learn more about tools, see Tools.</p>"},{"location":"complex-workflow-agents/#6-install-features","title":"6. Install features","text":"<p>Features let you add new capabilities to the agent, modify its behavior, provide access to external systems and resources, and log and monitor events while the agent is running. The following features are available:</p> <ul> <li>EventHandler</li> <li>AgentMemory</li> <li>Tracing</li> </ul> <p>To install the feature, call the <code>install</code> function and provide the feature as an argument. For example, to install the event handler feature, you need to do the following:</p> <pre><code>// install the EventHandler feature\ninstallFeatures = {\n    install(EventHandler) {\n        onBeforeAgentStarted { eventContext: AgentStartContext&lt;*&gt; -&gt;\n            println(\"Starting strategy: ${eventContext.strategy.name}\")\n        }\n        onAgentFinished { eventContext: AgentFinishedContext -&gt;\n            println(\"Result: ${eventContext.result}\")\n        }\n    }\n}\n</code></pre> <p>To learn more about feature configuration, see the dedicated page.</p>"},{"location":"complex-workflow-agents/#7-run-the-agent","title":"7. Run the agent","text":"<p>Create the agent with the configuration option created in the previous stages and run it with the provided input:</p> <pre><code>val agent = AIAgent(\n    promptExecutor = promptExecutor,\n    toolRegistry = toolRegistry,\n    strategy = agentStrategy,\n    agentConfig = agentConfig,\n    installFeatures = {\n        install(EventHandler) {\n            onBeforeAgentStarted { eventContext: AgentStartContext&lt;*&gt; -&gt;\n                println(\"Starting strategy: ${eventContext.strategy.name}\")\n            }\n            onAgentFinished { eventContext: AgentFinishedContext -&gt;\n                println(\"Result: ${eventContext.result}\")\n            }\n        }\n    }\n)\n\nfun main() {\n    runBlocking {\n        println(\"Enter two numbers to add (e.g., 'add 5 and 7' or '5 + 7'):\")\n\n        // Read the user input and send it to the agent\n        val userInput = readlnOrNull() ?: \"\"\n        val agentResult = agent.run(userInput)\n        println(\"The agent returned: $agentResult\")\n    }\n}\n</code></pre>"},{"location":"complex-workflow-agents/#working-with-structured-data","title":"Working with structured data","text":"<p>The <code>AIAgent</code> can process structured data from LLM outputs. For more details, see Structured data processing.</p>"},{"location":"complex-workflow-agents/#using-parallel-tool-calls","title":"Using parallel tool calls","text":"<p>The <code>AIAgent</code> supports parallel tool calls. This feature lets you process multiple tools concurrently, improving performance for independent operations.</p> <p>For more details, see Parallel tool calls.</p>"},{"location":"complex-workflow-agents/#full-code-sample","title":"Full code sample","text":"<p>Here is the complete implementation of the agent:</p> <pre><code>// Use the OpenAI executor with an API key from an environment variable\nval promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\"))\n\n// Create a simple strategy\nval agentStrategy = strategy(\"Simple calculator\") {\n    // Define nodes for the strategy\n    val nodeSendInput by nodeLLMRequest()\n    val nodeExecuteTool by nodeExecuteTool()\n    val nodeSendToolResult by nodeLLMSendToolResult()\n\n    // Define edges between nodes\n    // Start -&gt; Send input\n    edge(nodeStart forwardTo nodeSendInput)\n\n    // Send input -&gt; Finish\n    edge(\n        (nodeSendInput forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n\n    // Send input -&gt; Execute tool\n    edge(\n        (nodeSendInput forwardTo nodeExecuteTool)\n                onToolCall { true }\n    )\n\n    // Execute tool -&gt; Send the tool result\n    edge(nodeExecuteTool forwardTo nodeSendToolResult)\n\n    // Send the tool result -&gt; finish\n    edge(\n        (nodeSendToolResult forwardTo nodeFinish)\n                transformed { it }\n                onAssistantMessage { true }\n    )\n}\n\n// Configure the agent\nval agentConfig = AIAgentConfig(\n    prompt = Prompt.build(\"simple-calculator\") {\n        system(\n            \"\"\"\n                You are a simple calculator assistant.\n                You can add two numbers together using the calculator tool.\n                When the user provides input, extract the numbers they want to add.\n                The input might be in various formats like \"add 5 and 7\", \"5 + 7\", or just \"5 7\".\n                Extract the two numbers and use the calculator tool to add them.\n                Always respond with a clear, friendly message showing the calculation and result.\n                \"\"\".trimIndent()\n        )\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 10\n)\n\n// Implement a simple calculator tool that can add two numbers\n@LLMDescription(\"Tools for performing basic arithmetic operations\")\nclass CalculatorTools : ToolSet {\n    @Tool\n    @LLMDescription(\"Add two numbers together and return their sum\")\n    fun add(\n        @LLMDescription(\"First number to add (integer value)\")\n        num1: Int,\n\n        @LLMDescription(\"Second number to add (integer value)\")\n        num2: Int\n    ): String {\n        val sum = num1 + num2\n        return \"The sum of $num1 and $num2 is: $sum\"\n    }\n}\n\n// Add the tool to the tool registry\nval toolRegistry = ToolRegistry {\n    tools(CalculatorTools())\n}\n\n// Create the agent\nval agent = AIAgent(\n    promptExecutor = promptExecutor,\n    toolRegistry = toolRegistry,\n    strategy = agentStrategy,\n    agentConfig = agentConfig,\n    installFeatures = {\n        install(EventHandler) {\n            onBeforeAgentStarted { eventContext: AgentStartContext&lt;*&gt; -&gt;\n                println(\"Starting strategy: ${eventContext.strategy.name}\")\n            }\n            onAgentFinished { eventContext: AgentFinishedContext -&gt;\n                println(\"Result: ${eventContext.result}\")\n            }\n        }\n    }\n)\n\nfun main() {\n    runBlocking {\n        println(\"Enter two numbers to add (e.g., 'add 5 and 7' or '5 + 7'):\")\n\n        // Read the user input and send it to the agent\n        val userInput = readlnOrNull() ?: \"\"\n        val agentResult = agent.run(userInput)\n        println(\"The agent returned: $agentResult\")\n    }\n}\n</code></pre>"},{"location":"content-moderation/","title":"Content moderation","text":"<p>Content moderation is the process of analyzing text, images, or other content to identify potentially harmful, inappropriate, or unsafe material. In the context of AI systems, moderation helps:</p> <ul> <li>Filter out harmful or inappropriate user inputs</li> <li>Prevent the generation of harmful or inappropriate AI responses</li> <li>Ensure compliance with ethical guidelines and legal requirements</li> <li>Protect users from exposure to potentially harmful content</li> </ul> <p>Moderation systems typically analyze content against predefined categories of harmful content (such as hate speech, violence, sexual content, etc.) and provide a determination of whether the content violates policies in any of these categories.</p> <p>Content moderation is crucial in AI applications for several reasons:</p> <ul> <li> <p>Safety and security</p> <ul> <li>Protect users from harmful, offensive, or disturbing content</li> <li>Prevent the misuse of AI systems for generating harmful content</li> <li>Maintain a safe environment for all users</li> </ul> </li> <li> <p>Legal and ethical compliance</p> <ul> <li>Comply with regulations regarding content distribution</li> <li>Adhere to ethical guidelines for AI deployment</li> <li>Avoid potential legal liabilities associated with harmful content</li> </ul> </li> <li> <p>Quality control</p> <ul> <li>Maintain the quality and appropriateness of interactions</li> <li>Ensure AI responses align with organizational values and standards</li> <li>Build user trust by consistently providing safe and appropriate content</li> </ul> </li> </ul>"},{"location":"content-moderation/#types-of-moderated-content","title":"Types of moderated content","text":"<p>Koog's moderation system can analyze various types of content:</p> <ul> <li> <p>User messages</p> <ul> <li>Text inputs from users before they are processed by the AI</li> <li>Images uploaded by users (with OpenAI Moderation.Omni model)</li> </ul> </li> <li> <p>Assistant messages</p> <ul> <li>AI-generated responses before they are shown to users</li> <li>Responses can be checked to ensure they don't contain harmful content</li> </ul> </li> <li> <p>Tool content</p> <ul> <li>Content generated by or passed to tools integrated with the AI system</li> <li>Ensures that tool inputs and outputs maintain content safety standards</li> </ul> </li> </ul>"},{"location":"content-moderation/#supported-providers-and-models","title":"Supported providers and models","text":"<p>Koog supports content moderation through multiple providers and models:</p>"},{"location":"content-moderation/#openai","title":"OpenAI","text":"<p>OpenAI offers two moderation models:</p> <ul> <li> <p>OpenAIModels.Moderation.Text</p> <ul> <li>Text-only moderation</li> <li>Previous generation moderation model</li> <li>Analyzes text content against multiple harm categories</li> <li>Fast and cost-effective</li> </ul> </li> <li> <p>OpenAIModels.Moderation.Omni</p> <ul> <li>Supports both text and image moderation</li> <li>Most capable OpenAI moderation model</li> <li>Can identify harmful content in both text and images</li> <li>More comprehensive than the Text model</li> </ul> </li> </ul>"},{"location":"content-moderation/#ollama","title":"Ollama","text":"<p>Ollama supports moderation through the following model:</p> <ul> <li>OllamaModels.Meta.LLAMA_GUARD_3<ul> <li>Text-only moderation</li> <li>Based on Meta's Llama Guard family of models</li> <li>Specialized for content moderation tasks</li> <li>Runs locally through Ollama</li> </ul> </li> </ul>"},{"location":"content-moderation/#using-moderation-with-llm-clients","title":"Using moderation with LLM clients","text":"<p>Koog provides two main approaches to content moderation, direct moderation on an <code>LLMClient</code> instance, or using the <code>moderate</code> method on a <code>PromptExecutor</code>.</p>"},{"location":"content-moderation/#direct-moderation-with-llmclient","title":"Direct Moderation with LLMClient","text":"<p>You can use the <code>moderate</code> method directly on an LLMClient instance:</p> <pre><code>// Example with OpenAI client\nval openAIClient = OpenAILLMClient(apiKey)\nval prompt = prompt(\"harmful-prompt\") { \n    user(\"I want to build a bomb\")\n}\n\n// Moderate with OpenAI's Omni moderation model\nval result = openAIClient.moderate(prompt, OpenAIModels.Moderation.Omni)\n\nif (result.isHarmful) {\n    println(\"Content was flagged as harmful\")\n    // Handle harmful content (e.g., reject the prompt)\n} else {\n    // Proceed with processing the prompt\n} \n</code></pre> <p>The <code>moderate</code> method takes the following arguments:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt to moderate. <code>model</code> LLModel Yes The model to use for moderation. <p>The method returns a ModerationResult.</p> <p>Here is an example of using content moderation with the Llama Guard 3 model through Ollama:</p> <pre><code>// Example with Ollama client\nval ollamaClient = OllamaClient()\nval prompt = prompt(\"harmful-prompt\") {\n    user(\"How to hack into someone's account\")\n}\n\n// Moderate with Llama Guard 3\nval result = ollamaClient.moderate(prompt, OllamaModels.Meta.LLAMA_GUARD_3)\n\nif (result.isHarmful) {\n    println(\"Content was flagged as harmful\")\n    // Handle harmful content\n} else {\n    // Proceed with processing the prompt\n}\n</code></pre>"},{"location":"content-moderation/#moderation-with-promptexecutor","title":"Moderation with PromptExecutor","text":"<p>You can also use the <code>moderate</code> method on a PromptExecutor, which will use the appropriate LLMClient based on the model's provider:</p> <pre><code>// Create a multi-provider executor\nval executor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to OpenAILLMClient(openAIApiKey),\n    LLMProvider.Ollama to OllamaClient()\n)\n\nval prompt = prompt(\"harmful-prompt\") {\n    user(\"How to create illegal substances\")\n}\n\n// Moderate with OpenAI\nval openAIResult = executor.moderate(prompt, OpenAIModels.Moderation.Omni)\n\n// Or moderate with Ollama\nval ollamaResult = executor.moderate(prompt, OllamaModels.Meta.LLAMA_GUARD_3)\n\n// Process the results\nif (openAIResult.isHarmful || ollamaResult.isHarmful) {\n    // Handle harmful content\n}\n</code></pre> <p>The <code>moderate</code> method takes the following arguments:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt to moderate. <code>model</code> LLModel Yes The model to use for moderation. <p>The method returns a ModerationResult.</p>"},{"location":"content-moderation/#moderationresult-structure","title":"ModerationResult structure","text":"<p>The moderation process returns a <code>ModerationResult</code> object with the following structure:</p> <pre><code>@Serializable\npublic data class ModerationResult(\n    val isHarmful: Boolean,\n    val categories: Map&lt;ModerationCategory, Boolean&gt;,\n    val categoryScores: Map&lt;ModerationCategory, Double&gt; = emptyMap(),\n    val categoryAppliedInputTypes: Map&lt;ModerationCategory, List&lt;InputType&gt;&gt; = emptyMap()\n) {\n    /**\n     * Represents the type of input provided for content moderation.\n     *\n     * This enumeration is used in conjunction with moderation categories to specify\n     * the format of the input being analyzed.\n     */\n    @Serializable\n    public enum class InputType {\n        /**\n         * This enum value is typically used to classify inputs as textual data\n         * within the supported input types.\n         */\n        TEXT,\n\n        /**\n         * Represents an input type specifically designed for handling and processing images.\n         * This enum constant can be used to classify or determine behavior for workflows requiring image-based inputs.\n         */\n        IMAGE,\n    }\n}\n</code></pre> <p>A <code>ModerationResult</code> object includes the following properties:</p> Name Data type Required Default Description <code>isHarmful</code> Boolean Yes If true, the content was flagged as harmful. <code>categories</code> Map&lt;ModerationCategory, Boolean&gt; Yes A map of moderation categories to boolean values indicating which categories were flagged. <code>categoryScores</code> Map&lt;ModerationCategory, Double&gt; No emptyMap() A map of moderation categories to confidence scores (0.0 to 1.0). <code>categoryAppliedInputTypes</code> Map&lt;ModerationCategory, List&lt;InputType&gt;&gt; No emptyMap() A map indicating which input types (<code>TEXT</code> or <code>IMAGE</code>) triggered each category."},{"location":"content-moderation/#moderation-categories","title":"Moderation categories","text":""},{"location":"content-moderation/#koog-moderation-categories","title":"Koog moderation categories","text":"<p>Possible moderation categories provided by the Koog framework (regardless of the underlying LLM and LLM provider) are as follows:</p> <ol> <li>Harassment: content that involves intimidation, bullying, or other behaviors directed towards individuals or groups with the intent to harass or demean.</li> <li>HarassmentThreatening: harmful interactions or communications that are intended to intimidate, coerce, or threaten individuals or groups.</li> <li>Hate: content that contains elements perceived as offensive, discriminatory, or expressing hatred towards individuals or groups based on attributes such as race, religion, gender, or other characteristics.</li> <li>HateThreatening: hate-related moderation category focusing on harmful content that not only spreads hate but also includes threatening language, behavior, or implications.</li> <li>Illicit: content that violates legal frameworks or ethical guidelines, including illegal or illicit activities.</li> <li>IllicitViolent: content that involves a combination of illegal or illicit activities with elements of violence.</li> <li>SelfHarm: content that pertains to self-harm or related behavior.</li> <li>SelfHarmIntent: material that contains expressions or indications of an individual's intent to harm themselves.</li> <li>SelfHarmInstructions: content that provides guidance, techniques, or encouragement for engaging in self-harm behaviors.</li> <li>Sexual: content that is sexually explicit or contains sexual references.</li> <li>SexualMinors: content concerning the exploitation, abuse, or endangerment of minors in a sexual context.</li> <li>Violence: content that promotes, incites, or depicts violence and physical harm towards individuals or groups.</li> <li>ViolenceGraphic: content that includes graphic depictions of violence, which may be harmful, distressing, or triggering to viewers.</li> <li>Defamation: responses that are verifiably false and likely to injure a living person's reputation.</li> <li>SpecializedAdvice: content that contains specialized financial, medical, or legal advice.</li> <li>Privacy: content that contains sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security.</li> <li>IntellectualProperty: responses that may violate the intellectual property rights of any third party.</li> <li>ElectionsMisinformation: content that contains factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections.</li> </ol> <p>Note</p> <p>These categories are subject to change as new moderation categories might be added, and existing ones may evolve over time.</p>"},{"location":"content-moderation/#openai-moderation-categories","title":"OpenAI moderation categories","text":"<p>OpenAI's moderation API provides the following categories:</p> <ul> <li>Harassment: content that expresses, incites, or promotes harassing language towards any target.</li> <li>Harassment/threatening: harassment content that also includes violence or serious harm towards any target.</li> <li>Hate: content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups is harassment.</li> <li>Hate/threatening: hateful content that also includes violence or serious harm towards the targeted group based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste.</li> <li>Illicit: content that gives advice or instruction on how to commit illicit acts. A phrase like \"how to shoplift\" would fit this category.</li> <li>Illicit/violent: the same types of content flagged by the illicit category, but also includes references to violence or procuring a weapon.</li> <li>Self-harm: content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders.</li> <li>Self-harm/intent: content where the speaker expresses that they are engaging or intend to engage in acts of self-harm, such as suicide, cutting, and eating disorders.</li> <li>Self-harm/instructions: content that encourages performing acts of self-harm, such as suicide, cutting, and eating disorders, or that gives instructions or advice on how to commit such acts.</li> <li>Sexual: content meant to arouse sexual excitement, such as the description of sexual activity, or that promotes sexual services (excluding sex education and wellness).</li> <li>Sexual/minors: sexual content that includes an individual who is under 18 years old.</li> <li>Violence: content that depicts death, violence, or physical injury.</li> <li>Violence/graphic: content that depicts death, violence, or physical injury in graphic detail.</li> </ul>"},{"location":"content-moderation/#ollama-hazard-categories","title":"Ollama hazard categories","text":"<p>Ollama's Llama Guard models use the following hazard categories:</p> <ul> <li> <p>S1 - Violent crimes: responses that enable, encourage, or endorse the commission of violent crimes, including:</p> <ul> <li>Unlawful violence toward people such as terrorism, genocide, murder, hate-motivated violent crimes, child abuse, assault, battery, kidnapping.</li> <li>Unlawful violence toward animals such as animal abuse.</li> </ul> </li> <li> <p>S2 - Non-violent crimes: responses that enable, encourage, or endorse the commission of non-violent crimes, including:</p> <ul> <li>Personal crimes such as labor trafficking, threats, intimidation, hate-motivated non-violent crimes</li> <li>Financial crimes such as fraud, scams, money laundering</li> <li>Property crimes such as theft, arson, vandalism</li> <li>Drug crimes such as creating or using regulated substances</li> <li>Weapons crimes such as producing unlicensed firearms</li> <li>Cyber crimes such as hacking</li> </ul> </li> <li> <p>S3 - Sex-related crimes: responses that enable, encourage, or endorse the commission of sex-related crimes, including:</p> <ul> <li>Sex trafficking</li> <li>Sexual assault such as rape</li> <li>Sexual harassment (physical, verbal, or visual)</li> <li>Prostitution</li> </ul> </li> <li> <p>S4 - Child sexual exploitation: responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.</p> </li> <li> <p>S5 - Defamation: responses that are both verifiably false and likely to injure a living person's reputation.</p> </li> <li> <p>S6 - Specialized advice: responses that contain specialized financial, medical, or legal advice, or that indicate dangerous activities or objects are safe.</p> </li> <li> <p>S7 - Privacy: responses that contain sensitive, nonpublic personal information that could undermine someone's physical, digital, or financial security.</p> </li> <li> <p>S8 - Intellectual property: responses that may violate the intellectual property rights of any third party.</p> </li> <li> <p>S9 - Indiscriminate weapons: responses that enable, encourage, or endorse the creation of indiscriminate weapons, including:</p> <ul> <li>Chemical weapons such as nerve gas</li> <li>Biological weapons such as anthrax</li> <li>Radiological weapons such as cobalt bombs</li> <li>Nuclear weapons such as fission bombs</li> <li>High-yield explosive weapons such as cluster munitions</li> </ul> </li> <li> <p>S10 - Hate: responses that demean or dehumanize people on the basis of their sensitive, personal characteristics such as race, color, ethnicity, national origin, disability, religious affiliation, caste, sexual orientation, sex, gender identity, or serious disease.</p> </li> <li> <p>S11 - Suicide and self-harm: responses that enable, encourage, or endorse acts of intentional self-harm, including:</p> <ul> <li>Suicide</li> <li>Self-injury such as cutting</li> <li>Disordered eating</li> </ul> </li> <li> <p>S12 - Sexual content: responses that contain erotica.</p> </li> <li> <p>S13 - Elections: responses that contain factually incorrect information about electoral systems and processes, including in the time, place, or manner of voting in civic elections.</p> </li> </ul>"},{"location":"content-moderation/#category-mapping-between-providers","title":"Category mapping between providers","text":"<p>The following table shows the mapping between Ollama and OpenAI moderation categories:</p> Ollama category Closest OpenAI moderation category or categories Notes S1 \u2013 Violent crimes <code>illicit/violent</code>, <code>violence</code> (<code>violence/graphic</code> when gore is described) Covers instructions or endorsement of violent wrongdoing, plus the violent content itself. S2 \u2013 Non\u2011violent crimes <code>illicit</code> Provides or encourages non\u2011violent criminal activity (fraud, hacking, drug making, etc.). S3 \u2013 Sex\u2011related crimes <code>illicit/violent</code> (rape, trafficking, etc.)<code>sexual</code> (sexual\u2011assault descriptions) Violent sexual wrongdoing combines illicit instructions + sexual content. S4 \u2013 Child sexual exploitation <code>sexual/minors</code> Any sexual content involving minors. S5 \u2013 Defamation UNIQUE OpenAI's categories don't have a dedicated defamation flag. S6 \u2013 Specialized advice (medical, legal, financial, dangerous\u2011activity \"safe\" claims) UNIQUE Not directly represented in the OpenAI schema. S7 \u2013 Privacy (exposed personal data, doxxing) UNIQUE No direct privacy\u2011disclosure category in OpenAI moderation. S8 \u2013 Intellectual property UNIQUE Copyright / IP issues are not a moderation category in OpenAI. S9 \u2013 Indiscriminate weapons <code>illicit/violent</code> Instructions to build or deploy WMDs are violent illicit content. S10 \u2013 Hate <code>hate</code> (demeaning) <code>hate/threatening</code> (violent or murderous hate) Same protected\u2011class scope. S11 \u2013 Suicide and self\u2011harm <code>self-harm</code>, <code>self-harm/intent</code>, <code>self-harm/instructions</code> Matches exactly to OpenAI's three self\u2011harm sub\u2011types. S12 \u2013 Sexual content (erotica) <code>sexual</code> Ordinary adult erotica (minors would shift to <code>sexual/minors</code>). S13 \u2013 Elections misinformation UNIQUE Electoral\u2011process misinformation isn't singled out in OpenAI's categories."},{"location":"content-moderation/#examples-of-moderation-results","title":"Examples of moderation results","text":""},{"location":"content-moderation/#openai-moderation-example-harmful-content","title":"OpenAI moderation example (harmful content)","text":"<p>OpenAI provides the specific <code>/moderations</code> API that provides responses in the following JSON format:</p> <pre><code>{\n  \"isHarmful\": true,\n  \"categories\": {\n    \"Harassment\": false,\n    \"HarassmentThreatening\": false,\n    \"Hate\": false,\n    \"HateThreatening\": false,\n    \"Sexual\": false,\n    \"SexualMinors\": false,\n    \"Violence\": false,\n    \"ViolenceGraphic\": false,\n    \"SelfHarm\": false,\n    \"SelfHarmIntent\": false,\n    \"SelfHarmInstructions\": false,\n    \"Illicit\": true,\n    \"IllicitViolent\": true\n  },\n  \"categoryScores\": {\n    \"Harassment\": 0.0001,\n    \"HarassmentThreatening\": 0.0001,\n    \"Hate\": 0.0001,\n    \"HateThreatening\": 0.0001,\n    \"Sexual\": 0.0001,\n    \"SexualMinors\": 0.0001,\n    \"Violence\": 0.0145,\n    \"ViolenceGraphic\": 0.0001,\n    \"SelfHarm\": 0.0001,\n    \"SelfHarmIntent\": 0.0001,\n    \"SelfHarmInstructions\": 0.0001,\n    \"Illicit\": 0.9998,\n    \"IllicitViolent\": 0.9876\n  },\n  \"categoryAppliedInputTypes\": {\n    \"Illicit\": [\"TEXT\"],\n    \"IllicitViolent\": [\"TEXT\"]\n  }\n}\n</code></pre> <p>In Koog, the structure of the response above maps to the following response:</p> <pre><code>ModerationResult(\n    isHarmful = true,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Hate to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Sexual to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Violence to ModerationCategoryResult(false, confidenceScore = 0.0145),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Illicit to ModerationCategoryResult(true, confidenceScore = 0.9998, appliedInputTypes = listOf(InputType.TEXT)),\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(true, confidenceScore = 0.9876, appliedInputTypes = listOf(InputType.TEXT)),\n    )\n)\n</code></pre>"},{"location":"content-moderation/#openai-moderation-example-safe-content","title":"OpenAI moderation example (safe content)","text":"<pre><code>{\n  \"isHarmful\": false,\n  \"categories\": {\n    \"Harassment\": false,\n    \"HarassmentThreatening\": false,\n    \"Hate\": false,\n    \"HateThreatening\": false,\n    \"Sexual\": false,\n    \"SexualMinors\": false,\n    \"Violence\": false,\n    \"ViolenceGraphic\": false,\n    \"SelfHarm\": false,\n    \"SelfHarmIntent\": false,\n    \"SelfHarmInstructions\": false,\n    \"Illicit\": false,\n    \"IllicitViolent\": false\n  },\n  \"categoryScores\": {\n    \"Harassment\": 0.0001,\n    \"HarassmentThreatening\": 0.0001,\n    \"Hate\": 0.0001,\n    \"HateThreatening\": 0.0001,\n    \"Sexual\": 0.0001,\n    \"SexualMinors\": 0.0001,\n    \"Violence\": 0.0001,\n    \"ViolenceGraphic\": 0.0001,\n    \"SelfHarm\": 0.0001,\n    \"SelfHarmIntent\": 0.0001,\n    \"SelfHarmInstructions\": 0.0001,\n    \"Illicit\": 0.0001,\n    \"IllicitViolent\": 0.0001\n  },\n  \"categoryAppliedInputTypes\": {}\n}\n</code></pre> <p>In Koog, the OpenAI response above is presented as follows:</p> <pre><code>ModerationResult(\n    isHarmful = false,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Hate to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Sexual to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Violence to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.Illicit to ModerationCategoryResult(false, confidenceScore = 0.0001),\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(false, confidenceScore = 0.0001),\n    )\n)\n</code></pre>"},{"location":"content-moderation/#ollama-moderation-example-harmful-content","title":"Ollama moderation example (harmful content)","text":"<p>Ollama approach to the moderation format significantly differs from the OpenAI approach. There are no specific moderation-related API endpoints in Ollama.  Instead, Ollama uses the general chat API.</p> <p>Ollama moderation models such as <code>llama-guard3</code> respond with a plain text result (Assistant message), where the first line is always <code>unsafe</code> or <code>safe</code>, and the next line or lines contain coma-separated Ollama hazard categories.</p> <p>For example:</p> <pre><code>unsafe\nS1,S10\n</code></pre> <p>This is translated to the following result in Koog:</p> <pre><code>ModerationResult(\n    isHarmful = true,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Hate to ModerationCategoryResult(true),    // from S10\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Sexual to ModerationCategoryResult(false),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false),\n        ModerationCategory.Violence to ModerationCategoryResult(false),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false),\n        ModerationCategory.Illicit to ModerationCategoryResult(true),    // from S1\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(true),    // from S1\n    )\n)\n</code></pre>"},{"location":"content-moderation/#ollama-moderation-example-safe-content","title":"Ollama moderation example (safe content)","text":"<p>Here is an example of an Ollama response that marks the content as safe:</p> <pre><code>safe\n</code></pre> <p>Koog translates the response in the following way:</p> <pre><code>ModerationResult(\n    isHarmful = false,\n    categories = mapOf(\n        ModerationCategory.Harassment to ModerationCategoryResult(false),\n        ModerationCategory.HarassmentThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Hate to ModerationCategoryResult(false),\n        ModerationCategory.HateThreatening to ModerationCategoryResult(false),\n        ModerationCategory.Sexual to ModerationCategoryResult(false),\n        ModerationCategory.SexualMinors to ModerationCategoryResult(false),\n        ModerationCategory.Violence to ModerationCategoryResult(false),\n        ModerationCategory.ViolenceGraphic to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarm to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmIntent to ModerationCategoryResult(false),\n        ModerationCategory.SelfHarmInstructions to ModerationCategoryResult(false),\n        ModerationCategory.Illicit to ModerationCategoryResult(false),\n        ModerationCategory.IllicitViolent to ModerationCategoryResult(false),\n    )\n)\n</code></pre>"},{"location":"custom-nodes/","title":"Custom node implementation","text":"<p>This page provides detailed instructions on how to implement your own custom nodes in the Koog framework.  Custom nodes let you extend the functionality of agent workflows by creating reusable components that perform specific operations.</p> <p>To learn more about what graph nodes are, their usage, and existing default nodes, see Graph nodes.</p>"},{"location":"custom-nodes/#node-architecture-overview","title":"Node architecture overview","text":"<p>Before diving into implementation details, it is important to understand the architecture of nodes in the Koog framework. Nodes are the fundamental building blocks of agent workflows, where each node represents a specific operation or transformation in the workflow. You connect nodes using edges, which define the flow of execution between nodes.</p> <p>Each node has an <code>execute</code> method that takes an input and produces an output, which is then passed to the next node in the workflow.</p>"},{"location":"custom-nodes/#implementing-a-custom-node","title":"Implementing a custom node","text":"<p>Custom node implementations range from simple implementations that perform a basic logic on the input data and return an output, to more complex node implementations that accept parameters and maintain state between runs.</p>"},{"location":"custom-nodes/#basic-node-implementation","title":"Basic node implementation","text":"<p>The simplest way to implement a custom node in a graph and define your own custom logic would be to use the following pattern:</p> <pre><code>val myNode by node&lt;Input, Output&gt;(\"node_name\") { input -&gt;\n    // Processing\n    returnValue\n}\n</code></pre> <p>The code above represents a custom node <code>myNode</code> with predefined <code>Input</code> and <code>Output</code> types, with the optional name string parameter (<code>node_name</code>). In an actual example, here is a simple node that takes a string input and returns the input's length:</p> <pre><code>val myNode by node&lt;String, Int&gt;(\"node_name\") { input -&gt;\n    // Processing\n    input.length\n}\n</code></pre> <p>Another way to create a custom node is to define an extension function on <code>AIAgentSubgraphBuilderBase</code> that calls the <code>node</code> function:</p> <pre><code>fun AIAgentSubgraphBuilderBase&lt;*, *&gt;.myCustomNode(\n    name: String? = null\n): AIAgentNodeDelegate&lt;Input, Output&gt; = node(name) { input -&gt;\n    // Custom logic\n    input // Return the input as output (pass-through)\n}\n\nval myCustomNode by myCustomNode(\"node_name\")\n</code></pre> <p>This creates a pass-through node that performs some custom logic but returns the input as the output without modification.</p>"},{"location":"custom-nodes/#nodes-with-additional-arguments","title":"Nodes with additional arguments","text":"<p>You can create nodes that accept arguments to customize their behavior:</p> <pre><code>    fun AIAgentSubgraphBuilderBase&lt;*, *&gt;.myNodeWithArguments(\n    name: String? = null,\n    arg1: String,\n    arg2: Int\n): AIAgentNodeDelegate&lt;Input, Output&gt; = node(name) { input -&gt;\n    // Use arg1 and arg2 in your custom logic\n    input // Return the input as the output\n}\n\nval myCustomNode by myNodeWithArguments(\"node_name\", arg1 = \"value1\", arg2 = 42)\n</code></pre>"},{"location":"custom-nodes/#parameterized-nodes","title":"Parameterized nodes","text":"<p>You can define nodes with input and output parameters:</p> <pre><code>inline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.myParameterizedNode(\n    name: String? = null,\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { input -&gt;\n    // Do some additional actions\n    // Return the input as the output\n    input\n}\n\nval strategy = strategy&lt;String, String&gt;(\"strategy_name\") {\n    val myCustomNode by myParameterizedNode&lt;String&gt;(\"node_name\")\n}\n</code></pre>"},{"location":"custom-nodes/#stateful-nodes","title":"Stateful nodes","text":"<p>If your node needs to maintain state between runs, you can use closure variables:</p> <pre><code>fun AIAgentSubgraphBuilderBase&lt;*, *&gt;.myStatefulNode(\n    name: String? = null\n): AIAgentNodeDelegate&lt;Input, Output&gt; {\n    var counter = 0\n\n    return node(name) { input -&gt;\n        counter++\n        println(\"Node executed $counter times\")\n        input\n    }\n}\n</code></pre>"},{"location":"custom-nodes/#node-input-and-output-types","title":"Node input and output types","text":"<p>Nodes can have different input and output types, which are specified as generic parameters:</p> <pre><code>val stringToIntNode by node&lt;String, Int&gt;(\"node_name\") { input: String -&gt;\n    // Processing\n    input.toInt() // Convert string to integer\n}\n</code></pre> <p>Note</p> <p>The input and output types determine how the node can be connected to other nodes in the workflow. Nodes can only be connected if the output type of the source node is compatible with the input type of the target node.</p>"},{"location":"custom-nodes/#best-practices","title":"Best practices","text":"<p>When implementing custom nodes, follow these best practices:</p> <ol> <li>Keep nodes focused: each node should perform a single, well-defined operation.</li> <li>Use descriptive names: node names should clearly indicate their purpose.</li> <li>Document parameters: provide clear documentation for all parameters.</li> <li>Handle errors gracefully: implement proper error handling to prevent workflow failures.</li> <li>Make nodes reusable: design nodes to be reusable across different workflows.</li> <li>Use type parameters: use generic type parameters when appropriate to make nodes more flexible.</li> <li>Provide default values: when possible, provide sensible default values for parameters.</li> </ol>"},{"location":"custom-nodes/#common-patterns","title":"Common patterns","text":"<p>The following sections provide some common patterns for implementing custom nodes.</p>"},{"location":"custom-nodes/#pass-through-nodes","title":"Pass-through nodes","text":"<p>Nodes that perform an operation but return the input as the output.</p> <pre><code>val loggingNode by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    println(\"Processing input: $input\")\n    input // Return the input as the output\n}\n</code></pre>"},{"location":"custom-nodes/#transformation-nodes","title":"Transformation nodes","text":"<p>Nodes that transform the input into a different output.</p> <pre><code>val upperCaseNode by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    println(\"Processing input: $input\")\n    input.uppercase() // Transform the input to uppercase\n}\n</code></pre>"},{"location":"custom-nodes/#llm-interaction-nodes","title":"LLM interaction nodes","text":"<p>Nodes that interact with the LLM.</p> <pre><code>val summarizeTextNode by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    llm.writeSession {\n        updatePrompt {\n            user(\"Please summarize the following text: $input\")\n        }\n\n        val response = requestLLMWithoutTools()\n        response.content\n    }\n}\n</code></pre>"},{"location":"custom-nodes/#tool-run-node","title":"Tool run node","text":"<pre><code>val nodeExecuteCustomTool by node&lt;String, String&gt;(\"node_name\") { input -&gt;\n    val toolCall = Message.Tool.Call(\n        id = UUID.randomUUID().toString(),\n        tool = toolName,\n        metaInfo = ResponseMetaInfo.create(Clock.System),\n        content = Json.encodeToString(ToolArgs(arg1 = input, arg2 = 42)) // Use the input as tool arguments\n    )\n\n    val result = environment.executeTool(toolCall)\n    result.content\n}\n</code></pre>"},{"location":"custom-strategy-graphs/","title":"Custom strategy graphs","text":"<p>Strategy graphs are the backbone of agent workflows in the Koog framework. They define how the agent processes input, interacts with tools, and generates output. A strategy graph consists of nodes connected by edges, with conditions determining the flow of execution.</p> <p>Creating a strategy graph lets you tailor the behavior of an agent to your specific needs, whether you are building a simple chatbot, a complex data processing pipeline, or anything in between.</p>"},{"location":"custom-strategy-graphs/#strategy-graph-architecture","title":"Strategy graph architecture","text":"<p>At a high level, a strategy graph consists of the following components:</p> <ul> <li>Strategy: the top-level container for the graph, created using the <code>strategy</code> function with the specified input    and output types using generic parameters.</li> <li>Subgraphs: sections of the graph that can have their own set of tools and context.</li> <li>Nodes: individual operations or transformations in the workflow.</li> <li>Edges: connections between nodes that define transition conditions and transformations.</li> </ul> <p>The strategy graph begins at a special node called <code>nodeStart</code> and ends at <code>nodeFinish</code>. The path between these nodes is determined by the edges and conditions specified in the graph.</p>"},{"location":"custom-strategy-graphs/#strategy-graph-components","title":"Strategy graph components","text":""},{"location":"custom-strategy-graphs/#nodes","title":"Nodes","text":"<p>Nodes are building blocks of a strategy graph. Each node represents a specific operation.</p> <p>The Koog framework provides predefined nodes and also lets you create custom nodes by using the <code>node</code> function.</p> <p>For details, see Predefined nodes and components and Custom nodes.</p>"},{"location":"custom-strategy-graphs/#edges","title":"Edges","text":"<p>Edges connect nodes and define the flow of operation in the strategy graph. An edge is created using the <code>edge</code> function and the <code>forwardTo</code> infix function:</p> <pre><code>edge(sourceNode forwardTo targetNode)\n</code></pre>"},{"location":"custom-strategy-graphs/#conditions","title":"Conditions","text":"<p>Conditions determine when to follow a particular edge in the strategy graph. There are several types of conditions, here are some common ones:</p> Condition type Description onCondition A general-purpose condition that takes a lambda expression that returns a boolean value. onToolCall A condition that matches when the LLM calls a tool. onAssistantMessage A condition that matches when the LLM responds with a message. onMultipleToolCalls A condition that matches when the LLM calls multiple tools. onToolNotCalled A condition that matches when the LLM does not call a tool. <p>You can transform the output before passing it to the target node by using the <code>transformed</code> function:</p> <pre><code>edge(sourceNode forwardTo targetNode \n        onCondition { input -&gt; input.length &gt; 10 }\n        transformed { input -&gt; input.uppercase() }\n)\n</code></pre>"},{"location":"custom-strategy-graphs/#subgraphs","title":"Subgraphs","text":"<p>Subgraphs are sections of the strategy graph that operate with their own set of tools and context. The strategy graph can contain multiple subgraphs. Each subgraph is defined by using the <code>subgraph</code> function:</p> <pre><code>val strategy = strategy&lt;Input, Output&gt;(\"strategy-name\") {\n    val firstSubgraph by subgraph&lt;FirstInput, FirstOutput&gt;(\"first\") {\n        // Define nodes and edges for this subgraph\n    }\n    val secondSubgraph by subgraph&lt;SecondInput, SecondOutput&gt;(\"second\") {\n        // Define nodes and edges for this subgraph\n    }\n}\n</code></pre> <p>A subgraph can use any tool from a tool registry.  However, you can specify a subset of tools from this registry that can be used in the subgraph and pass it as an argument to the <code>subgraph</code> function:</p> <pre><code>val strategy = strategy&lt;Input, Output&gt;(\"strategy-name\") {\n    val firstSubgraph by subgraph&lt;FirstInput, FirstOutput&gt;(\n        name = \"first\",\n        tools = listOf(someTool)\n    ) {\n        // Define nodes and edges for this subgraph\n    }\n   // Define other subgraphs\n}\n</code></pre>"},{"location":"custom-strategy-graphs/#basic-strategy-graph-creation","title":"Basic strategy graph creation","text":"<p>The basic strategy graph operates as follows: </p> <ol> <li>Sends the input to the LLM.</li> <li>If the LLM responds with a message, finishes the process.</li> <li>If the LLM calls a tool, runs the tool.</li> <li>Sends the tool result back to the LLM.</li> <li>If the LLM responds with a message, finishes the process.</li> <li>If the LLM calls another tool, runs the tool, and the process repeats from step 4.</li> </ol> <p></p> <p>Here is an example of a basic strategy graph:</p> <pre><code>val myStrategy = strategy&lt;String, String&gt;(\"my-strategy\") {\n    val nodeCallLLM by nodeLLMRequest()\n    val executeToolCall by nodeExecuteTool()\n    val sendToolResult by nodeLLMSendToolResult()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeCallLLM forwardTo executeToolCall onToolCall { true })\n    edge(executeToolCall forwardTo sendToolResult)\n    edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(sendToolResult forwardTo executeToolCall onToolCall { true })\n}\n</code></pre>"},{"location":"custom-strategy-graphs/#advanced-strategy-techniques","title":"Advanced strategy techniques","text":""},{"location":"custom-strategy-graphs/#history-compression","title":"History compression","text":"<p>For long-running conversations, the history can grow large and consume a lot of tokens. To learn how to compress the history, see History compression.</p>"},{"location":"custom-strategy-graphs/#parallel-tool-execution","title":"Parallel tool execution","text":"<p>For workflows that require executing multiple tools in parallel, you can use the <code>nodeExecuteMultipleTools</code> node:</p> <pre><code>val executeMultipleTools by nodeExecuteMultipleTools()\nval processMultipleResults by nodeLLMSendMultipleToolResults()\n\nedge(someNode forwardTo executeMultipleTools)\nedge(executeMultipleTools forwardTo processMultipleResults)\n</code></pre> <p>You can also use the <code>toParallelToolCallsRaw</code> extension function for streaming data:</p> <pre><code>parseMarkdownStreamToBooks(markdownStream).toParallelToolCallsRaw(BookTool::class).collect()\n</code></pre> <p>To learn more, see Tools. </p>"},{"location":"custom-strategy-graphs/#parallel-node-execution","title":"Parallel node execution","text":"<p>Parallel node execution lets you run multiple nodes concurrently, improving performance and enabling complex workflows.</p> <p>To initiate parallel node runs, use the <code>parallel</code> method:</p> <pre><code>val calc by parallel&lt;String, Int&gt;(\n    nodeCalcTokens, nodeCalcSymbols, nodeCalcWords,\n) {\n    selectByMax { it }\n}\n</code></pre> <p>The code above creates a node named <code>calc</code> that runs the <code>nodeCalcTokens</code>, <code>nodeCalcSymbols</code>, and <code>nodeCalcWords</code> nodes  in parallel and returns the results as an instance of <code>AsyncParallelResult</code>.</p> <p>For more information related to parallel node execution and a detailed reference, see Parallel node execution.</p>"},{"location":"custom-strategy-graphs/#conditional-branching","title":"Conditional branching","text":"<p>For complex workflows that require different paths based on certain conditions, you can use conditional branching:</p> <pre><code>val branchA by node&lt;String, String&gt; { input -&gt;\n    // Logic for branch A\n    \"Branch A: $input\"\n}\n\nval branchB by node&lt;String, String&gt; { input -&gt;\n    // Logic for branch B\n    \"Branch B: $input\"\n}\n\nedge(\n    (someNode forwardTo branchA)\n            onCondition { input -&gt; input.contains(\"A\") }\n)\nedge(\n    (someNode forwardTo branchB)\n            onCondition { input -&gt; input.contains(\"B\") }\n)\n</code></pre>"},{"location":"custom-strategy-graphs/#best-practices","title":"Best practices","text":"<p>When you create custom strategy graphs, follow these best practices:</p> <ul> <li>Keep it simple. Start with a simple graph and add complexity as needed.</li> <li>Give your nodes and edges descriptive names to make the graph easier to understand.</li> <li>Handle all possible paths and edge cases.</li> <li>Test your graph with various inputs to ensure it behaves as expected.</li> <li>Document the purpose and behavior of your graph for future reference.</li> <li>Use predefined strategies or common patterns as a starting point.</li> <li>For long-running conversations, use history compression to reduce token usage.</li> <li>Use subgraphs to organize your graph and manage tool access.</li> </ul>"},{"location":"custom-strategy-graphs/#usage-examples","title":"Usage examples","text":""},{"location":"custom-strategy-graphs/#tone-analysis-strategy","title":"Tone analysis strategy","text":"<p>The tone analysis strategy is a good example of a tool-based strategy that includes history compression:</p> <pre><code>fun toneStrategy(name: String, toolRegistry: ToolRegistry): AIAgentStrategy&lt;String, String&gt; {\n    return strategy(name) {\n        val nodeSendInput by nodeLLMRequest()\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n        val nodeCompressHistory by nodeLLMCompressHistory&lt;ReceivedToolResult&gt;()\n\n        // Define the flow of the agent\n        edge(nodeStart forwardTo nodeSendInput)\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendInput forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n\n        // If the LLM calls a tool, execute it\n        edge(\n            (nodeSendInput forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // If the history gets too large, compress it\n        edge(\n            (nodeExecuteTool forwardTo nodeCompressHistory)\n                    onCondition { _ -&gt; llm.readSession { prompt.messages.size &gt; 100 } }\n        )\n\n        edge(nodeCompressHistory forwardTo nodeSendToolResult)\n\n        // Otherwise, send the tool result directly\n        edge(\n            (nodeExecuteTool forwardTo nodeSendToolResult)\n                    onCondition { _ -&gt; llm.readSession { prompt.messages.size &lt;= 100 } }\n        )\n\n        // If the LLM calls another tool, execute it\n        edge(\n            (nodeSendToolResult forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendToolResult forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n    }\n}\n</code></pre> <p>This strategy does the following:</p> <ol> <li>Sends the input to the LLM.</li> <li>If the LLM responds with a message, the strategy finishes the process.</li> <li>If the LLM calls a tool, the strategy runs the tool.</li> <li>If the history is too large (more than 100 messages), the strategy compresses it before sending the tool result.</li> <li>Otherwise, the strategy sends the tool result directly.</li> <li>If the LLM calls another tool, the strategy runs it.</li> <li>If the LLM responds with a message, the strategy finishes the process.</li> </ol>"},{"location":"custom-strategy-graphs/#troubleshooting","title":"Troubleshooting","text":"<p>When creating custom strategy graphs, you might encounter some common issues. Here are some troubleshooting tips:</p>"},{"location":"custom-strategy-graphs/#graph-fails-to-reach-the-finish-node","title":"Graph fails to reach the finish node","text":"<p>If your graph does not reach the finish node, check the following:</p> <ul> <li>All paths from the start node eventually lead to the finish node.</li> <li>Your conditions are not too restrictive, preventing edges from being followed.</li> <li>There are no cycles in the graph that do not have an exit condition.</li> </ul>"},{"location":"custom-strategy-graphs/#tool-calls-are-not-running","title":"Tool calls are not running","text":"<p>If tool calls are not running, check the following:</p> <ul> <li>The tools are properly registered in the tool registry.</li> <li>The edge from the LLM node to the tool execution node has the correct condition (<code>onToolCall { true }</code>).</li> </ul>"},{"location":"custom-strategy-graphs/#history-gets-too-large","title":"History gets too large","text":"<p>If your history gets too large and consumes too many tokens, consider the following:</p> <ul> <li>Add a history compression node.</li> <li>Use a condition to check the size of the history and compress it when it gets too large.</li> <li>Use a more aggressive compression strategy (e.g., <code>FromLastNMessages</code> with a smaller N value).</li> </ul>"},{"location":"custom-strategy-graphs/#graph-behaves-unexpectedly","title":"Graph behaves unexpectedly","text":"<p>If your graph takes unexpected branches, check the following:</p> <ul> <li>Your conditions are correctly defined.</li> <li>The conditions are evaluated in the expected order (edges are checked in the order they are defined).</li> <li>You are not accidentally overriding conditions with more general ones.</li> </ul>"},{"location":"custom-strategy-graphs/#performance-issues-occur","title":"Performance issues occur","text":"<p>If your graph has performance issues, consider the following:</p> <ul> <li>Simplify the graph by removing unnecessary nodes and edges.</li> <li>Use parallel tool execution for independent operations.</li> <li>Compress history.</li> <li>Use more efficient nodes and operations.</li> </ul>"},{"location":"custom-subgraphs/","title":"Custom subgraphs","text":""},{"location":"custom-subgraphs/#creating-and-configuring-subgraphs","title":"Creating and configuring subgraphs","text":"<p>The following sections provide code templates and common patterns in the creation of subgraphs for agentic workflows.</p>"},{"location":"custom-subgraphs/#basic-subgraph-creation","title":"Basic subgraph creation","text":"<p>Custom subgraphs are typically created using the following patterns:</p> <ul> <li>Subgraph with a specified tool selection strategy:</li> </ul> <pre><code>strategy&lt;StrategyInput, StrategyOutput&gt;(\"strategy-name\") {\n    val subgraphIdentifier by subgraph&lt;Input, Output&gt;(\n        name = \"subgraph-name\",\n        toolSelectionStrategy = ToolSelectionStrategy.ALL\n    ) {\n        // Define nodes and edges for this subgraph\n    }\n}\n</code></pre> <ul> <li>Subgraph with a specified list of tools (subset of tools from a defined tool registry):</li> </ul> <pre><code>strategy&lt;StrategyInput, StrategyOutput&gt;(\"strategy-name\") {\n   val subgraphIdentifier by subgraph&lt;Input, Output&gt;(\n       name = \"subgraph-name\", \n       tools = listOf(firstTool, secondTool)\n   ) {\n        // Define nodes and edges for this subgraph\n    }\n}\n</code></pre> <p>For more information about parameters and parameter values, see the <code>subgraph</code> API reference. For more information about tools, see Tools.</p> <p>The following code sample shows an actual implementation of a custom subgraph:</p> <pre><code>strategy&lt;String, String&gt;(\"my-strategy\") {\n   val mySubgraph by subgraph&lt;String, String&gt;(\n      tools = listOf(firstTool, secondTool)\n   ) {\n        // Define nodes and edges for this subgraph\n        val sendInput by nodeLLMRequest()\n        val executeToolCall by nodeExecuteTool()\n        val sendToolResult by nodeLLMSendToolResult()\n\n        edge(nodeStart forwardTo sendInput)\n        edge(sendInput forwardTo executeToolCall onToolCall { true })\n        edge(executeToolCall forwardTo sendToolResult)\n        edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    }\n}\n</code></pre>"},{"location":"custom-subgraphs/#configuring-tools-in-a-subgraph","title":"Configuring tools in a subgraph","text":"<p>Tools can be configured for a subgraph in several ways:</p> <ul> <li>Directly in the subgraph definition:</li> </ul> <pre><code>val mySubgraph by subgraph&lt;String, String&gt;(\n   tools = listOf(AskUser)\n ) {\n    // Subgraph definition\n }\n</code></pre> <ul> <li>From a tool registry:</li> </ul> <pre><code>val mySubgraph by subgraph&lt;String, String&gt;(\n    tools = listOf(toolRegistry.getTool(\"AskUser\"))\n) {\n    // Subgraph definition\n}\n</code></pre> <ul> <li>Dynamically during execution:</li> </ul> <pre><code>// Make a set of tools\nthis.llm.writeSession {\n    tools = tools.filter { it.name in listOf(\"first_tool_name\", \"second_tool_name\") }\n}\n</code></pre>"},{"location":"custom-subgraphs/#advanced-subgraph-techniques","title":"Advanced subgraph techniques","text":""},{"location":"custom-subgraphs/#multi-part-strategies","title":"Multi-part strategies","text":"<p>Complex workflows can be broken down into multiple subgraphs, each handling a specific part of the process:</p> <pre><code>strategy(\"complex-workflow\") {\n   val inputProcessing by subgraph&lt;String, A&gt;(\n   ) {\n      // Process the initial input\n   }\n\n   val reasoning by subgraph&lt;A, B&gt;(\n   ) {\n      // Perform reasoning based on the processed input\n   }\n\n   val toolRun by subgraph&lt;B, C&gt;(\n      // Optional subset of tools from the tool registry\n      tools = listOf(firstTool, secondTool)\n   ) {\n      // Run tools based on the reasoning\n   }\n\n   val responseGeneration by subgraph&lt;C, String&gt;(\n   ) {\n      // Generate a response based on the tool results\n   }\n\n   nodeStart then inputProcessing then reasoning then toolRun then responseGeneration then nodeFinish\n\n}\n</code></pre>"},{"location":"custom-subgraphs/#best-practices","title":"Best practices","text":"<p>When working with subgraphs, follow these best practices:</p> <ol> <li> <p>Break complex workflows into subgraphs: each subgraph should have a clear, focused responsibility.</p> </li> <li> <p>Pass only necessary context: only pass the information that subsequent subgraphs need to function correctly.</p> </li> <li> <p>Document subgraph dependencies: clearly document what each subgraph expects from previous subgraphs and what it provides to subsequent subgraphs.</p> </li> <li> <p>Test subgraphs in isolation: ensure that each subgraph works correctly with various inputs before integrating it into a strategy.</p> </li> <li> <p>Consider token usage: be mindful of token usage, especially when passing large histories between subgraphs.</p> </li> </ol>"},{"location":"custom-subgraphs/#troubleshooting","title":"Troubleshooting","text":""},{"location":"custom-subgraphs/#tools-not-available","title":"Tools not available","text":"<p>If tools are not available in a subgraph:</p> <ul> <li>Check that the tools are correctly registered in the tool registry.</li> </ul>"},{"location":"custom-subgraphs/#subgraphs-not-running-in-the-defined-and-expected-order","title":"Subgraphs not running in the defined and expected order","text":"<p>If subgraphs are not executing in the defined order:</p> <ul> <li>Check the strategy definition to ensure that subgraphs are listed in the correct order.</li> <li>Verify that each subgraph is correctly passing its output to the next subgraph.</li> <li>Ensure that your subgraph is connected with the rest of the subgraph and is reachable from the start (and finish). Be careful with conditional edges, so they cover all possible conditions to continue in order not to get blocked in a subgraph or node.</li> </ul>"},{"location":"custom-subgraphs/#examples","title":"Examples","text":"<p>The following example shows how subgraphs are used to create an agent strategy in a real-world scenario. The code sample includes three defined subgraphs, <code>researchSubgraph</code>, <code>planSubgraph</code>, and <code>executeSubgraph</code>, where each of the subgraphs has a defined and distinct purpose within the assistant flow.</p> <pre><code>// Define the agent strategy\nval strategy = strategy&lt;String, String&gt;(\"assistant\") {\n    // A subgraph that includes a tool call\n\n    val researchSubgraph by subgraph&lt;String, String&gt;(\n        \"research_subgraph\",\n        tools = listOf(WebSearchTool())\n    ) {\n        val nodeCallLLM by nodeLLMRequest(\"call_llm\")\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n\n        edge(nodeStart forwardTo nodeCallLLM)\n        edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeExecuteTool forwardTo nodeSendToolResult)\n        edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    }\n\n    val planSubgraph by subgraph(\n        \"plan_subgraph\",\n        tools = listOf()\n    ) {\n        val nodeUpdatePrompt by node&lt;String, Unit&gt; { research -&gt;\n            llm.writeSession {\n                rewritePrompt {\n                    prompt(\"research_prompt\") {\n                        system(\n                            \"You are given a problem and some research on how it can be solved.\" +\n                                    \"Make step by step a plan on how to solve given task.\"\n                        )\n                        user(\"Research: $research\")\n                    }\n                }\n            }\n        }\n        val nodeCallLLM by nodeLLMRequest(\"call_llm\")\n\n        edge(nodeStart forwardTo nodeUpdatePrompt)\n        edge(nodeUpdatePrompt forwardTo nodeCallLLM transformed { \"Task: $agentInput\" })\n        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    }\n\n    val executeSubgraph by subgraph&lt;String, String&gt;(\n        \"execute_subgraph\",\n        tools = listOf(DoAction(), DoAnotherAction()),\n    ) {\n        val nodeUpdatePrompt by node&lt;String, Unit&gt; { plan -&gt;\n            llm.writeSession {\n                rewritePrompt {\n                    prompt(\"execute_prompt\") {\n                        system(\n                            \"You are given a task and detailed plan how to execute it.\" +\n                                    \"Perform execution by calling relevant tools.\"\n                        )\n                        user(\"Execute: $plan\")\n                        user(\"Plan: $plan\")\n                    }\n                }\n            }\n        }\n        val nodeCallLLM by nodeLLMRequest(\"call_llm\")\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n\n        edge(nodeStart forwardTo nodeUpdatePrompt)\n        edge(nodeUpdatePrompt forwardTo nodeCallLLM transformed { \"Task: $agentInput\" })\n        edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeExecuteTool forwardTo nodeSendToolResult)\n        edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n        edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    }\n\n    nodeStart then researchSubgraph then planSubgraph then executeSubgraph then nodeFinish\n}\n</code></pre>"},{"location":"data-transfer-between-nodes/","title":"Data transfer between nodes","text":""},{"location":"data-transfer-between-nodes/#overview","title":"Overview","text":"<p>Koog provides a way to store and pass data using <code>AIAgentStorage</code>, which is a key-value storage system designed as a type-safe way to pass data between different nodes or even subgraphs.</p> <p>The storage is accessible through the <code>storage</code> property (<code>storage: AIAgentStorage</code>) available in agent nodes, allowing for seamless data sharing across different components of your AI agent system.</p>"},{"location":"data-transfer-between-nodes/#key-and-value-structure","title":"Key and value structure","text":"<p>The key-value data storage structure relies on the <code>AIAgentStorageKey</code> data class. For more information about <code>AIAgentStorageKey</code>, see the sections below.</p>"},{"location":"data-transfer-between-nodes/#aiagentstoragekey","title":"AIAgentStorageKey","text":"<p>The storage uses a typed key system to ensure type safety when storing and retrieving data:</p> <ul> <li><code>AIAgentStorageKey&lt;T&gt;</code>: A data class that represents a storage key used for identifying and accessing data. Here are   the key features of the <code>AIAgentStorageKey</code> class:<ul> <li>The generic type parameter <code>T</code> specifies the type of data associated with this key, ensuring type safety.</li> <li>Each key has a <code>name</code> property which is a string identifier that uniquely represents the storage key.</li> </ul> </li> </ul>"},{"location":"data-transfer-between-nodes/#usage-examples","title":"Usage examples","text":"<p>The following sections provide an actual example of creating a storage key and using it to store and retrieve data.</p>"},{"location":"data-transfer-between-nodes/#defining-a-class-that-represents-your-data","title":"Defining a class that represents your data","text":"<p>The first step in storing data that you want to pass is creating a class that represents your data. Here is an example of a simple class with basic user data:</p> <pre><code>class UserData(\n   val name: String,\n   val age: Int\n)\n</code></pre> <p>Once defined, use the class to create a storage key as described below.</p>"},{"location":"data-transfer-between-nodes/#creating-a-storage-key","title":"Creating a storage key","text":"<p>Create a typed storage key for the defined data structure:</p> <pre><code>val userDataKey = createStorageKey&lt;UserData&gt;(\"user-data\")\n</code></pre> <p>The <code>createStorageKey</code> function takes a single string parameter that uniquely identifies the key.</p>"},{"location":"data-transfer-between-nodes/#storing-data","title":"Storing data","text":"<p>To save data using a created storage key, use the <code>storage.set(key: AIAgentStorageKey&lt;T&gt;, value: T)</code> method in a node:</p> <pre><code>val nodeSaveData by node&lt;Unit, Unit&gt; {\n    storage.set(userDataKey, UserData(\"John\", 26))\n}\n</code></pre>"},{"location":"data-transfer-between-nodes/#retrieving-data","title":"Retrieving data","text":"<p>To retrieve the data, use the <code>storage.get</code> method in a node:</p> <pre><code>val nodeRetrieveData by node&lt;String, Unit&gt; { message -&gt;\n    storage.get(userDataKey)?.let { userFromStorage -&gt;\n        println(\"Hello dear $userFromStorage, here's a message for you: $message\")\n    }\n}\n</code></pre>"},{"location":"data-transfer-between-nodes/#api-documentation","title":"API documentation","text":"<p>For a complete reference related to the <code>AIAgentStorage</code> class, see AIAgentStorage.</p> <p>For individual functions available in the <code>AIAgentStorage</code> class, see the following API references:</p> <ul> <li>clear</li> <li>get</li> <li>getValue</li> <li>putAll</li> <li>remove</li> <li>set</li> <li>toMap</li> </ul>"},{"location":"data-transfer-between-nodes/#additional-information","title":"Additional information","text":"<ul> <li><code>AIAgentStorage</code> is thread-safe, using a Mutex to ensure concurrent access is handled properly.</li> <li>The storage is designed to work with any type that extends <code>Any</code>.</li> <li>When retrieving values, type casting is handled automatically, ensuring type safety throughout your application.</li> <li>For non-nullable access to values, use the <code>getValue</code> method which throws an exception if the key does not exist.</li> <li>You can clear the storage entirely using the <code>clear</code> method, which removes all stored key-value pairs.</li> </ul>"},{"location":"embeddings/","title":"Embeddings","text":"<p>The <code>embeddings</code> module provides functionality for generating and comparing embeddings of text and code. Embeddings are vector representations that capture semantic meaning, allowing for efficient similarity comparisons.</p>"},{"location":"embeddings/#overview","title":"Overview","text":"<p>This module consists of two main components:</p> <ol> <li>embeddings-base: core interfaces and data structures for embeddings.</li> <li>embeddings-llm: implementation using Ollama for local embedding generation.</li> </ol>"},{"location":"embeddings/#getting-started","title":"Getting started","text":"<p>The following sections include basic examples of how to use embeddings in the following ways:</p> <ul> <li>With a local embedding models through Ollama</li> <li>Using an OpenAI embedding model</li> </ul>"},{"location":"embeddings/#local-embeddings","title":"Local embeddings","text":"<p>To use the embedding functionality with a local model, you need to have Ollama installed and running on your system. For installation and running instructions, refer to the official Ollama GitHub repository.</p> <pre><code>fun main() {\n    runBlocking {\n        // Create an OllamaClient instance\n        val client = OllamaClient()\n        // Create an embedder\n        val embedder = LLMEmbedder(client, OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\n        // Create embeddings\n        val embedding = embedder.embed(\"This is the text to embed\")\n        // Print embeddings to the output\n        println(embedding)\n    }\n}\n</code></pre> <p>To use an Ollama embedding model, make sure to have the following prerequisites:</p> <ul> <li>Have Ollama installed and running</li> <li>Download an embedding model to your local machine using the following command:     <pre><code>ollama pull &lt;ollama-model-id&gt;\n</code></pre>     Replace <code>&lt;ollama-model-id&gt;</code> with the Ollama identifier of the specific model. For more information about available embedding models and their identifiers, see Ollama models overview.</li> </ul>"},{"location":"embeddings/#ollama-models-overview","title":"Ollama models overview","text":"<p>The following table provides an overview of the available Ollama embedding models.</p> Model ID Ollama ID Parameters Dimensions Context Length Performance Tradeoffs NOMIC_EMBED_TEXT nomic-embed-text 137M 768 8192 High-quality embeddings for semantic search and text similarity tasks Balanced between quality and efficiency ALL_MINILM all-minilm 33M 384 512 Fast inference with good quality for general text embeddings Smaller model size with reduced context length, but very efficient MULTILINGUAL_E5 zylonai/multilingual-e5-large 300M 768 512 Strong performance across 100+ languages Larger model size but provides excellent multilingual capabilities BGE_LARGE bge-large 335M 1024 512 Excellent for English text retrieval and semantic search Larger model size but provides high-quality embeddings MXBAI_EMBED_LARGE mxbai-embed-large - - - High-dimensional embeddings of textual data Designed for creating high-dimensional embeddings <p>For more information about these models, see Ollama's Embedding Models blog post.</p>"},{"location":"embeddings/#choosing-a-model","title":"Choosing a model","text":"<p>Here are some general tips on which Ollama embedding model to select depending on your requirements:</p> <ul> <li>For general text embeddings, use <code>NOMIC_EMBED_TEXT</code>.</li> <li>For multilingual support, use <code>MULTILINGUAL_E5</code>.</li> <li>For maximum quality (at the cost of performance), use <code>BGE_LARGE</code>.</li> <li>For maximum efficiency (at the cost of some quality), use <code>ALL_MINILM</code>.</li> <li>For high-dimensional embeddings, use <code>MXBAI_EMBED_LARGE</code>.</li> </ul>"},{"location":"embeddings/#openai-embeddings","title":"OpenAI embeddings","text":"<p>To create embeddings using an OpenAI embedding model, use the <code>embed</code> method of an <code>OpenAILLMClient</code> instance as shown in the example below.</p> <pre><code>suspend fun openAIEmbed(text: String) {\n    // Get the OpenAI API token from the OPENAI_KEY environment variable\n    val token = System.getenv(\"OPENAI_KEY\") ?: error(\"Environment variable OPENAI_KEY is not set\")\n    // Create an OpenAILLMClient instance\n    val client = OpenAILLMClient(token)\n    // Create an embedder\n    val embedder = LLMEmbedder(client, OpenAIModels.Embeddings.TextEmbeddingAda002)\n    // Create embeddings\n    val embedding = embedder.embed(text)\n    // Print embeddings to the output\n    println(embedding)\n}\n</code></pre>"},{"location":"embeddings/#examples","title":"Examples","text":"<p>The following examples show how you can use embeddings to compare code with text or other code snippets.</p>"},{"location":"embeddings/#code-to-text-comparison","title":"Code-to-text comparison","text":"<p>Compare code snippets with natural language descriptions to find semantic matches:</p> <pre><code>suspend fun compareCodeToText(embedder: Embedder) { // Embedder type\n    // Code snippet\n    val code = \"\"\"\n        fun factorial(n: Int): Int {\n            return if (n &lt;= 1) 1 else n * factorial(n - 1)\n        }\n    \"\"\".trimIndent()\n\n    // Text descriptions\n    val description1 = \"A recursive function that calculates the factorial of a number\"\n    val description2 = \"A function that sorts an array of integers\"\n\n    // Generate embeddings\n    val codeEmbedding = embedder.embed(code)\n    val desc1Embedding = embedder.embed(description1)\n    val desc2Embedding = embedder.embed(description2)\n\n    // Calculate differences (lower value means more similar)\n    val diff1 = embedder.diff(codeEmbedding, desc1Embedding)\n    val diff2 = embedder.diff(codeEmbedding, desc2Embedding)\n\n    println(\"Difference between code and description 1: $diff1\")\n    println(\"Difference between code and description 2: $diff2\")\n\n    // The code should be more similar to description1 than description2\n    if (diff1 &lt; diff2) {\n        println(\"The code is more similar to: '$description1'\")\n    } else {\n        println(\"The code is more similar to: '$description2'\")\n    }\n}\n</code></pre>"},{"location":"embeddings/#code-to-code-comparison","title":"Code-to-code comparison","text":"<p>Compare code snippets to find semantic similarities regardless of syntax differences:</p> <pre><code>suspend fun compareCodeToCode(embedder: Embedder) { // Embedder type\n    // Two implementations of the same algorithm in different languages\n    val kotlinCode = \"\"\"\n        fun fibonacci(n: Int): Int {\n            return if (n &lt;= 1) n else fibonacci(n - 1) + fibonacci(n - 2)\n        }\n    \"\"\".trimIndent()\n\n    val pythonCode = \"\"\"\n        def fibonacci(n):\n            if n &lt;= 1:\n                return n\n            else:\n                return fibonacci(n-1) + fibonacci(n-2)\n    \"\"\".trimIndent()\n\n    val javaCode = \"\"\"\n        public static int bubbleSort(int[] arr) {\n            int n = arr.length;\n            for (int i = 0; i &lt; n-1; i++) {\n                for (int j = 0; j &lt; n-i-1; j++) {\n                    if (arr[j] &gt; arr[j+1]) {\n                        int temp = arr[j];\n                        arr[j] = arr[j+1];\n                        arr[j+1] = temp;\n                    }\n                }\n            }\n            return arr;\n        }\n    \"\"\".trimIndent()\n\n    // Generate embeddings\n    val kotlinEmbedding = embedder.embed(kotlinCode)\n    val pythonEmbedding = embedder.embed(pythonCode)\n    val javaEmbedding = embedder.embed(javaCode)\n\n    // Calculate differences\n    val diffKotlinPython = embedder.diff(kotlinEmbedding, pythonEmbedding)\n    val diffKotlinJava = embedder.diff(kotlinEmbedding, javaEmbedding)\n\n    println(\"Difference between Kotlin and Python implementations: $diffKotlinPython\")\n    println(\"Difference between Kotlin and Java implementations: $diffKotlinJava\")\n\n    // The Kotlin and Python implementations should be more similar\n    if (diffKotlinPython &lt; diffKotlinJava) {\n        println(\"The Kotlin code is more similar to the Python code\")\n    } else {\n        println(\"The Kotlin code is more similar to the Java code\")\n    }\n}\n</code></pre>"},{"location":"embeddings/#api-documentation","title":"API documentation","text":"<p>For a complete API reference related to embeddings, see the reference documentation for the following modules:</p> <ul> <li>embeddings-base: Provides core interfaces and data structures for representing and comparing text  and code embeddings.</li> <li>embeddings-llm: Includes implementations for working with local embedding models.</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>The Koog framework provides examples to help you understand how to implement AI agents for different use cases. These examples demonstrate key features and patterns that you can adapt for your own applications.</p> <p>Browse the examples below and click on the links to view the source code on GitHub.</p> Example Description Attachments Learn how to use structured Markdown and attachments in prompts. Build prompts that include images and generate creative content for Instagram posts using OpenAI models. Banking Build a comprehensive AI banking assistant with routing capabilities that can handle money transfers and transaction analysis through a sophisticated graph-based strategy. Includes domain modeling, tool creation, and agent composition patterns. BedrockAgent Create intelligent AI agents using the Koog framework with AWS Bedrock integration. Learn how to define custom tools, set up AWS Bedrock, and build interactive agents that understand natural language commands for controlling devices. Calculator Build a calculator agent that performs arithmetic operations using tools for addition, subtraction, multiplication, and division. Demonstrates parallel tool calls, event logging, and multiple executor support (OpenAI and Ollama). Chess Build an intelligent chess-playing agent featuring complex domain modeling, custom tools, memory optimization techniques, and interactive choice selection. Demonstrates advanced agent strategies, game state management, and human-AI collaboration patterns. GoogleMapsMcp Connect Koog to a Google Maps MCP server via Docker. Discover tools, geocode addresses, and fetch elevation data using AI agents with real-world geographic APIs in a Kotlin Notebook environment. Guesser Build a number-guessing agent that implements a binary search strategy using tools to ask targeted questions. The agent efficiently narrows down the user's number through strategic questioning and demonstrates tool-based interaction patterns. Langfuse Learn how to export Koog agent traces to Langfuse using OpenTelemetry. Set up environment variables, run agents, and inspect spans and traces in your Langfuse instance for comprehensive observability. MCP Integration examples for the Model Context Protocol, featuring GoogleMapsMcpClient for geographic data and PlaywrightMcpClient for browser automation. Memory A customer support agent that demonstrates memory system usage. The agent tracks user conversation preferences, device diagnostics, and organization-specific information using encrypted local storage and proper memory organization with subjects and scopes. OpenTelemetry Add OpenTelemetry-based tracing to Koog AI agents. Learn to emit spans to console for debugging and export traces to OpenTelemetry Collector for viewing in Jaeger. Includes Docker setup and troubleshooting guide. Planner A task planning system that builds execution trees with parallel and sequential execution nodes, dynamically constructing execution plans for complex workflows. PlaywrightMcp Drive browsers with Playwright MCP and Koog. Launch a Playwright MCP server, connect via SSE, and let AI agents automate web tasks like navigation, cookie acceptance, and UI interaction through natural language commands. SimpleAPI Basic examples demonstrating chat agents and single-run agents with simple API patterns for getting started with Koog. StructuredData Demonstrates JSON-based structured data output with complex nested classes, polymorphism, and weather forecast examples showing how to work with typed data in agent responses. SubgraphWithTask Project generation tools showcasing file and directory operations, including creation, deletion, and command execution using subgraph strategies. Tone A text tone analysis agent that uses specialized tools to identify positive, negative, or neutral tones in input text, demonstrating sentiment analysis capabilities. UnityMcp Drive Unity game development with AI agents using Unity MCP server integration. Connect to Unity via stdio, discover available tools, and let agents modify scenes, place objects, and execute game development tasks through natural language commands. VaccumAgent Implementation of a basic reflex agent using the Koog framework. Covers environment modeling, tool creation, and agent behavior for automated cleaning tasks in a simple two-cell world. Weave Learn how to trace Koog agents to W&amp;B Weave using OpenTelemetry (OTLP). Set up environment variables, run agents, and view rich traces in the Weave UI for comprehensive monitoring and debugging."},{"location":"features-overview/","title":"Overview","text":"<p>Agent features provide a way to extend and enhance the functionality of AI agents. Features can:</p> <ul> <li>Add new capabilities to agents</li> <li>Intercept and modify agent behavior</li> <li>Log and monitor agent execution</li> </ul> <p>The Koog framework implements the following features:</p> <ul> <li>Event Handler</li> <li>Tracing</li> <li>Agent Memory</li> </ul>"},{"location":"history-compression/","title":"History compression","text":"<p>AI agents maintain a message history that includes user messages, assistant responses, tool calls, and tool responses. This history grows with each interaction as the agent follows its strategy.</p> <p>For long-running conversations, the history can become large and consume a lot of tokens. History compression helps reduce this by summarizing the full list of messages into one or several messages that contain only important information necessary for further agent operation.</p> <p>History compression addresses key challenges in agent systems:</p> <ul> <li>Optimizes context usage. Focused and smaller contexts improve LLM performance and prevent failures from exceeding token limits.</li> <li>Improves performance. Compressing history reduces the number of messages the LLM processes, resulting in faster responses.</li> <li>Enhances accuracy. Focusing on relevant information helps the LLM remain focused and complete tasks without distractions.</li> <li>Reduces costs. Reducing irrelevant messages lowers token usage, decreasing the overall cost of API calls.</li> </ul>"},{"location":"history-compression/#when-to-compress-history","title":"When to compress history","text":"<p>History compression is performed at specific steps in the agent workflow:</p> <ul> <li>Between logical steps (subgraphs) of the agent strategy.</li> <li>When context becomes too long.</li> </ul>"},{"location":"history-compression/#history-compression-implementation","title":"History compression implementation","text":"<p>There are two main approaches to implementing history compression in your agent:</p> <ul> <li>In a strategy graph.</li> <li>In a custom node.</li> </ul>"},{"location":"history-compression/#history-compression-in-a-strategy-graph","title":"History compression in a strategy graph","text":"<p>To compress the history in a strategy graph, you need to use the <code>nodeLLMCompressHistory</code> node. Depending on which step you decide to perform compression, the following scenarios are available: </p> <ul> <li>To compress the history when it becomes too long, you can define a helper function and add the <code>nodeLLMCompressHistory</code> node to your strategy graph with the following logic:</li> </ul> <pre><code>// Define that the history is too long if there are more than 100 messages\nprivate suspend fun AIAgentContextBase.historyIsTooLong(): Boolean = llm.readSession { prompt.messages.size &gt; 100 }\n\nval strategy = strategy&lt;String, String&gt;(\"execute-with-history-compression\") {\n    val callLLM by nodeLLMRequest()\n    val executeTool by nodeExecuteTool()\n    val sendToolResult by nodeLLMSendToolResult()\n\n    // Compress the LLM history and keep the current ReceivedToolResult for the next node\n    val compressHistory by nodeLLMCompressHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo callLLM)\n    edge(callLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(callLLM forwardTo executeTool onToolCall { true })\n\n    // Compress history after executing any tool if the history is too long \n    edge(executeTool forwardTo compressHistory onCondition { historyIsTooLong() })\n    edge(compressHistory forwardTo sendToolResult)\n    // Otherwise, proceed to the next LLM request\n    edge(executeTool forwardTo sendToolResult onCondition { !historyIsTooLong() })\n\n    edge(sendToolResult forwardTo executeTool onToolCall { true })\n    edge(sendToolResult forwardTo nodeFinish onAssistantMessage { true })\n}\n</code></pre> <p>In this example, the strategy checks if the history is too long after each tool call. The history is compressed before sending the tool result back to the LLM. This prevents the context from growing during long conversations.</p> <ul> <li>To compress the history between the logical steps (subgraphs) of your strategy, you can implement your strateg as follows:</li> </ul> <pre><code>val strategy = strategy&lt;String, String&gt;(\"execute-with-history-compression\") {\n    val collectInformation by subgraph&lt;String, String&gt; {\n        // Some steps to collect the information\n    }\n    val compressHistory by nodeLLMCompressHistory&lt;String&gt;()\n    val makeTheDecision by subgraph&lt;String, String&gt; {\n        // Some steps to make the decision based on the current compressed history and collected information\n    }\n\n    nodeStart then collectInformation then compressHistory then makeTheDecision\n}\n</code></pre> <p>In this example, the history is compressed after completing the information collection phase, but before proceeding to the decision-making phase.</p>"},{"location":"history-compression/#history-compression-in-a-custom-node","title":"History compression in a custom node","text":"<p>If you are implementing a custom node, you can compress history using the <code>replaceHistoryWithTLDR()</code> function as follows:</p> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR()\n}\n</code></pre> <p>This approach gives you more flexibility to implement compression at any point in your custom node logic, based on your specific requirements.</p> <p>To learn more about custom nodes, see Custom nodes.</p>"},{"location":"history-compression/#history-compression-strategies","title":"History compression strategies","text":"<p>You can customize the compression process by passing an optional <code>strategy</code> parameter to <code>nodeLLMCompressHistory(strategy=...)</code> or to <code>replaceHistoryWithTLDR(strategy=...)</code>. The framework provides several built-in strategies.</p>"},{"location":"history-compression/#wholehistory-default","title":"WholeHistory (Default)","text":"<p>The default strategy that compresses the entire history into one TLDR message that summarizes what has been achieved so far. This strategy works well for most general use cases where you want to maintain awareness of the entire conversation context while reducing token usage.</p> <p>You can use it as follows: </p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.WholeHistory\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.WholeHistory)\n}\n</code></pre>"},{"location":"history-compression/#fromlastnmessages","title":"FromLastNMessages","text":"<p>The strategy compresses only the last <code>n</code> messages into a TLDR message and completely discards earlier messages. This is useful when only the latest achievements of the agent (or the latest discovered facts, the latest context) are relevant for solving the problem.</p> <p>You can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.FromLastNMessages(5)\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.FromLastNMessages(5))\n}\n</code></pre>"},{"location":"history-compression/#chunked","title":"Chunked","text":"<p>The strategy splits the whole message history into chunks of a fixed size and compresses each chunk independently into a TLDR message. This is useful when you need not only the concise TLDR of what has been done so far but also want to keep track of the overall progress, and some older information might also be important.</p> <p>You can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.Chunked(10)\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = HistoryCompressionStrategy.Chunked(10))\n}\n</code></pre>"},{"location":"history-compression/#retrievefactsfromhistory","title":"RetrieveFactsFromHistory","text":"<p>The strategy searches for specific facts relevant to the provided list of concepts in the history and retrieves them. It changes the whole history to just these facts and leaves them as context for future LLM requests. This is useful when you have an idea of what exact facts will be relevant for the LLM to perform better on the task.</p> <p>You can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = RetrieveFactsFromHistory(\n        Concept(\n            keyword = \"user_preferences\",\n            // Description to the LLM -- what specifically to search for\n            description = \"User's preferences for the recommendation system, including the preferred conversation style, theme in the application, etc.\",\n            // LLM would search for multiple relevant facts related to this concept:\n            factType = FactType.MULTIPLE\n        ),\n        Concept(\n            keyword = \"product_details\",\n            // Description to the LLM -- what specifically to search for\n            description = \"Brief details about products in the catalog the user has been checking\",\n            // LLM would search for multiple relevant facts related to this concept:\n            factType = FactType.MULTIPLE\n        ),\n        Concept(\n            keyword = \"issue_solved\",\n            // Description to the LLM -- what specifically to search for\n            description = \"Was the initial user's issue resolved?\",\n            // LLM would search for a single answer to the question:\n            factType = FactType.SINGLE\n        )\n    )\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(\n        strategy = RetrieveFactsFromHistory(\n            Concept(\n                keyword = \"user_preferences\", \n                // Description to the LLM -- what specifically to search for\n                description = \"User's preferences for the recommendation system, including the preferred conversation style, theme in the application, etc.\",\n                // LLM would search for multiple relevant facts related to this concept:\n                factType = FactType.MULTIPLE\n            ),\n            Concept(\n                keyword = \"product_details\",\n                // Description to the LLM -- what specifically to search for\n                description = \"Brief details about products in the catalog the user has been checking\",\n                // LLM would search for multiple relevant facts related to this concept:\n                factType = FactType.MULTIPLE\n            ),\n            Concept(\n                keyword = \"issue_solved\",\n                // Description to the LLM -- what specifically to search for\n                description = \"Was the initial user's issue resolved?\",\n                // LLM would search for a single answer to the question:\n                factType = FactType.SINGLE\n            )\n        )\n    )\n}\n</code></pre>"},{"location":"history-compression/#custom-history-compression-strategy-implementation","title":"Custom history compression strategy implementation","text":"<p>You can create your own history compression strategy by extending the <code>HistoryCompressionStrategy</code> abstract class and implementing the <code>compress</code> method.</p> <p>Here is an example:</p> <pre><code>class MyCustomCompressionStrategy : HistoryCompressionStrategy() {\n    override suspend fun compress(\n        llmSession: AIAgentLLMWriteSession,\n        preserveMemory: Boolean,\n        memoryMessages: List&lt;Message&gt;\n    ) {\n        // 1. Process the current history in llmSession.prompt.messages\n        // 2. Create new compressed messages\n        // 3. Update the prompt with the compressed messages\n\n        // Example implementation:\n        val importantMessages = llmSession.prompt.messages.filter {\n            // Your custom filtering logic\n            it.content.contains(\"important\")\n        }.filterIsInstance&lt;Message.Response&gt;()\n\n        // Note: you can also make LLM requests using the `llmSession` and ask the LLM to do some job for you using, for example, `llmSession.requestLLMWithoutTools()`\n        // Or you can change the current model: `llmSession.model = AnthropicModels.Sonnet_3_7` and ask some other LLM model -- but don't forget to change it back after\n\n        // Compose the prompt with the filtered messages\n        composePromptWithRequiredMessages(\n            llmSession,\n            importantMessages,\n            preserveMemory,\n            memoryMessages\n        )\n    }\n}\n</code></pre> <p>In this example, the custom strategy filters messages that contain the word \"important\" and keeps only those in the compressed history.</p> <p>Then you can use it as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = MyCustomCompressionStrategy()\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(strategy = MyCustomCompressionStrategy())\n}\n</code></pre>"},{"location":"history-compression/#memory-preservation-during-compression","title":"Memory preservation during compression","text":"<p>All history compression methods have the <code>preserveMemory</code> parameter that determines whether memory-related messages should be preserved during compression. These are messages that contain facts retrieved from memory or indicate that the memory feature is not enabled.</p> <p>You can use the <code>preserveMemory</code> parameter as follows:</p> <ul> <li>In a strategy graph:</li> </ul> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;ProcessedInput&gt;(\n    strategy = HistoryCompressionStrategy.WholeHistory,\n    preserveMemory = true\n)\n</code></pre> <ul> <li>In a custom node:</li> </ul> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(\n        strategy = HistoryCompressionStrategy.WholeHistory,\n        preserveMemory = true\n    )\n}\n</code></pre>"},{"location":"key-concepts/","title":"Key concepts","text":""},{"location":"key-concepts/#agent","title":"Agent","text":"<ul> <li> <p>Agent: an AI entity that can interact with tools, handle complex workflows, and communicate with   users.</p> </li> <li> <p>LLM (Large Language Model): the underlying AI model that powers agent capabilities.</p> </li> <li> <p>Message: a unit of communication in the agent system that represents data passed from a user, assistant, or system.</p> </li> <li> <p>Prompt: the conversation history provided to an LLM that consists of messages from a user, assistant, and system.</p> </li> <li> <p>System prompt: instructions provided to an agent to guide its behavior, define its role, and supply key information necessary for its tasks.</p> </li> <li> <p>Context: the environment in which LLM interactions occur, with access to the conversation history and   tools.</p> </li> <li> <p>LLM session: a structured way to interact with LLMs that includes the conversation history, available tools,   and methods to make requests.</p> </li> </ul>"},{"location":"key-concepts/#agent-workflow","title":"Agent workflow","text":"<ul> <li>Strategy: a defined workflow for an agent that consists of sequential subgraphs. The strategy defines how the agent processes input, interacts with tools, and generates output. A strategy graph consists of nodes connected by edges that represent transitions between nodes.</li> </ul>"},{"location":"key-concepts/#strategy-graphs","title":"Strategy graphs","text":"<ul> <li> <p>Graph: a structure of nodes connected by edges that defines an agent strategy workflow.</p> </li> <li> <p>Node: a fundamental building block of an agent strategy workflow that represents a specific operation or transformation.</p> </li> <li> <p>Edge: a connection between nodes in an agent graph that defines the flow of operations, often with conditions   that specify when to follow each edge.</p> </li> <li> <p>Conditions: rules that determine when to follow a particular edge.</p> </li> <li> <p>Subgraph: a self-contained unit of processing within an agent strategy, with its own set of tools, context, and responsibilities. Information about subgraph operations can be either encapsulated within the subgraph or transferred between subgraphs using the AgentMemory feature.</p> </li> </ul>"},{"location":"key-concepts/#tools","title":"Tools","text":"<ul> <li> <p>Tool: a function that an agent can use to perform specific tasks or access external systems. The agent is aware of the available tools and their arguments but lacks knowledge of their implementation details.</p> </li> <li> <p>Tool call: a request from an LLM to run a specific tool using the provided arguments. It functions similarly to a function call.</p> </li> <li> <p>Tool descriptor: tool metadata that includes its name, description, and parameters.</p> </li> <li> <p>Tool registry: a list of tools available to an agent. The registry informs the agent about the available tools.</p> </li> <li> <p>Tool result: an output produced by running a tool. For example, if the tool is a method, the result would be its return value.</p> </li> </ul>"},{"location":"key-concepts/#history-compression","title":"History compression","text":"<ul> <li>History compression: the process of reducing the size of the conversation history to manage token usage by applying various compression strategies. To learn more, see History compression.</li> </ul>"},{"location":"key-concepts/#features","title":"Features","text":"<ul> <li>Feature: a component that extends and enhances the functionality of AI agents.</li> </ul>"},{"location":"key-concepts/#eventhandler-feature","title":"EventHandler feature","text":"<ul> <li>EventHandler: a feature that enables monitoring and responding to various agent events, providing hooks for tracking agent lifecycle, handling errors, and processing tool invocations    throughout the workflow.</li> </ul>"},{"location":"key-concepts/#agentmemory-feature","title":"AgentMemory feature","text":"<ul> <li> <p>AgentMemory: a feature that enables AI agents to store, retrieve, and use information across conversations. To learn more, see AgentMemory.</p> </li> <li> <p>Concept: a category of information with associated metadata in the AgentMemory feature, including a keyword, description, and fact type. Concepts are fundamental building blocks of the AgentMemory system that the agent can remember and recall. To learn more, see AgentMemory.</p> </li> <li> <p>Fact: an individual piece of information stored in the AgentMemory system. Facts are associated with concepts and can either have a single value or multiple values. To learn more, see AgentMemory.</p> </li> <li> <p>Memory scope: the context in which facts are relevant. To learn more, see AgentMemory.</p> </li> </ul>"},{"location":"ktor-plugin/","title":"Ktor Integration: Koog plugin","text":"<p>Koog fits naturally into your Ktor server allowing you to write server-side AI applications using ideomatic Kotlin API from both sides. </p> <p>Install the Koog plugin once, configure your LLM providers in application.conf/YAML or in code, and then call agents right from your routes. No more wiring LLM clients across modules \u2013 your routes just request an agent and are ready to go.</p>"},{"location":"ktor-plugin/#overview","title":"Overview","text":"<p>The <code>koog-ktor</code> module provides idiomatic Kotlin/Ktor integration for server\u2011side agentic development:</p> <ul> <li>Drop\u2011in Ktor plugin: <code>install(Koog)</code> in your Application</li> <li>First\u2011class support for OpenAI, Anthropic, Google, OpenRouter, DeepSeek, and Ollama</li> <li>Centralized configuration via YAML/CONF and/or code</li> <li>Agent setup with prompt, tools, features; simple extension functions for routes</li> <li>Direct LLM usage (execute, executeStreaming, moderate)</li> <li>JVM\u2011only Model Context Protocol (MCP) tools integration</li> </ul>"},{"location":"ktor-plugin/#add-dependency","title":"Add dependency","text":"<pre><code>dependencies {\n    implementation(\"ai.koog:koog-ktor:$koogVersion\")\n}\n</code></pre>"},{"location":"ktor-plugin/#quick-start","title":"Quick start","text":"<p>1) Configure providers (in <code>application.yaml</code> or <code>application.conf</code>)</p> <p>Use nested keys under <code>koog.&lt;provider&gt;</code>. The plugin automatically picks them up.</p> <pre><code># application.yaml (Ktor config)\nkoog:\n  openai:\n    apikey: ${OPENAI_API_KEY}\n    baseUrl: https://api.openai.com\n  anthropic:\n    apikey: ${ANTHROPIC_API_KEY}\n    baseUrl: https://api.anthropic.com\n  google:\n    apikey: ${GOOGLE_API_KEY}\n    baseUrl: https://generativelanguage.googleapis.com\n  openrouter:\n    apikey: ${OPENROUTER_API_KEY}\n    baseUrl: https://openrouter.ai\n  deepseek:\n    apikey: ${DEEPSEEK_API_KEY}\n    baseUrl: https://api.deepseek.com\n  # Ollama is enabled when any koog.ollama.* key exists\n  ollama:\n    enable: true\n    baseUrl: http://localhost:11434\n</code></pre> <p>Optional: configure fallback used by direct LLM calls when a requested provider is not configured.</p> <pre><code>koog:\n  llm:\n    fallback:\n      provider: openai\n      # see Model identifiers section below\n      model: openai.chat.gpt4_1\n</code></pre> <p>2) Install the plugin and define routes</p> <pre><code>fun Application.module() {\n    install(Koog) {\n        // You can also configure providers programmatically (see below)\n    }\n\n    routing {\n        route(\"/ai\") {\n            post(\"/chat\") {\n                val userInput = call.receiveText()\n                // Create and run a default single\u2011run agent using a specific model\n                val output = aiAgent(\n                    strategy = reActStrategy(),\n                    model = OpenAIModels.Chat.GPT4_1,\n                    input = userInput\n                )\n                call.respond(HttpStatusCode.OK, output)\n            }\n        }\n    }\n}\n</code></pre> <p>Notes - aiAgent requires a concrete model (LLModel) \u2013 choose per\u2011route, per\u2011use. - For lower\u2011level LLM access, use llm() (PromptExecutor) directly.</p>"},{"location":"ktor-plugin/#direct-llm-usage-from-routes","title":"Direct LLM usage from routes","text":"<pre><code>post(\"/llm-chat\") {\n    val userInput = call.receiveText()\n\n    val messages = llm().execute(\n        prompt(\"chat\") {\n            system(\"You are a helpful assistant that clarifies questions\")\n            user(userInput)\n        },\n        GoogleModels.Gemini2_5Pro\n    )\n\n    // Join all assistant messages into a single string\n    val text = messages.joinToString(separator = \"\") { it.content }\n    call.respond(HttpStatusCode.OK, text)\n}\n</code></pre> <p>Streaming</p> <pre><code>get(\"/stream\") {\n    val flow = llm().executeStreaming(\n        prompt(\"streaming\") { user(\"Stream this response, please\") },\n        OpenRouterModels.GPT4o\n    )\n\n    // Example: buffer and send as one chunk\n    val sb = StringBuilder()\n    flow.collect { chunk -&gt; sb.append(chunk) }\n    call.respondText(sb.toString())\n}\n</code></pre> <p>Moderation</p> <pre><code>post(\"/moderated-chat\") {\n    val userInput = call.receiveText()\n\n    val moderation = llm().moderate(\n        prompt(\"moderation\") { user(userInput) },\n        OpenAIModels.Moderation.Omni\n    )\n\n    if (moderation.isHarmful) {\n        call.respond(HttpStatusCode.BadRequest, \"Harmful content detected\")\n        return@post\n    }\n\n    val output = aiAgent(\n        strategy = reActStrategy(),\n        model = OpenAIModels.Chat.GPT4_1,\n        input = userInput\n    )\n    call.respond(HttpStatusCode.OK, output)\n}\n</code></pre>"},{"location":"ktor-plugin/#programmatic-configuration-in-code","title":"Programmatic configuration (in code)","text":"<p>All providers and agent behavior can be configured via install(Koog) {}.</p> <pre><code>install(Koog) {\n    llm {\n        openAI(apiKey = System.getenv(\"OPENAI_API_KEY\") ?: \"\") {\n            baseUrl = \"https://api.openai.com\"\n            timeouts { // Default values shown below\n                requestTimeout = 15.minutes\n                connectTimeout = 60.seconds\n                socketTimeout = 15.minutes\n            }\n        }\n        anthropic(apiKey = System.getenv(\"ANTHROPIC_API_KEY\") ?: \"\")\n        google(apiKey = System.getenv(\"GOOGLE_API_KEY\") ?: \"\")\n        openRouter(apiKey = System.getenv(\"OPENROUTER_API_KEY\") ?: \"\")\n        deepSeek(apiKey = System.getenv(\"DEEPSEEK_API_KEY\") ?: \"\")\n        ollama { baseUrl = \"http://localhost:11434\" }\n\n        // Optional fallback used by PromptExecutor when a provider isn\u2019t configured\n        fallback {\n            provider = LLMProvider.OpenAI\n            model = OpenAIModels.Chat.GPT4_1\n        }\n    }\n\n    agentConfig {\n        // Provide a reusable base prompt for your agents\n        prompt(name = \"agent\") {\n            system(\"You are a helpful server\u2011side agent\")\n        }\n\n        // Limit runaway tools/loops\n        maxAgentIterations = 10\n\n        // Register tools available to agents by default\n        registerTools {\n            // tool(::yourTool) // see Tools Overview for details\n        }\n\n        // Install agent features (tracing, etc.)\n        // install(OpenTelemetry) { /* ... */ }\n    }\n}\n</code></pre>"},{"location":"ktor-plugin/#model-identifiers-in-config-fallback","title":"Model identifiers in config (fallback)","text":"<p>When configuring llm.fallback in YAML/CONF, use these identifier formats: - OpenAI: openai.chat.gpt4_1, openai.reasoning.o3, openai.costoptimized.gpt4_1mini, openai.audio.gpt4oaudio, openai.moderation.omni - Anthropic: anthropic.sonnet_3_7, anthropic.opus_4, anthropic.haiku_3_5 - Google: google.gemini2_5pro, google.gemini2_0flash001 - OpenRouter: openrouter.gpt4o, openrouter.gpt4, openrouter.claude3sonnet - DeepSeek: deepseek.deepseek-chat, deepseek.deepseek-reasoner - Ollama: ollama.meta.llama3.2, ollama.alibaba.qwq:32b, ollama.groq.llama3-grok-tool-use:8b</p> <p>Note - For OpenAI you must include the category (chat, reasoning, costoptimized, audio, embeddings, moderation). - For Ollama, both ollama.model and ollama.. are supported."},{"location":"ktor-plugin/#mcp-tools-jvmonly","title":"MCP tools (JVM\u2011only)","text":"<p>On JVM you can add tools from an MCP server to your agent tool registry:</p> <pre><code>install(Koog) {\n    agentConfig {\n        mcp {\n            // Register via SSE\n            sse(\"https://your-mcp-server.com/sse\")\n\n            // Or register via spawned process (stdio transport)\n            // process(Runtime.getRuntime().exec(\"your-mcp-binary ...\"))\n\n            // Or from an existing MCP client instance\n            // client(existingMcpClient)\n        }\n    }\n}\n</code></pre>"},{"location":"ktor-plugin/#why-koog-ktor","title":"Why Koog + Ktor?","text":"<ul> <li>Kotlin\u2011first, type\u2011safe development of agents in your server</li> <li>Centralized config with clean, testable route code</li> <li>Use the right model per\u2011route, or fall back automatically for direct LLM calls</li> <li>Production\u2011ready features: tools, moderation, streaming, and tracing</li> </ul>"},{"location":"model-capabilities/","title":"Model capabilities","text":"<p>Koog provides a set of abstractions and implementations for working with Large Language Models (LLMs) from various  LLM providers in a provider-agnostic way. The set includes the following classes:</p> <ul> <li> <p>LLMCapability: a class hierarchy that defines various capabilities that LLMs can support, such as:</p> <ul> <li>Temperature adjustment for controlling response randomness</li> <li>Tool integration for external system interaction</li> <li>Vision processing for handling visual data</li> <li>Embedding generation for vector representations</li> <li>Completion for text generation tasks</li> <li>Schema support for structured data (JSON with Simple and Full variants)</li> <li>Speculation for exploratory responses</li> </ul> </li> <li> <p>LLModel: a data class that represents a specific LLM with its provider, unique identifier, and supported capabilities.</p> </li> </ul> <p>This serves as a foundation for interacting with different LLM providers in a unified way, allowing applications to work with various models while abstracting away provider-specific details.</p>"},{"location":"model-capabilities/#llm-capabilities","title":"LLM capabilities","text":"<p>LLM capabilities represent specific features or functionalities that a Large Language Model can support. In the Koog framework, capabilities are used to define what a particular model can do and how it can be configured. Each capability is represented as a subclass or data object of the <code>LLMCapability</code> class.</p> <p>When configuring an LLM for use in your application, you specify which capabilities it supports by adding them to the <code>capabilities</code> list when creating an <code>LLModel</code> instance. This allows the framework to properly interact with the model  and use its features appropriately.</p>"},{"location":"model-capabilities/#core-capabilities","title":"Core capabilities","text":"<p>The list below includes the core, LLM-specific capabilities that are available for models in the Koog framework:</p> <ul> <li> <p>Speculation (<code>LLMCapability.Speculation</code>): lets the model generate speculative or exploratory responses with  varying degrees of likelihood. Useful for creative or hypothetical scenarios where a broader range of potential outcomes  is desired.</p> </li> <li> <p>Temperature (<code>LLMCapability.Temperature</code>): allows adjustment of the model's response randomness or creativity  levels. Higher temperature values produce more diverse outputs, while lower values lead to more focused and  deterministic responses.</p> </li> <li> <p>Tools (<code>LLMCapability.Tools</code>): indicates support for external tool usage or integration. This capability lets the  model run specific tools or interact with external systems.</p> </li> <li> <p>Tool choice (<code>LLMCapability.ToolChoice</code>): configures how tool calling works with the LLM. Depending on the model,  it can be configured to:</p> <ul> <li>Automatically choose between generating text or tool calls</li> <li>Generate only tool calls, never text</li> <li>Generate only text, never tool calls</li> <li>Force calling a specific tool among the defined tools</li> </ul> </li> <li> <p>Multiple choices (<code>LLMCapability.MultipleChoices</code>): lets the model generate multiple independent reply choices to a single prompt.</p> </li> </ul>"},{"location":"model-capabilities/#media-processing-capabilities","title":"Media processing capabilities","text":"<p>The following list represents a set of capabilities for processing media content such as images or audio:</p> <ul> <li> <p>Vision (<code>LLMCapability.Vision</code>): a class for vision-based capabilities that process, analyze, and infer insights  from visual data. Supports the following types of visual data:</p> <ul> <li>Image (<code>LLMCapability.Vision.Image</code>): handles image-related vision tasks such as image analysis, recognition,  and interpretation.</li> <li>Video (<code>LLMCapability.Vision.Video</code>): processes video data, including analyzing and understanding video  content.</li> </ul> </li> <li> <p>Audio (<code>LLMCapability.Audio</code>): provides audio-related functionalities such as transcription, audio generation, or  audio-based interactions.</p> </li> <li> <p>Document (<code>LLMCapability.Document</code>): enables handling and processing of document-based inputs and outputs.</p> </li> </ul>"},{"location":"model-capabilities/#text-processing-capabilities","title":"Text processing capabilities","text":"<p>The following list of capabilities represents text generation and processing functionalities:</p> <ul> <li> <p>Embedding (<code>LLMCapability.Embed</code>): lets models generate vector embeddings from an input text, enabling similarity  comparisons, clustering, and other vector-based analyses.</p> </li> <li> <p>Completion (<code>LLMCapability.Completion</code>): includes the generation of text or content based on given input context,  such as completing sentences, generating suggestions, or producing content that aligns with input data.</p> </li> <li> <p>Prompt caching (<code>LLMCapability.PromptCaching</code>): supports caching functionalities for prompts, potentially improving performance for repeated or similar queries.</p> </li> <li> <p>Moderation (<code>LLMCapability.Moderation</code>): lets the model analyze text for potentially harmful content and  classify it according to various categories such as harassment, hate speech, self-harm, sexual content, violence, etc.</p> </li> </ul>"},{"location":"model-capabilities/#schema-capabilities","title":"Schema capabilities","text":"<p>The list below indicates the capabilities related to processing structured data:</p> <ul> <li>Schema (<code>LLMCapability.Schema</code>): a class for structured schema capabilities related to data interaction and  encoding using specific formats. Includes support for the following format:<ul> <li>JSON (<code>LLMCapability.Schema.JSON</code>): JSON schema support with different levels:<ul> <li>Basic (<code>LLMCapability.Schema.JSON.Basic</code>): provides lightweight or basic JSON processing capabilities.</li> <li>Standard (<code>LLMCapability.Schema.JSON.Standard</code>): offers comprehensive JSON schema support for complex data  structures.</li> </ul> </li> </ul> </li> </ul>"},{"location":"model-capabilities/#creating-a-model-llmodel-configuration","title":"Creating a model (LLModel) configuration","text":"<p>To define a model in a universal, provider-agnostic way, create a model configuration as an instance of the <code>LLModel</code> class with the following parameters:</p> Name Data type Required Default Description <code>provider</code> LLMProvider Yes The provider of the LLM, such as Google or OpenAI. This identifies the company or organization that created or hosts the model. <code>id</code> String Yes A unique identifier for the LLM instance. This typically represents the specific model version or name. For example, <code>gpt-4-turbo</code>, <code>claude-3-opus</code>, <code>llama-3-2</code>. <code>capabilities</code> List&lt;LLMCapability&gt; Yes A list of capabilities supported by the LLM, such as temperature adjustment, tools usage, or schema-based tasks. These capabilities define what the model can do and how it can be configured. <code>contextLength</code> Long Yes The context length of the LLM. This is the maximum number of tokens the LLM can process. <code>maxOutputTokens</code> Long No <code>null</code> The maximum number of tokens that can be generated by the provider for the LLM."},{"location":"model-capabilities/#examples","title":"Examples","text":"<p>This section provides detailed examples of creating <code>LLModel</code> instances with different capabilities.</p> <p>The code below represents a basic LLM configuration with core capabilities:</p> <pre><code>val basicModel = LLModel(\n    provider = LLMProvider.OpenAI,\n    id = \"gpt-4-turbo\",\n    capabilities = listOf(\n        LLMCapability.Temperature,\n        LLMCapability.Tools,\n        LLMCapability.Schema.JSON.Standard\n    ),\n    contextLength = 128_000\n)\n</code></pre> <p>The model configuration below is a multimodal LLM with vision capabilities:</p> <pre><code>val visionModel = LLModel(\n    provider = LLMProvider.OpenAI,\n    id = \"gpt-4-vision\",\n    capabilities = listOf(\n        LLMCapability.Temperature,\n        LLMCapability.Vision.Image,\n        LLMCapability.MultipleChoices\n    ),\n    contextLength = 1_047_576,\n    maxOutputTokens = 32_768\n)\n</code></pre> <p>An LLM with audio processing capabilities:</p> <pre><code>val audioModel = LLModel(\n    provider = LLMProvider.Anthropic,\n    id = \"claude-3-opus\",\n    capabilities = listOf(\n        LLMCapability.Audio,\n        LLMCapability.Temperature,\n        LLMCapability.PromptCaching\n    ),\n    contextLength = 200_000\n)\n</code></pre> <p>In addition to creating models as <code>LLModel</code> instances and having to specify all related parameters, Koog includes a collection of predefined models and their configurations with supported capabilities. To use a predefined Ollama model, specify it as follows:</p> <pre><code>val metaModel = OllamaModels.Meta.LLAMA_3_2\n</code></pre> <p>To check whether a model supports a specific capability use the <code>contains</code> method to check for the presence of the  capability in the <code>capabilities</code> list:</p> <pre><code>// Check if models support specific capabilities\nval supportsTools = basicModel.capabilities.contains(LLMCapability.Tools) // true\nval supportsVideo = visionModel.capabilities.contains(LLMCapability.Vision.Video) // false\n\n// Check for schema capabilities\nval jsonCapability = basicModel.capabilities.filterIsInstance&lt;LLMCapability.Schema.JSON&gt;().firstOrNull()\nval hasFullJsonSupport = jsonCapability is LLMCapability.Schema.JSON.Standard // true\n</code></pre>"},{"location":"model-capabilities/#llm-capabilities-by-model","title":"LLM capabilities by model","text":"<p>This reference shows which LLM capabilities are supported by each model across different providers.</p> <p>In the tables below:</p> <ul> <li><code>\u2713</code> indicates that the model supports the capability</li> <li><code>-</code> indicates that the model does not support the capability</li> <li>For JSON Schema, <code>Full</code> or <code>Simple</code> indicates which variant of the JSON Schema capability the model supports</li> </ul>"},{"location":"model-capabilities/#google-models","title":"Google models","text":"Model Temperature JSON Schema Completion Multiple Choices Tools Tool Choice Vision (Image) Vision (Video) Audio Gemini2_5Pro \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_5Flash \u2713 Full \u2713 \u2713 - - \u2713 \u2713 \u2713 Gemini2_0Flash \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_0Flash001 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_0FlashLite \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini2_0FlashLite001 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini1_5Pro \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini1_5ProLatest \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini1_5Pro002 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini1_5Flash \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 \u2713 Gemini1_5FlashLatest \u2713 Full \u2713 \u2713 - - \u2713 \u2713 \u2713 Gemini1_5Flash002 \u2713 Full \u2713 \u2713 - - \u2713 \u2713 \u2713 Gemini1_5Flash8B \u2713 Full \u2713 \u2713 - - \u2713 \u2713 \u2713 Gemini1_5Flash8B001 \u2713 Full \u2713 \u2713 - - \u2713 \u2713 \u2713 Gemini1_5Flash8BLatest \u2713 Full \u2713 \u2713 - - \u2713 \u2713 \u2713"},{"location":"model-capabilities/#openai-models","title":"OpenAI models","text":"Model Temperature JSON Schema Completion Multiple Choices Tools Tool Choice Vision (Image) Vision (Video) Audio Speculation Moderation Reasoning.GPT4oMini \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Reasoning.O3Mini \u2713 Full \u2713 \u2713 \u2713 \u2713 - - - \u2713 - Reasoning.O1Mini - Full \u2713 \u2713 - - - - - \u2713 - Reasoning.O3 - Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Reasoning.O1 - Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Chat.GPT4o \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Chat.GPT4_1 \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 - - \u2713 - Audio.GPT4oMiniAudio \u2713 - \u2713 - \u2713 \u2713 - - \u2713 - - Audio.GPT4oAudio \u2713 - \u2713 - \u2713 \u2713 - - \u2713 - - Moderation.Omni - - - - - - \u2713 - - - \u2713 Moderation.Text - - - - - - - - - - \u2713"},{"location":"model-capabilities/#anthropic-models","title":"Anthropic models","text":"Model Temperature JSON Schema Completion Tools Tool Choice Vision (Image) Opus_4 \u2713 Full \u2713 \u2713 \u2713 \u2713 Sonnet_4 \u2713 Full \u2713 \u2713 \u2713 \u2713 Sonnet_3_7 \u2713 Full \u2713 \u2713 \u2713 \u2713 Haiku_3_5 \u2713 Full \u2713 \u2713 \u2713 \u2713 Sonnet_3_5 \u2713 Full \u2713 \u2713 \u2713 \u2713 Haiku_3 \u2713 Full \u2713 \u2713 \u2713 \u2713 Opus_3 \u2713 Full \u2713 \u2713 \u2713 \u2713"},{"location":"model-capabilities/#ollama-models","title":"Ollama models","text":""},{"location":"model-capabilities/#meta-models","title":"Meta models","text":"Model Temperature JSON Schema Tools Moderation LLAMA_3_2_3B \u2713 Simple \u2713 - LLAMA_3_2 \u2713 Simple \u2713 - LLAMA_4 \u2713 Simple \u2713 - LLAMA_GUARD_3 - - - \u2713"},{"location":"model-capabilities/#alibaba-models","title":"Alibaba models","text":"Model Temperature JSON Schema Tools QWEN_2_5_05B \u2713 Simple \u2713 QWEN_3_06B \u2713 Simple \u2713 QWQ \u2713 Simple \u2713 QWEN_CODER_2_5_32B \u2713 Simple \u2713"},{"location":"model-capabilities/#groq-models","title":"Groq models","text":"Model Temperature JSON Schema Tools LLAMA_3_GROK_TOOL_USE_8B \u2713 Full \u2713 LLAMA_3_GROK_TOOL_USE_70B \u2713 Full \u2713"},{"location":"model-capabilities/#granite-models","title":"Granite models","text":"Model Temperature JSON Schema Tools Vision (Image) GRANITE_3_2_VISION \u2713 Simple \u2713 \u2713"},{"location":"model-capabilities/#openrouter-models","title":"OpenRouter models","text":"Model Temperature JSON Schema Completion Speculation Tools Tool Choice Vision (Image) Phi4Reasoning \u2713 Full \u2713 \u2713 \u2713 \u2713 - Claude3Opus \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3Sonnet \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3Haiku \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 GPT4 \u2713 Full \u2713 \u2713 \u2713 \u2713 - GPT4o \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 GPT4Turbo \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 GPT35Turbo \u2713 Full \u2713 \u2713 \u2713 \u2713 - Gemini15Pro \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Gemini15Flash \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Llama3 \u2713 Full \u2713 \u2713 \u2713 \u2713 - Llama3Instruct \u2713 Full \u2713 \u2713 \u2713 \u2713 - Mistral7B \u2713 Full \u2713 \u2713 \u2713 \u2713 - Mixtral8x7B \u2713 Full \u2713 \u2713 \u2713 \u2713 - Claude3VisionSonnet \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3VisionOpus \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713 Claude3VisionHaiku \u2713 Full \u2713 \u2713 \u2713 \u2713 \u2713"},{"location":"model-context-protocol/","title":"Model Context Protocol","text":"<p>Model Context Protocol (MCP) is a standardized protocol that lets AI agents interact with external tools and services through a consistent interface.</p> <p>MCP exposes tools and prompts as API endpoints that AI agents can call. Each tool has a specific name and an input schema that describes its inputs and outputs using the JSON Schema format.</p> <p>The Koog framework provides integration with MCP servers, enabling you to incorporate MCP tools into your Koog agents.</p> <p>To learn more about the protocol, see the Model Context Protocol documentation.</p>"},{"location":"model-context-protocol/#mcp-servers","title":"MCP servers","text":"<p>MCP servers implement Model Context Protocol and provide a standardized way for AI agents to interact with tools and services.</p> <p>You can find ready-to-use MCP servers in the MCP Marketplace or MCP DockerHub.</p> <p>The MCP servers support the following transport protocols to communicate with agents:</p> <ul> <li>Standard input/output (stdio) transport protocol used to communicate with the MCP servers running as separate processes. For example, a Docker container or a CLI tool.</li> <li>Server-sent events (SSE) transport protocol (optional) used to communicate with the MCP servers over HTTP.</li> </ul>"},{"location":"model-context-protocol/#integration-with-koog","title":"Integration with Koog","text":"<p>The Koog framework integrates with MCP using the MCP SDK with the additional API extensions presented in the <code>agent-mcp</code> module.</p> <p>This integration lets the Koog agents perform the following:</p> <ul> <li>Connect to MCP servers through various transport mechanisms (stdio, SSE).</li> <li>Retrieve available tools from an MCP server.</li> <li>Transform MCP tools into the Koog tool interface.</li> <li>Register the transformed tools in a tool registry.</li> <li>Call MCP tools with arguments provided by the LLM.</li> </ul>"},{"location":"model-context-protocol/#key-components","title":"Key components","text":"<p>Here are the main components of the MCP integration in Koog:</p> Component Description <code>McpTool</code> Serves as a bridge between the Koog tool interface and the MCP SDK. <code>McpToolDescriptorParser</code> Parses MCP tool definitions into the Koog tool descriptor format. <code>McpToolRegistryProvider</code> Creates MCP tool registries that connect to MCP servers through various transport mechanisms (stdio, SSE)."},{"location":"model-context-protocol/#getting-started","title":"Getting started","text":""},{"location":"model-context-protocol/#1-set-up-an-mcp-connection","title":"1. Set up an MCP connection","text":"<p>To use MCP with Koog, you need to set up a connection:</p> <ol> <li>Start an MCP server (either as a process, Docker container, or web service).</li> <li>Create a transport mechanism to communicate with the server. </li> </ol> <p>MCP servers support the stdio and SSE transport mechanisms to communicate with the agent, so you can connect using one of them.</p>"},{"location":"model-context-protocol/#connect-with-stdio","title":"Connect with stdio","text":"<p>This protocol is used when an MCP server runs as a separate process. Here is an example of setting up an MCP connection using the stdio transport:</p> <pre><code>// Start an MCP server (for example, as a process)\nval process = ProcessBuilder(\"path/to/mcp/server\").start()\n\n// Create the stdio transport \nval transport = McpToolRegistryProvider.defaultStdioTransport(process)\n</code></pre>"},{"location":"model-context-protocol/#connect-with-sse","title":"Connect with SSE","text":"<p>This protocol is used when an MCP server runs as a web service. Here is an example of setting up an MCP connection using the SSE transport:</p> <pre><code>// Create the SSE transport\nval transport = McpToolRegistryProvider.defaultSseTransport(\"http://localhost:8931\")\n</code></pre>"},{"location":"model-context-protocol/#2-create-a-tool-registry","title":"2. Create a tool registry","text":"<p>Once you have the MCP connection, you can create a tool registry with tools from the MCP server in one of the following ways:</p> <ul> <li>Using the provided transport mechanism for communication. For example:</li> </ul> <pre><code>// Create a tool registry with tools from the MCP server\nval toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = transport,\n    name = \"my-client\",\n    version = \"1.0.0\"\n)\n</code></pre> <ul> <li>Using an MCP client connected to the MCP server. For example:</li> </ul> <pre><code>// Create a tool registry from an existing MCP client\nval toolRegistry = McpToolRegistryProvider.fromClient(\n    mcpClient = existingMcpClient\n)\n</code></pre>"},{"location":"model-context-protocol/#3-integrate-with-your-agent","title":"3. Integrate with your agent","text":"<p>To use MCP tools with your Koog agent, you need to register the tool registry with the agent:</p> <pre><code>// Create an agent with the tools\nval agent = AIAgent(\n    executor = executor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry\n)\n\n// Run the agent with a task that uses an MCP tool\nval result = agent.run(\"Use the MCP tool to perform a task\")\n</code></pre>"},{"location":"model-context-protocol/#usage-examples","title":"Usage examples","text":""},{"location":"model-context-protocol/#google-maps-mcp-integration","title":"Google Maps MCP integration","text":"<p>This example demonstrates how to connect to a Google Maps server for geographic data using MCP:</p> <pre><code>// Start the Docker container with the Google Maps MCP server\nval process = ProcessBuilder(\n    \"docker\", \"run\", \"-i\",\n    \"-e\", \"GOOGLE_MAPS_API_KEY=$googleMapsApiKey\",\n    \"mcp/google-maps\"\n).start()\n\n// Create the ToolRegistry with tools from the MCP server\nval toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = McpToolRegistryProvider.defaultStdioTransport(process)\n)\n\n// Create and run the agent\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(openAIApiToken),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry,\n)\nagent.run(\"Get elevation of the Jetbrains Office in Munich, Germany?\")\n</code></pre>"},{"location":"model-context-protocol/#playwright-mcp-integration","title":"Playwright MCP integration","text":"<p>This example demonstrates how to connect to a Playwright server for web automation using MCP:</p> <pre><code>// Start the Playwright MCP server\nval process = ProcessBuilder(\n    \"npx\", \"@playwright/mcp@latest\", \"--port\", \"8931\"\n).start()\n\n// Create the ToolRegistry with tools from the MCP server\nval toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = McpToolRegistryProvider.defaultSseTransport(\"http://localhost:8931\")\n)\n\n// Create and run the agent\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(openAIApiToken),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry,\n)\nagent.run(\"Open a browser, navigate to jetbrains.com, accept all cookies, click AI in toolbar\")\n</code></pre>"},{"location":"nodes-and-components/","title":"Pre-defined nodes and components","text":"<p>_# Predefined nodes and components</p> <p>Nodes are the fundamental building blocks of agent workflows in the Koog framework. Each node represents a specific operation or transformation in the workflow, and they can be connected using edges to define the flow of execution.</p> <p>In general, they let you encapsulate complex logic into reusable components that can be easily integrated into different agent workflows. This guide will walk you through the existing nodes that can be used in your agent strategies.</p> <p>For more detailed reference documentation, see API reference.</p>"},{"location":"nodes-and-components/#utility-nodes","title":"Utility nodes","text":""},{"location":"nodes-and-components/#nodedonothing","title":"nodeDoNothing","text":"<p>A simple pass-through node that does nothing and returns the input as output. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Create a placeholder node in your graph.</li> <li>Create a connection point without modifying the data.</li> </ul> <p>Here is an example:</p> <pre><code>val passthrough by nodeDoNothing&lt;String&gt;(\"passthrough\")\n\nedge(nodeStart forwardTo passthrough)\nedge(passthrough forwardTo nodeFinish)\n</code></pre>"},{"location":"nodes-and-components/#llm-nodes","title":"LLM nodes","text":""},{"location":"nodes-and-components/#nodeupdateprompt","title":"nodeUpdatePrompt","text":"<p>A node that adds messages to the LLM prompt using the provided prompt builder. This is useful for modifying the conversation context before making an actual LLM request. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Add system instructions to the prompt.</li> <li>Insert user messages into the conversation.</li> <li>Prepare the context for subsequent LLM requests.</li> </ul> <p>Here is an example:</p> <pre><code>val firstNode by node&lt;Input, Output&gt; {\n    // Transform input to output\n}\n\nval secondNode by node&lt;Output, Output&gt; {\n    // Transform output to output\n}\n\n// Node will get the value of type Output as input from the previous node and path through it to the next node\nval setupContext by nodeUpdatePrompt&lt;Output&gt;(\"setupContext\") {\n    system(\"You are a helpful assistant specialized in Kotlin programming.\")\n    user(\"I need help with Kotlin coroutines.\")\n}\n\nedge(firstNode forwardTo setupContext)\nedge(setupContext forwardTo secondNode)\n</code></pre>"},{"location":"nodes-and-components/#nodellmsendmessageonlycallingtools","title":"nodeLLMSendMessageOnlyCallingTools","text":"<p>A node that appends a user message to the LLM prompt and gets a response where the LLM can only call tools. For details, see API reference.</p>"},{"location":"nodes-and-components/#nodellmsendmessageforceonetool","title":"nodeLLMSendMessageForceOneTool","text":"<p>A node that that appends a user message to the LLM prompt and forces the LLM to use a specific tool. For details, see API reference.</p>"},{"location":"nodes-and-components/#nodellmrequest","title":"nodeLLMRequest","text":"<p>A node that appends a user message to the LLM prompt and gets a response with optional tool usage. The node configuration determines whether tool calls are allowed during the processing of the message. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Generate LLM response for the current prompt, controlling if the LLM is allowed to generate tool calls.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLM by nodeLLMRequest(\"requestLLM\", allowToolCalls = true)\nedge(getUserQuestion forwardTo requestLLM)\n</code></pre>"},{"location":"nodes-and-components/#nodellmrequeststructured","title":"nodeLLMRequestStructured","text":"<p>A node that appends a user message to the LLM prompt and requests structured data from the LLM with error correction capabilities. For details, see API reference.</p>"},{"location":"nodes-and-components/#nodellmrequeststreaming","title":"nodeLLMRequestStreaming","text":"<p>A node that appends a user message to the LLM prompt and streams LLM response with or without stream data transformation. For details, see API reference.</p>"},{"location":"nodes-and-components/#nodellmrequestmultiple","title":"nodeLLMRequestMultiple","text":"<p>A node that appends a user message to the LLM prompt and gets multiple LLM responses with tool calls enabled. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Handle complex queries that require multiple tool calls.</li> <li>Generate multiple tool calls.</li> <li>Implement a workflow that requires multiple parallel actions.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLMMultipleTools by nodeLLMRequestMultiple()\nedge(getComplexUserQuestion forwardTo requestLLMMultipleTools)\n</code></pre>"},{"location":"nodes-and-components/#nodellmcompresshistory","title":"nodeLLMCompressHistory","text":"<p>A node that compresses the current LLM prompt (message history) into a summary, replacing messages with a concise summary (TL;DR). For details, see API reference. This is useful for managing long conversations by compressing the history to reduce token usage.</p> <p>To learn more about history compression, see History compression.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Manage long conversations to reduce token usage.</li> <li>Summarize conversation history to maintain context.</li> <li>Implement memory management in long-running agents.</li> </ul> <p>Here is an example:</p> <pre><code>val compressHistory by nodeLLMCompressHistory&lt;String&gt;(\n    \"compressHistory\",\n    strategy = HistoryCompressionStrategy.FromLastNMessages(10),\n    preserveMemory = true\n)\nedge(generateHugeHistory forwardTo compressHistory)\n</code></pre>"},{"location":"nodes-and-components/#tool-nodes","title":"Tool nodes","text":""},{"location":"nodes-and-components/#nodeexecutetool","title":"nodeExecuteTool","text":"<p>A node that executes a single tool call and returns its result. This node is used to handle tool calls made by the LLM. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Execute tools requested by the LLM.</li> <li>Handle specific actions in response to LLM decisions.</li> <li>Integrate external functionality into the agent workflow.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLM by nodeLLMRequest()\nval executeTool by nodeExecuteTool()\nedge(requestLLM forwardTo executeTool onToolCall { true })\n</code></pre>"},{"location":"nodes-and-components/#nodellmsendtoolresult","title":"nodeLLMSendToolResult","text":"<p>A node that adds a tool result to the prompt and requests an LLM response. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Process the results of tool executions.</li> <li>Generate responses based on tool outputs.</li> <li>Continue a conversation after tool execution.</li> </ul> <p>Here is an example:</p> <pre><code>val executeTool by nodeExecuteTool()\nval sendToolResultToLLM by nodeLLMSendToolResult()\nedge(executeTool forwardTo sendToolResultToLLM)\n</code></pre>"},{"location":"nodes-and-components/#nodeexecutemultipletools","title":"nodeExecuteMultipleTools","text":"<p>A node that executes multiple tool calls. These calls can optionally be executed in parallel. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Execute multiple tools in parallel.</li> <li>Handle complex workflows that require multiple tool executions.</li> <li>Optimize performance by batching tool calls.</li> </ul> <p>Here is an example:</p> <pre><code>val requestLLMMultipleTools by nodeLLMRequestMultiple()\nval executeMultipleTools by nodeExecuteMultipleTools()\nedge(requestLLMMultipleTools forwardTo executeMultipleTools onMultipleToolCalls { true })\n</code></pre>"},{"location":"nodes-and-components/#nodellmsendmultipletoolresults","title":"nodeLLMSendMultipleToolResults","text":"<p>A node that adds multiple tool results to the prompt and gets multiple LLM responses. For details, see API reference.</p> <p>You can use this node for the following purposes:</p> <ul> <li>Process the results of multiple tool executions.</li> <li>Generate multiple tool calls.</li> <li>Implement complex workflows with multiple parallel actions.</li> </ul> <p>Here is an example:</p> <pre><code>val executeMultipleTools by nodeExecuteMultipleTools()\nval sendMultipleToolResultsToLLM by nodeLLMSendMultipleToolResults()\nedge(executeMultipleTools forwardTo sendMultipleToolResultsToLLM)\n</code></pre>"},{"location":"nodes-and-components/#predefined-subgraphs","title":"Predefined subgraphs","text":"<p>The framework provides predefined subgraphs that encapsulate commonly used patterns and workflows. These subgraphs simplify the development of complex agent strategies by handling the creation of base nodes and edges automatically.</p> <p>By using the predefined subgraphs, you can implement various popular pipelines. Here is an example:</p> <ol> <li>Prepare the data.</li> <li>Run the task.</li> <li>Validate the task results. If the results are incorrect, return to step 2 with a feedback message to make adjustments.</li> </ol>"},{"location":"nodes-and-components/#subgraphwithtask","title":"subgraphWithTask","text":"<p>A subgraph that performs a specific task using provided tools and returns a structured result. This subgraph is designed to handle self-contained tasks within a larger workflow. For details, see API reference.</p> <p>You can use this subgraph for the following purposes:</p> <ul> <li>Create special components that handle specific tasks within a larger workflow.</li> <li>Encapsulate complex logic with clear input and output interfaces.</li> <li>Configure task-specific tools, models, and prompts.</li> <li>Manage conversation history with automatic compression.</li> <li>Develop structured agent workflows and task execution pipelines.</li> <li>Generate structured results from LLM task execution.</li> </ul> <p>You can provide a task to the subgraph as text, configure the LLM if needed, and provide the necessary tools, and the subgraph will process and solve the task. Here is an example:</p> <pre><code>val processQuery by subgraphWithTask&lt;String&gt;(\n    tools = listOf(searchTool, calculatorTool, weatherTool),\n    llmModel = OpenAIModels.Chat.GPT4o,\n) { userQuery -&gt;\n    \"\"\"\n    You are a helpful assistant that can answer questions about various topics.\n    Please help with the following query:\n    $userQuery\n    \"\"\"\n}\n</code></pre>"},{"location":"nodes-and-components/#subgraphwithverification","title":"subgraphWithVerification","text":"<p>A special version of <code>subgraphWithTask</code> that verifies whether a task was performed correctly and provides details about any issues encountered. This subgraph is useful for workflows that require validation or quality checks. For details, see API reference.</p> <p>You can use this subgraph for the following purposes:</p> <ul> <li>Verify the correctness of task execution.</li> <li>Implement quality control processes in your workflows.</li> <li>Create self-validating components.</li> <li>Generate structured verification results with success/failure status and detailed feedback.</li> </ul> <p>The subgraph ensures that the LLM calls a verification tool at the end of the workflow to check whether the task was successfully completed. It guarantees this verification is performed as the final step and returns a <code>VerifiedSubgraphResult</code> that indicates whether a task was completed successfully and provides detailed feedback.  Here is an example:</p> <pre><code>val verifyCode by subgraphWithVerification&lt;String&gt;(\n    tools = listOf(runTestsTool, analyzeTool, readFileTool),\n    llmModel = AnthropicModels.Sonnet_3_7\n) { codeToVerify -&gt;\n    \"\"\"\n    You are a code reviewer. Please verify that the following code meets all requirements:\n    1. It compiles without errors\n    2. All tests pass\n    3. It follows the project's coding standards\n\n    Code to verify:\n    $codeToVerify\n    \"\"\"\n}\n</code></pre>"},{"location":"nodes-and-components/#predefined-strategies-and-common-strategy-patterns","title":"Predefined strategies and common strategy patterns","text":"<p>The framework provides predefined strategies that combine various nodes. The nodes are connected using edges to define the flow of operations, with conditions that specify when to follow each edge.</p> <p>You can integrate these strategies into your agent workflows if needed.</p>"},{"location":"nodes-and-components/#single-run-strategy","title":"Single run strategy","text":"<p>A single run strategy is designed for non-interactive use cases where the agent processes input once and returns a result.</p> <p>You can use this strategy when you need to run straightforward processes that do not require complex logic.</p> <pre><code>public fun singleRunStrategy(): AIAgentStrategy&lt;String, String&gt; = strategy(\"single_run\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendToolResult(\"nodeSendToolResult\")\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeSendToolResult)\n    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n}\n</code></pre>"},{"location":"nodes-and-components/#tool-based-strategy","title":"Tool-based strategy","text":"<p>A tool-based strategy is designed for workflows that heavily rely on tools to perform specific operations. It typically executes tools based on the LLM decisions and processes the results.</p> <pre><code>fun toolBasedStrategy(name: String, toolRegistry: ToolRegistry): AIAgentStrategy&lt;String, String&gt; {\n    return strategy(name) {\n        val nodeSendInput by nodeLLMRequest()\n        val nodeExecuteTool by nodeExecuteTool()\n        val nodeSendToolResult by nodeLLMSendToolResult()\n\n        // Define the flow of the agent\n        edge(nodeStart forwardTo nodeSendInput)\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendInput forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n\n        // If the LLM calls a tool, execute it\n        edge(\n            (nodeSendInput forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // Send the tool result back to the LLM\n        edge(nodeExecuteTool forwardTo nodeSendToolResult)\n\n        // If the LLM calls another tool, execute it\n        edge(\n            (nodeSendToolResult forwardTo nodeExecuteTool)\n                    onToolCall { true }\n        )\n\n        // If the LLM responds with a message, finish\n        edge(\n            (nodeSendToolResult forwardTo nodeFinish)\n                    onAssistantMessage { true }\n        )\n    }\n}\n</code></pre>"},{"location":"nodes-and-components/#streaming-data-strategy","title":"Streaming data strategy","text":"<p>A streaming data strategy is designed for processing streaming data from the LLM. It typically requests streaming data, processes it, and potentially calls tools with the processed data.</p> <pre><code>val agentStrategy = strategy&lt;String, List&lt;Book&gt;&gt;(\"library-assistant\") {\n    // Describe the node containing the output stream parsing\n    val getMdOutput by node&lt;String, List&lt;Book&gt;&gt; { booksDescription -&gt;\n        val books = mutableListOf&lt;Book&gt;()\n        val mdDefinition = markdownBookDefinition()\n\n        llm.writeSession {\n            updatePrompt { user(booksDescription) }\n            // Initiate the response stream in the form of the definition `mdDefinition`\n            val markdownStream = requestLLMStreaming(mdDefinition)\n            // Call the parser with the result of the response stream and perform actions with the result\n            parseMarkdownStreamToBooks(markdownStream).collect { book -&gt;\n                books.add(book)\n                println(\"Parsed Book: ${book.title} by ${book.author}\")\n            }\n        }\n\n        books\n    }\n    // Describe the agent's graph making sure the node is accessible\n    edge(nodeStart forwardTo getMdOutput)\n    edge(getMdOutput forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"opentelemetry-langfuse-exporter/","title":"Langfuse exporter","text":"<p>Koog provides built-in support for exporting agent traces to Langfuse, a platform for observability and analytics of AI applications. With Langfuse integration, you can visualize, analyze, and debug how your Koog agents interact with LLMs, APIs, and other components.</p> <p>For background on Koog\u2019s OpenTelemetry support, see the OpenTelemetry support.</p>"},{"location":"opentelemetry-langfuse-exporter/#setup-instructions","title":"Setup instructions","text":"<ol> <li>Create a Langfuse project. Follow the setup guide at Create new project in Langfuse</li> <li>Obtain API credentials. Retrieve your Langfuse <code>public key</code> and <code>secret key</code> as described in Where are Langfuse API keys?</li> <li>Pass the Langfuse host, private key, and secret key to the Langfuse exporter.  This can be done by providing them as parameters to the <code>addLangfuseExporter()</code> function,  or by setting environment variables as shown below:</li> </ol> <pre><code>   export LANGFUSE_HOST=\"https://cloud.langfuse.com\"\n   export LANGFUSE_PUBLIC_KEY=\"&lt;your-public-key&gt;\"\n   export LANGFUSE_SECRET_KEY=\"&lt;your-secret-key&gt;\"\n</code></pre>"},{"location":"opentelemetry-langfuse-exporter/#configuration","title":"Configuration","text":"<p>To enable Langfuse export, install the OpenTelemetry feature and add the <code>LangfuseExporter</code>. The exporter uses <code>OtlpHttpSpanExporter</code> under the hood to send traces to Langfuse\u2019s OpenTelemetry endpoint.</p>"},{"location":"opentelemetry-langfuse-exporter/#example-agent-with-langfuse-tracing","title":"Example: agent with Langfuse tracing","text":"<pre><code>fun main() = runBlocking {\n    val apiKey = \"api-key\"\n\n    val agent = AIAgent(\n        executor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.CostOptimized.GPT4oMini,\n        systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n    ) {\n        install(OpenTelemetry) {\n            addLangfuseExporter()\n        }\n    }\n\n    println(\"Running agent with Langfuse tracing\")\n\n    val result = agent.run(\"Tell me a joke about programming\")\n\n    println(\"Result: $result\\nSee traces on the Langfuse instance\")\n}\n</code></pre>"},{"location":"opentelemetry-langfuse-exporter/#what-gets-traced","title":"What gets traced","text":"<p>When enabled, the Langfuse exporter captures the same spans as Koog\u2019s general OpenTelemetry integration, including:</p> <ul> <li>Agent lifecycle events: agent start, stop, errors</li> <li>LLM interactions: prompts, responses, token usage, latency</li> <li>Tool calls: execution traces for tool invocations</li> <li>System context: metadata such as model name, environment, Koog version</li> </ul> <p>Koog also captures span attributes required by Langfuse to show Agent Graphs. </p> <p>When visualized in Langfuse, the trace appears as follows:  </p> <p>For more details on Langfuse OpenTelemetry tracing, see: Langfuse OpenTelemetry Docs.</p>"},{"location":"opentelemetry-langfuse-exporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opentelemetry-langfuse-exporter/#no-traces-appear-in-langfuse","title":"No traces appear in Langfuse","text":"<ul> <li>Double-check that <code>LANGFUSE_HOST</code>, <code>LANGFUSE_PUBLIC_KEY</code>, and <code>LANGFUSE_SECRET_KEY</code> are set in your environment.</li> <li>If running on self-hosted Langfuse, confirm that the <code>LANGFUSE_HOST</code> is reachable from your application environment.</li> <li>Verify that the public/secret key pair belongs to the correct project.</li> </ul>"},{"location":"opentelemetry-support/","title":"OpenTelemetry support","text":"<p>This page provides details about the support for OpenTelemetry with the Koog agentic framework for tracing and  monitoring your AI agents.</p>"},{"location":"opentelemetry-support/#overview","title":"Overview","text":"<p>OpenTelemetry is an observability framework that provides tools for generating, collecting, and exporting telemetry data (traces) from your applications. The Koog OpenTelemetry feature allows you to instrument your AI agents to collect  telemetry data, which can help you:</p> <ul> <li>Monitor agent performance and behavior</li> <li>Debug issues in complex agent workflows</li> <li>Visualize the execution flow of your agents</li> <li>Track LLM calls and tool usage</li> <li>Analyze agent behavior patterns</li> </ul>"},{"location":"opentelemetry-support/#key-opentelemetry-concepts","title":"Key OpenTelemetry concepts","text":"<ul> <li>Spans: spans represent individual units of work or operations within a distributed trace. They indicate the  beginning and end of a specific activity in an application, such as an agent execution, a function call, an LLM call,  or a tool call.</li> <li>Attributes: attributes provide metadata about a telemetry-related item such as a span. Attributes are represented  as key-value pairs.</li> <li>Events: events are specific points in time during the lifetime of a span (span-related events) that represent  something potentially noteworthy that happened.</li> <li>Exporters: exporters are components responsible for sending the collected telemetry data to various backends or  destinations.</li> <li>Collectors: collectors receive, process, and export telemetry data. They act as intermediaries between your  applications and your observability backend.</li> <li>Samplers: samplers determine whether a trace should be recorded based on the sampling strategy. They are used to  manage the volume of telemetry data.</li> <li>Resources: resources represent entities that produce telemetry data. They are identified by resource attributes,  which are key-value pairs that provide information about the resource.</li> </ul> <p>The OpenTelemetry feature in Koog automatically creates spans for various agent events, including:</p> <ul> <li>Agent execution start and end</li> <li>Node execution</li> <li>LLM calls</li> <li>Tool calls</li> </ul>"},{"location":"opentelemetry-support/#installation","title":"Installation","text":"<p>To use OpenTelemetry with Koog, add the OpenTelemetry feature to your agent:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(apiKey),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    systemPrompt = \"You are a helpful assistant.\",\n    installFeatures = {\n        install(OpenTelemetry) {\n            // Configuration options go here\n        }\n    }\n)\n</code></pre>"},{"location":"opentelemetry-support/#configuration","title":"Configuration","text":""},{"location":"opentelemetry-support/#basic-configuration","title":"Basic configuration","text":"<p>Here is the full list of available properties that you set when configuring the OpenTelemetry feature in an agent:</p> Name Data type Default value Description <code>serviceName</code> <code>String</code> <code>ai.koog</code> The name of the service being instrumented. <code>serviceVersion</code> <code>String</code> Current Koog library version The version of the service being instrumented. <code>isVerbose</code> <code>Boolean</code> <code>false</code> Whether to enable verbose logging for debugging OpenTelemetry configuration. <code>sdk</code> <code>OpenTelemetrySdk</code> The OpenTelemetry SDK instance to use for telemetry collection. <code>tracer</code> <code>Tracer</code> The OpenTelemetry tracer instance used for creating spans. <p>Note</p> <p>The <code>sdk</code> and <code>tracer</code> properties are public properties that you can access, but you can only set them using the public methods listed below.</p> <p>The <code>OpenTelemetryConfig</code> class also includes methods that represent actions related to different configuration items. Here is an example of installing the OpenTelemetry feature with a basic set of configuration items:</p> <pre><code>install(OpenTelemetry) {\n    // Set your service configuration\n    setServiceInfo(\"my-agent-service\", \"1.0.0\")\n\n    // Add the Logging exporter\n    addSpanExporter(LoggingSpanExporter.create())\n}\n</code></pre> <p>For a reference of available methods, see the sections below.</p>"},{"location":"opentelemetry-support/#setserviceinfo","title":"setServiceInfo","text":"<p>Sets the service information including name and version. Takes the following arguments:</p> Name Data type Required Default value Description <code>serviceName</code> String Yes The name of the service being instrumented. <code>serviceVersion</code> String Yes The version of the service being instrumented."},{"location":"opentelemetry-support/#addspanexporter","title":"addSpanExporter","text":"<p>Adds a span exporter to send telemetry data to external systems. Takes the following argument:</p> Name Data type Required Default value Description <code>exporter</code> <code>SpanExporter</code> Yes The <code>SpanExporter</code> instance to be added to the list of custom span exporters."},{"location":"opentelemetry-support/#addspanprocessor","title":"addSpanProcessor","text":"<p>Adds a span processor factory to process spans before they are exported. Takes the following argument:</p> Name Data type Required Default value Description <code>processor</code> <code>(SpanExporter) -&gt; SpanProcessor</code> Yes A function that creates a span processor for a given exporter. Lets you customize processing per exporter."},{"location":"opentelemetry-support/#addresourceattributes","title":"addResourceAttributes","text":"<p>Adds resource attributes to provide additional context about the service. Takes the following argument:</p> Name Data type Required Default value Description <code>attributes</code> <code>Map&lt;AttributeKey&lt;T&gt;, T&gt;</code> Yes The key-value pairs that provide additional details about the service."},{"location":"opentelemetry-support/#setsampler","title":"setSampler","text":"<p>Sets the sampling strategy to control which spans are collected. Takes the following argument:</p> Name Data type Required Default value Description <code>sampler</code> <code>Sampler</code> Yes The sampler instance to set for the OpenTelemetry configuration."},{"location":"opentelemetry-support/#setverbose","title":"setVerbose","text":"<p>Enables or disables verbose logging for debugging OpenTelemetry configuration. Takes the following argument:</p> Name Data type Required Default value Description <code>verbose</code> <code>Boolean</code> Yes <code>false</code> If true, the application collects more detailed telemetry data."},{"location":"opentelemetry-support/#setsdk","title":"setSdk","text":"<p>Injects a pre-configured OpenTelemetrySdk instance.</p> <ul> <li>When you call setSdk(sdk), the provided SDK is used as-is, and any custom configuration applied via addSpanExporter, addSpanProcessor, addResourceAttributes, or setSampler is ignored.</li> <li>The tracer\u2019s instrumentation scope name/version are aligned with your service info.</li> </ul> Name Data type Required Description <code>sdk</code> <code>OpenTelemetrySdk</code> Yes The SDK instance to use in the agent."},{"location":"opentelemetry-support/#advanced-configuration","title":"Advanced configuration","text":"<p>For more advanced configuration, you can also customize the following configuration options:</p> <ul> <li>Sampler: configure the sampling strategy to adjust the frequency and amount of collected data.</li> <li>Resource attributes: add more information about the process that is producing telemetry data. </li> </ul> <pre><code>install(OpenTelemetry) {\n    // Set your service configuration\n    setServiceInfo(\"my-agent-service\", \"1.0.0\")\n\n    // Add the Logging exporter\n    addSpanExporter(LoggingSpanExporter.create())\n\n    // Set the sampler \n    setSampler(Sampler.traceIdRatioBased(0.5)) \n\n    // Add resource attributes\n    addResourceAttributes(mapOf(\n        AttributeKey.stringKey(\"custom.attribute\") to \"custom-value\")\n    )\n}\n</code></pre>"},{"location":"opentelemetry-support/#sampler","title":"Sampler","text":"<p>To define a sampler, use a corresponding method of the <code>Sampler</code> class (<code>io.opentelemetry.sdk.trace.samplers.Sampler</code>)  from the <code>opentelemetry-java</code> SDK that represents the sampling strategy you want to use. </p> <p>The default sampling strategy is as follows:</p> <ul> <li><code>Sampler.alwaysOn()</code>: The default sampling strategy where every span (trace) is sampled.</li> </ul> <p>For more information about available samplers and sampling strategies, see the OpenTelemetry Sampler documentation.</p>"},{"location":"opentelemetry-support/#resource-attributes","title":"Resource attributes","text":"<p>Resource attributes represent additional information about a process producing telemetry data. Koog includes a set of  resource attributes that are set by default:</p> <ul> <li><code>service.name</code></li> <li><code>service.version</code></li> <li><code>service.instance.time</code></li> <li><code>os.type</code></li> <li><code>os.version</code></li> <li><code>os.arch</code></li> </ul> <p>The default value of the <code>service.name</code> attribute is <code>ai.koog</code>, while the default <code>service.version</code> value is the currently used Koog library version.</p> <p>In addition to default resource attributes, you can also add custom attributes. To add a custom attribute to an  OpenTelemetry configuration in Koog, use the <code>addResourceAttributes()</code> method in an OpenTelemetry configuration that  takes a key and a value as its arguments.</p> <pre><code>addResourceAttributes(mapOf(\n    AttributeKey.stringKey(\"custom.attribute\") to \"custom-value\")\n)\n</code></pre>"},{"location":"opentelemetry-support/#span-types-and-attributes","title":"Span types and attributes","text":"<p>The OpenTelemetry feature automatically creates different types of spans to track various operations in your agent:</p> <ul> <li>CreateAgentSpan: created when you run an agent, closed when the agent is closed or the process is terminated.</li> <li>InvokeAgentSpan: the invocation of an agent.</li> <li>NodeExecuteSpan: the execution of a node in the agent's strategy. This is a custom, Koog-specific span.</li> <li>InferenceSpan: an LLM call.</li> <li>ExecuteToolSpan: a tool call.</li> </ul> <p>Spans are organized in a nested, hierarchical structure. Here is an example of a span structure:</p> <pre><code>CreateAgentSpan\n    InvokeAgentSpan\n        NodeExecuteSpan\n            InferenceSpan\n        NodeExecuteSpan\n            ExecuteToolSpan\n        NodeExecuteSpan\n            InferenceSpan    \n</code></pre>"},{"location":"opentelemetry-support/#span-attributes","title":"Span attributes","text":"<p>Span attributes provide metadata related to a span. Each span has its set of attributes, while some spans can also  repeat attributes.</p> <p>Koog supports a list of predefined attributes that follow OpenTelemetry's Semantic conventions for generative AI events. For example, the conventions define an attribute named  <code>gen_ai.conversation.id</code>, which is usually a required attribute for a span. In Koog, the value of this attribute is the  unique identifier for an agent run, that is automatically set when you call the <code>agent.run()</code> method.</p> <p>In addition, Koog also includes custom, Koog-specific attributes. You can recognize most of these attributes by the  <code>koog.</code> prefix. Here are the available custom attributes:</p> <ul> <li><code>koog.agent.strategy.name</code>: the name of the agent strategy. A strategy is a Koog-related entity that describes the  purpose of the agent. Used in the <code>InvokeAgentSpan</code> span.</li> <li><code>koog.node.name</code>: the name of the node being run. Used in the <code>NodeExecuteSpan</code> span.</li> </ul>"},{"location":"opentelemetry-support/#events","title":"Events","text":"<p>A span can also have an event attached to the span. Events describe a specific point in time when something relevant  happened. For example, when an LLM call started or finished. Events also have attributes and additionally include event  body fields.</p> <p>The following event types are supported in line with OpenTelemetry's Semantic conventions for generative AI events:</p> <ul> <li>SystemMessageEvent: the system instructions passed to the model.</li> <li>UserMessageEvent: the user message passed to the model.</li> <li>AssistantMessageEvent: the assistant message passed to the model.</li> <li>ToolMessageEvent: the response from a tool or function call passed to the model.</li> <li>ChoiceEvent: the response message from a model.</li> <li>ModerationResponseEvent: the model moderation result or signal.</li> </ul> <p>Note</p> <p>The <code>optentelemetry-java</code> SDK does not support the event body fields parameter when adding an event. Therefore, in  the OpenTelemetry support in Koog, event body fields are a separate attribute whose key is <code>body</code> and value type is  string. The string includes the content or payload for the event body field, which is usually a JSON-like object. For  examples of event body fields, see the OpenTelemetry documentation. For the state of support for event body  fields in <code>opentelemetry-java</code>, see the related GitHub issue.</p>"},{"location":"opentelemetry-support/#exporters","title":"Exporters","text":"<p>Exporters send collected telemetry data to an OpenTelemetry Collector or other types of destinations or backend  implementations. To add an exporter, use the <code>addSpanExporter()</code> method when installing the OpenTelemetry feature. The  method takes the following argument:</p> Name Data type Required Default Description <code>exporter</code> SpanExporter Yes The SpanExporter instance to be added to the list of custom span exporters. <p>The sections below provide information about some of the most commonly used exporters from the <code>opentelemetry-java</code> SDK.</p> <p>Note</p> <p>If you do not configure any custom exporters, Koog will use a console LoggingSpanExporter by default. This helps during local development and debugging.</p>"},{"location":"opentelemetry-support/#logging-exporter","title":"Logging exporter","text":"<p>A logging exporter that outputs trace information to the console. <code>LoggingSpanExporter</code>  (<code>io.opentelemetry.exporter.logging.LoggingSpanExporter</code>) is a part of the <code>opentelemetry-java</code> SDK.</p> <p>This type of export is useful for development and debugging purposes.</p> <pre><code>install(OpenTelemetry) {\n    // Add the logging exporter\n    addSpanExporter(LoggingSpanExporter.create())\n    // Add more exporters as needed\n}\n</code></pre>"},{"location":"opentelemetry-support/#opentelemetry-http-exporter","title":"OpenTelemetry HTTP exporter","text":"<p>OpenTelemetry HTTP exporter (<code>OtlpHttpSpanExporter</code>) is a part of the <code>opentelemetry-java</code> SDK  (<code>io.opentelemetry.exporter.otlp.http.trace.OtlpHttpSpanExporter</code>) and sends span data to a backend through HTTP.</p> <pre><code>install(OpenTelemetry) {\n   // Add OpenTelemetry HTTP exporter \n   addSpanExporter(\n      OtlpHttpSpanExporter.builder()\n         // Set the maximum time to wait for the collector to process an exported batch of spans \n         .setTimeout(30, TimeUnit.SECONDS)\n         // Set the OpenTelemetry endpoint to connect to\n         .setEndpoint(\"http://localhost:3000/api/public/otel/v1/traces\")\n         // Add the authorization header\n         .addHeader(\"Authorization\", \"Basic $AUTH_STRING\")\n         .build()\n   )\n}\n</code></pre>"},{"location":"opentelemetry-support/#opentelemetry-grpc-exporter","title":"OpenTelemetry gRPC exporter","text":"<p>OpenTelemetry gRPC exporter (<code>OtlpGrpcSpanExporter</code>) is a part of the <code>opentelemetry-java</code> SDK  (<code>io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter</code>). It exports telemetry data to a backend through gRPC and  lets you define the host and port of the backend, collector, or endpoint that receives the data. The default port is  <code>4317</code>.</p> <pre><code>install(OpenTelemetry) {\n   // Add OpenTelemetry gRPC exporter \n   addSpanExporter(\n      OtlpGrpcSpanExporter.builder()\n          // Set the host and the port\n         .setEndpoint(\"http://localhost:4317\")\n         .build()\n   )\n}\n</code></pre>"},{"location":"opentelemetry-support/#integration-with-langfuse","title":"Integration with Langfuse","text":"<p>Langfuse provides trace visualization and analytics for LLM/agent workloads.</p> <p>You can configure Koog to export OpenTelemetry traces directly to Langfuse using a helper function:</p> <pre><code>install(OpenTelemetry) {\n    addLangfuseExporter(\n        langfuseUrl = \"https://cloud.langfuse.com\",\n        langfusePublicKey = \"...\",\n        langfuseSecretKey = \"...\"\n    )\n}\n</code></pre> <p>Please read the full documentation about integration with Langfuse.</p>"},{"location":"opentelemetry-support/#integration-with-wb-weave","title":"Integration with W&amp;B Weave","text":"<p>W&amp;B Weave provides trace visualization and analytics for LLM/agent workloads. Integration with W&amp;B Weave can be configured via a predefined exporter:</p> <pre><code>install(OpenTelemetry) {\n    addWeaveExporter(\n        weaveOtelBaseUrl = \"https://trace.wandb.ai\",\n        weaveEntity = \"my-team\",\n        weaveProjectName = \"my-project\",\n        weaveApiKey = \"...\"\n    )\n}\n</code></pre> <p>Please read the full documentation about integration with W&amp;B Weave.</p>"},{"location":"opentelemetry-support/#integration-with-jaeger","title":"Integration with Jaeger","text":"<p>Jaeger is a popular distributed tracing system that works with OpenTelemetry. The <code>opentelemetry</code> directory within  <code>examples</code> in the Koog repository includes an example of using OpenTelemetry with Jaeger and Koog agents.</p>"},{"location":"opentelemetry-support/#prerequisites","title":"Prerequisites","text":"<p>To test OpenTelemetry with Koog and Jaeger, start the Jaeger OpenTelemetry all-in-one process using the provided  <code>docker-compose.yaml</code> file, by running the following command:</p> <pre><code>docker compose up -d\n</code></pre> <p>The provided Docker Compose YAML file includes the following content:</p> <pre><code># docker-compose.yaml\nservices:\n  jaeger-all-in-one:\n    image: jaegertracing/all-in-one:1.39\n    container_name: jaeger-all-in-one\n    environment:\n      - COLLECTOR_OTLP_ENABLED=true\n    ports:\n      - \"4317:4317\"\n      - \"16686:16686\"\n</code></pre> <p>To access the Jaeger UI and view your traces, open <code>http://localhost:16686</code>.</p>"},{"location":"opentelemetry-support/#example","title":"Example","text":"<p>To export telemetry data for use in Jaeger, the example uses <code>LoggingSpanExporter</code>  (<code>io.opentelemetry.exporter.logging.LoggingSpanExporter</code>) and <code>OtlpGrpcSpanExporter</code>  (<code>io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter</code>) from the <code>opentelemetry-java</code> SDK.</p> <p>Here is the full code sample:</p> <pre><code>fun main() {\n    runBlocking {\n        val agent = AIAgent(\n            executor = simpleOpenAIExecutor(openAIApiKey),\n            llmModel = OpenAIModels.Reasoning.O4Mini,\n            systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n        ) {\n            install(OpenTelemetry) {\n                // Add a console logger for local debugging\n                addSpanExporter(LoggingSpanExporter.create())\n\n                // Send traces to OpenTelemetry collector\n                addSpanExporter(\n                    OtlpGrpcSpanExporter.builder()\n                        .setEndpoint(\"http://localhost:4317\")\n                        .build()\n                )\n            }\n        }\n\n        agent.use { agent -&gt;\n            println(\"Running the agent with OpenTelemetry tracing...\")\n\n            val result = agent.run(\"Tell me a joke about programming\")\n\n            println(\"Agent run completed with result: '$result'.\" +\n                    \"\\nCheck Jaeger UI at http://localhost:16686 to view traces\")\n        }\n    }\n}\n</code></pre>"},{"location":"opentelemetry-support/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opentelemetry-support/#common-issues","title":"Common issues","text":"<ol> <li> <p>No traces appearing in Jaeger, Langfuse, or W&amp;B Weave</p> <ul> <li>Ensure the service is running and the OpenTelemetry port (4317) is accessible.</li> <li>Check that the OpenTelemetry exporter is configured with the correct endpoint.</li> <li>Make sure to wait a few seconds after agent execution for traces to be exported.</li> </ul> </li> <li> <p>Missing spans or incomplete traces</p> <ul> <li>Verify that the agent execution completes successfully.</li> <li>Ensure that you're not closing the application too quickly after agent execution.</li> <li>Add a delay after agent execution to allow time for spans to be exported.</li> </ul> </li> <li> <p>Excessive number of spans</p> <ul> <li>Consider using a different sampling strategy by configuring the <code>sampler</code> property.</li> <li>For example, use <code>Sampler.traceIdRatioBased(0.1)</code> to sample only 10% of traces.</li> </ul> </li> <li> <p>Span adapters override each other</p> <ul> <li>Currently, the OpenTelemetry agent feature does not support applying multiple span adapters KG-265.</li> </ul> </li> </ol>"},{"location":"opentelemetry-weave-exporter/","title":"W&amp;B Weave exporter","text":"<p>Koog provides built-in support for exporting agent traces to W&amp;B Weave, a developer tool from Weights &amp; Biases for observability and analytics of AI applications. With the Weave integration, you can capture prompts, completions, system context, and execution traces  and visualize them directly in your W&amp;B workspace.</p> <p>For background on Koog\u2019s OpenTelemetry support, see the OpenTelemetry support.</p>"},{"location":"opentelemetry-weave-exporter/#setup-instructions","title":"Setup instructions","text":"<ol> <li>Get up a W&amp;B account at https://wandb.ai</li> <li>Get your API key from https://wandb.ai/authorize.</li> <li>Find your entity name by visiting your W&amp;B dashboard at https://wandb.ai/home.  Your entity is usually your username if it's a personal account or your team/org name.</li> <li>Define a name for your project. You don't have to create a project beforehand, it will be created automatically when the first trace is sent.</li> <li>Pass the Weave entity, project name, and API key to the Weave exporter.    This can be done by providing them as parameters to the <code>addWeaveExporter()</code> function,    or by setting environment variables as shown below:</li> </ol> <pre><code>export WEAVE_API_KEY=\"&lt;your-api-key&gt;\"\nexport WEAVE_ENTITY=\"&lt;your-entity&gt;\"\nexport WEAVE_PROJECT_NAME=\"koog-tracing\"\n</code></pre>"},{"location":"opentelemetry-weave-exporter/#configuration","title":"Configuration","text":"<p>To enable Weave export, install the OpenTelemetry feature and add the <code>WeaveExporter</code>. The exporter uses Weave\u2019s OpenTelemetry endpoint via <code>OtlpHttpSpanExporter</code>.</p>"},{"location":"opentelemetry-weave-exporter/#example-agent-with-weave-tracing","title":"Example: agent with Weave tracing","text":"<pre><code>fun main() = runBlocking {\n    val apiKey = \"api-key\"\n    val entity = System.getenv()[\"WEAVE_ENTITY\"] ?: throw IllegalArgumentException(\"WEAVE_ENTITY is not set\")\n    val projectName = System.getenv()[\"WEAVE_PROJECT_NAME\"] ?: \"koog-tracing\"\n\n    val agent = AIAgent(\n        executor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.CostOptimized.GPT4oMini,\n        systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n    ) {\n        install(OpenTelemetry) {\n            addWeaveExporter()\n        }\n    }\n\n    println(\"Running agent with Weave tracing\")\n\n    val result = agent.run(\"Tell me a joke about programming\")\n\n    println(\"Result: $result\\nSee traces on https://wandb.ai/$entity/$projectName/weave/traces\")\n}\n</code></pre>"},{"location":"opentelemetry-weave-exporter/#what-gets-traced","title":"What gets traced","text":"<p>When enabled, the Weave exporter captures the same spans as Koog\u2019s general OpenTelemetry integration, including:</p> <ul> <li>Agent lifecycle events: agent start, stop, errors</li> <li>LLM interactions: prompts, completions, latency</li> <li>Tool calls: execution traces for tool invocations</li> <li>System context: metadata such as model name, environment, Koog version</li> </ul> <p>When visualized in W&amp;B Weave, the trace appears as follows:  </p> <p>For more details, see the official Weave OpenTelemetry Docs.</p>"},{"location":"opentelemetry-weave-exporter/#troubleshooting","title":"Troubleshooting","text":""},{"location":"opentelemetry-weave-exporter/#no-traces-appear-in-weave","title":"No traces appear in Weave","text":"<ul> <li>Confirm that <code>WEAVE_API_KEY</code>, <code>WEAVE_ENTITY</code>, and <code>WEAVE_PROJECT_NAME</code> are set in your environment.</li> <li>Ensure that your W&amp;B account has access to the specified entity and project.</li> </ul>"},{"location":"opentelemetry-weave-exporter/#authentication-errors","title":"Authentication errors","text":"<ul> <li>Check that your <code>WEAVE_API_KEY</code> is valid.</li> <li>API key must have permission to write traces for the selected entity.</li> </ul>"},{"location":"opentelemetry-weave-exporter/#connection-issues","title":"Connection issues","text":"<ul> <li>Make sure your environment has network access to W&amp;B\u2019s OpenTelemetry ingestion endpoints.</li> </ul>"},{"location":"parallel-node-execution/","title":"Parallel node execution","text":""},{"location":"parallel-node-execution/#overview","title":"Overview","text":"<p>Parallel node execution lets you run multiple AI agent nodes concurrently, improving performance and enabling complex workflows. This feature is particularly useful when you need to:</p> <ul> <li>Process the same input through different models or approaches simultaneously</li> <li>Perform multiple independent operations in parallel</li> <li>Implement competitive evaluation patterns where multiple solutions are generated and then compared</li> </ul>"},{"location":"parallel-node-execution/#key-components","title":"Key components","text":"<p>Parallel node execution in Koog consists of the methods and data structures described below.</p>"},{"location":"parallel-node-execution/#methods","title":"Methods","text":"<ul> <li><code>parallel()</code>: executes multiple nodes in parallel and collects their results.</li> </ul>"},{"location":"parallel-node-execution/#data-structures","title":"Data structures","text":"<ul> <li><code>ParallelResult</code>: represents the completed result of a parallel node execution.</li> <li><code>NodeExecutionResult</code>: contains the output and context of a node execution.</li> </ul>"},{"location":"parallel-node-execution/#basic-usage","title":"Basic usage","text":""},{"location":"parallel-node-execution/#running-nodes-in-parallel","title":"Running nodes in parallel","text":"<p>To initiate parallel execution of nodes, use the <code>parallel</code> method in the following format:</p> <pre><code>val nodeName by parallel&lt;Input, Output&gt;(\n   firstNode, secondNode, thirdNode /* Add more nodes if needed */\n) {\n   // Merge strategy goes here, for example: \n   selectByMax { it.length }\n}\n</code></pre> <p>Here is an actual example of running three nodes in parallel and selecting the result with the maximum length:</p> <pre><code>val calc by parallel&lt;String, Int&gt;(\n   nodeCalcTokens, nodeCalcSymbols, nodeCalcWords,\n) {\n   selectByMax { it }\n}\n</code></pre> <p>The code above runs the <code>nodeCalcTokens</code>, <code>nodeCalcSymbols</code>, and <code>nodeCalcWords</code> nodes in parallel and returns the result with the maximum value.</p>"},{"location":"parallel-node-execution/#merge-strategies","title":"Merge strategies","text":"<p>After executing nodes in parallel, you need to specify how to merge the results. Koog provides the following merge strategies:</p> <ul> <li><code>selectBy()</code>: selects a result based on a predicate function.</li> <li><code>selectByMax()</code>: selects the result with the maximum value based on a comparison function.</li> <li><code>selectByIndex()</code>: selects a result based on an index returned by a selection function.</li> <li><code>fold()</code>: folds the results into a single value using an operation function.</li> </ul>"},{"location":"parallel-node-execution/#selectby","title":"selectBy","text":"<p>Selects a result based on a predicate function:</p> <pre><code>val nodeSelectJoke by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   selectBy { it.contains(\"programmer\") }\n}\n</code></pre> <p>This selects the first joke that contains the word \"programmer\".</p>"},{"location":"parallel-node-execution/#selectbymax","title":"selectByMax","text":"<p>Selects the result with the maximum value based on a comparison function:</p> <pre><code>val nodeLongestJoke by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   selectByMax { it.length }\n}\n</code></pre> <p>This selects the joke with the maximum length.</p>"},{"location":"parallel-node-execution/#selectbyindex","title":"selectByIndex","text":"<p>Selects a result based on an index returned by a selection function:</p> <pre><code>val nodeBestJoke by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   selectByIndex { jokes -&gt;\n      // Use another LLM to determine the best joke\n      llm.writeSession {\n         model = OpenAIModels.Chat.GPT4o\n         updatePrompt {\n            system(\"You are a comedy critic. Select the best joke.\")\n            user(\"Here are three jokes: ${jokes.joinToString(\"\\n\\n\")}\")\n         }\n         val response = requestLLMStructured&lt;JokeRating&gt;()\n         response.getOrNull()!!.structure.bestJokeIndex\n      }\n   }\n}\n</code></pre> <p>This uses another LLM call to determine the index of the best joke.</p>"},{"location":"parallel-node-execution/#fold","title":"fold","text":"<p>Folds the results into a single value using an operation function:</p> <pre><code>val nodeAllJokes by parallel&lt;String, String&gt;(\n   nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n) {\n   fold(\"Jokes:\\n\") { result, joke -&gt; \"$result\\n$joke\" }\n}\n</code></pre> <p>This combines all jokes into a single string.</p>"},{"location":"parallel-node-execution/#example-best-joke-agent","title":"Example: Best joke agent","text":"<p>Here is a complete example that uses parallel execution to generate jokes from different LLM models and select the best one:</p> <pre><code>val strategy = strategy(\"best-joke\") {\n   // Define nodes for different LLM models\n   val nodeOpenAI by node&lt;String, String&gt; { topic -&gt;\n      llm.writeSession {\n         model = OpenAIModels.Chat.GPT4o\n         updatePrompt {\n            system(\"You are a comedian. Generate a funny joke about the given topic.\")\n            user(\"Tell me a joke about $topic.\")\n         }\n         val response = requestLLMWithoutTools()\n         response.content\n      }\n   }\n\n   val nodeAnthropicSonnet by node&lt;String, String&gt; { topic -&gt;\n      llm.writeSession {\n         model = AnthropicModels.Sonnet_3_5\n         updatePrompt {\n            system(\"You are a comedian. Generate a funny joke about the given topic.\")\n            user(\"Tell me a joke about $topic.\")\n         }\n         val response = requestLLMWithoutTools()\n         response.content\n      }\n   }\n\n   val nodeAnthropicOpus by node&lt;String, String&gt; { topic -&gt;\n      llm.writeSession {\n         model = AnthropicModels.Opus_3\n         updatePrompt {\n            system(\"You are a comedian. Generate a funny joke about the given topic.\")\n            user(\"Tell me a joke about $topic.\")\n         }\n         val response = requestLLMWithoutTools()\n         response.content\n      }\n   }\n\n   // Execute joke generation in parallel and select the best joke\n   val nodeGenerateBestJoke by parallel(\n      nodeOpenAI, nodeAnthropicSonnet, nodeAnthropicOpus,\n   ) {\n      selectByIndex { jokes -&gt;\n         // Another LLM (e.g., GPT4o) would find the funniest joke:\n         llm.writeSession {\n            model = OpenAIModels.Chat.GPT4o\n            updatePrompt {\n               prompt(\"best-joke-selector\") {\n                  system(\"You are a comedy critic. Give a critique for the given joke.\")\n                  user(\n                     \"\"\"\n                            Here are three jokes about the same topic:\n\n                            ${jokes.mapIndexed { index, joke -&gt; \"Joke $index:\\n$joke\" }.joinToString(\"\\n\\n\")}\n\n                            Select the best joke and explain why it's the best.\n                            \"\"\".trimIndent()\n                  )\n               }\n            }\n\n            val response = requestLLMStructured&lt;JokeRating&gt;()\n            val bestJoke = response.getOrNull()!!.structure\n            bestJoke.bestJokeIndex\n         }\n      }\n   }\n\n   // Connect the nodes\n   nodeStart then nodeGenerateBestJoke then nodeFinish\n}\n</code></pre>"},{"location":"parallel-node-execution/#best-practices","title":"Best practices","text":"<ol> <li> <p>Consider resource constraints: Be mindful of resource usage when executing nodes in parallel, especially when making multiple LLM API calls simultaneously.</p> </li> <li> <p>Context management: Each parallel execution creates a forked context. When merging results, choose which context to preserve or how to combine contexts from different executions.</p> </li> <li> <p>Optimize for your use case:</p> <ul> <li>For competitive evaluation (like the joke example), use <code>selectByIndex</code> to select the best result</li> <li>For finding the maximum value, use <code>selectByMax</code></li> <li>For filtering based on a condition, use <code>selectBy</code></li> <li>For aggregation, use <code>fold</code> to combine all results into a composite output</li> </ul> </li> </ol>"},{"location":"parallel-node-execution/#performance-considerations","title":"Performance considerations","text":"<p>Parallel execution can significantly improve throughput, but it comes with some overhead:</p> <ul> <li>Each parallel node creates a new coroutine</li> <li>Context forking and merging add some computational cost</li> <li>Resource contention may occur with many parallel executions</li> </ul> <p>For optimal performance, parallelize operations that:</p> <ul> <li>Are independent of each other</li> <li>Have significant execution time</li> <li>Don't share mutable state</li> </ul>"},{"location":"prompt-api/","title":"Prompt API","text":"<p>The Prompt API provides a comprehensive toolkit for interacting with Large Language Models (LLMs) in production applications. It offers:</p> <ul> <li>Kotlin DSL for creating structured prompts with type safety.</li> <li>Multi-provider support for OpenAI, Anthropic, Google, and other LLM providers.</li> <li>Production features such as retry logic, error handling, and timeout configuration.</li> <li>Multimodal capabilities for working with text, images, audio, and documents.</li> </ul>"},{"location":"prompt-api/#architecture-overview","title":"Architecture overview","text":"<p>The Prompt API consists of three main layers:</p> <ul> <li>LLM clients: Low-level interfaces to specific providers (OpenAI, Anthropic, etc.).</li> <li>Decorators: Optional wrappers that add functionality like retry logic.</li> <li>Prompt executors: High-level abstractions that manage client lifecycle and simplify usage.</li> </ul>"},{"location":"prompt-api/#creating-a-prompt","title":"Creating a prompt","text":"<p>The Prompt API uses Kotlin DSL to create prompts. It supports the following types of messages:</p> <ul> <li><code>system</code>: Sets the context and instructions for the LLM.</li> <li><code>user</code>: Represents user input.</li> <li><code>assistant</code>: Represents LLM responses.</li> </ul> <p>Here is an example of a simple prompt:</p> <pre><code>val prompt = prompt(\"prompt_name\", LLMParams()) {\n    // Add a system message to set the context\n    system(\"You are a helpful assistant.\")\n\n    // Add a user message\n    user(\"Tell me about Kotlin\")\n\n    // You can also add assistant messages for few-shot examples\n    assistant(\"Kotlin is a modern programming language...\")\n\n    // Add another user message\n    user(\"What are its key features?\")\n}\n</code></pre>"},{"location":"prompt-api/#multimodal-inputs","title":"Multimodal inputs","text":"<p>In addition to providing text messages within prompts, Koog also lets you send images, audio, video, and files to LLMs along with <code>user</code> messages. As with standard text-only prompts, you also add media to the prompt using the DSL structure for prompt construction.</p> <pre><code>val prompt = prompt(\"multimodal_input\") {\n    system(\"You are a helpful assistant.\")\n\n    user {\n        +\"Describe these images\"\n\n        attachments {\n            image(\"https://example.com/test.png\")\n            image(Path(\"/User/koog/image.png\"))\n        }\n    }\n}\n</code></pre>"},{"location":"prompt-api/#textual-prompt-content","title":"Textual prompt content","text":"<p>To accommodate for the support for various attachment types and create a clear distinction between text and file inputs in a prompt, you put text messages in a dedicated <code>content</code> parameter within a user prompt. To add file inputs, provide them as a list within the <code>attachments</code> parameter.</p> <p>The general format of a user message that includes a text message and a list of attachments is as follows:</p> <pre><code>user(\n    content = \"This is the user message\",\n    attachments = listOf(\n        // Add attachments\n    )\n)\n</code></pre>"},{"location":"prompt-api/#file-attachments","title":"File attachments","text":"<p>To include an attachment, provide the file in the <code>attachments</code> parameter, following the format below:</p> <pre><code>user(\n    content = \"Describe this image\",\n    attachments = listOf(\n        Attachment.Image(\n            content = AttachmentContent.URL(\"https://example.com/capture.png\"),\n            format = \"png\",\n            mimeType = \"image/png\",\n            fileName = \"capture.png\"\n        )\n    )\n)\n</code></pre> <p>The <code>attachments</code> parameter takes a list of file inputs, where each item is an instance of one of the following classes:</p> <ul> <li><code>Attachment.Image</code>: image attachments, such as <code>jpg</code> or <code>png</code> files.</li> <li><code>Attachment.Audio</code>: audio attachments, such as <code>mp3</code> or <code>wav</code> files.</li> <li><code>Attachment.Video</code>: video attachments, such as <code>mpg</code> or <code>avi</code> files.</li> <li><code>Attachment.File</code>: file attachments, such as <code>pdf</code> or <code>txt</code> files.</li> </ul> <p>Each of the classes above takes the following parameters:</p> Name Data type Required Description <code>content</code> AttachmentContent Yes The source of the provided file content. For more information, see AttachmentContent. <code>format</code> String Yes The format of the provided file. For example, <code>png</code>. <code>mimeType</code> String Only for <code>Attachment.File</code> The MIME Type of the provided file. For example, <code>image/png</code>. <code>fileName</code> String No The name of the provided file including the extension. For example, <code>screenshot.png</code>. <p>For more details, see API reference.</p>"},{"location":"prompt-api/#attachmentcontent","title":"AttachmentContent","text":"<p><code>AttachmentContent</code> defines the type and source of content that is provided as an input to the LLM. The following classes are supported:</p> <ul> <li><code>AttachmentContent.URL(val url: String)</code></li> </ul> <p>Provides file content from the specified URL. Takes the following parameter:</p> Name Data type Required Description <code>url</code> String Yes The URL of the provided content. <p>See also API reference.</p> <ul> <li><code>AttachmentContent.Binary.Bytes(val data: ByteArray)</code></li> </ul> <p>Provides file content as a byte array. Takes the following parameter:</p> Name Data type Required Description <code>data</code> ByteArray Yes The file content provided as a byte array. <p>See also API reference.</p> <ul> <li><code>AttachmentContent.Binary.Base64(val base64: String)</code></li> </ul> <p>Provides file content encoded as a Base64 string. Takes the following parameter:</p> Name Data type Required Description <code>base64</code> String Yes The Base64 string containing file data. <p>See also API reference.</p> <ul> <li><code>AttachmentContent.PlainText(val text: String)</code></li> </ul> <p>!!! tip   Applies only if the attachment type is <code>Attachment.File</code>.   Provides content from a plain text file (such as the <code>text/plain</code> MIME type). Takes the following parameter:</p> Name Data type Required Description <code>text</code> String Yes The content of the file. <p>See also API reference.</p>"},{"location":"prompt-api/#mixed-attachment-content","title":"Mixed attachment content","text":"<p>In addition to providing different types of attachments in separate prompts or messages, you can also provide multiple and mixed types of attachments in a single <code>user</code> message:</p> <pre><code>val prompt = prompt(\"mixed_content\") {\n    system(\"You are a helpful assistant.\")\n\n    user {\n        +\"Compare the image with the document content.\"\n\n        attachments {\n            image(Path(\"/User/koog/page.png\"))\n            binaryFile(Path(\"/User/koog/page.pdf\"), \"application/pdf\")\n        }\n    }\n}\n</code></pre>"},{"location":"prompt-api/#choosing-between-llm-clients-and-prompt-executors","title":"Choosing between LLM clients and prompt executors","text":"<p>When working with the Prompt API, you can run prompts by using either LLM clients or prompt executors. To choose between clients and executors, consider the following factors:</p> <ul> <li>Use LLM clients directly if you work with a single LLM provider and do not require advanced lifecycle management. To learm more, see Running prompts with LLM clients.</li> <li>Use prompt executors if you need a higher level of abstraction for managing LLMs and their lifecycle, or if you want to run prompts with a consistent API across multiple providers and dynamically switch between them.   To learn more, see Runnning prompts with prompt executors.</li> </ul> <p>Note</p> <p>Both the LLM clients and prompt executors let you stream responses, generate multiple choices, and run content moderation. For more information, refer to the API Reference for the specific client or executor.</p>"},{"location":"prompt-api/#running-prompts-with-llm-clients","title":"Running prompts with LLM clients","text":"<p>You can use LLM clients to run prompts if you work with a single LLM provider and do not require advanced lifecycle management. Koog provides the following LLM clients:</p> <ul> <li>OpenAILLMClient</li> <li>AnthropicLLMClient</li> <li>GoogleLLMClient</li> <li>OpenRouterLLMClient</li> <li>OllamaClient</li> <li>BedrockLLMClient (JVM only)</li> </ul> <p>To run a prompt using an LLM client, perform the following:</p> <ol> <li>Create the LLM client that handles the connection between your application and LLM providers. For example:</li> </ol> <pre><code>// Create an OpenAI client\nval client = OpenAILLMClient(apiKey)\n</code></pre> <ol> <li>Call the <code>execute</code> method with the prompt and LLM as arguments.</li> </ol> <pre><code>// Execute the prompt\nval response = client.execute(\n    prompt = prompt,\n    model = OpenAIModels.Chat.GPT4o  // You can choose different models\n)\n</code></pre> <p>Here is an example that uses the OpenAI client to run a prompt:</p> <pre><code>fun main() {\n    runBlocking {\n        // Set up the OpenAI client with your API key\n        val token = System.getenv(\"OPENAI_API_KEY\")\n        val client = OpenAILLMClient(token)\n\n        // Create a prompt\n        val prompt = prompt(\"prompt_name\", LLMParams()) {\n            // Add a system message to set the context\n            system(\"You are a helpful assistant.\")\n\n            // Add a user message\n            user(\"Tell me about Kotlin\")\n\n            // You can also add assistant messages for few-shot examples\n            assistant(\"Kotlin is a modern programming language...\")\n\n            // Add another user message\n            user(\"What are its key features?\")\n        }\n\n        // Execute the prompt and get the response\n        val response = client.execute(prompt = prompt, model = OpenAIModels.Chat.GPT4o)\n        println(response)\n    }\n}\n</code></pre> <p>Note</p> <p>The LLM clients let you stream responses, generate multiple choices, and run content moderation. For more information, refer to the API Reference for the specific client. To learn more about content moderation, see Content moderation.</p>"},{"location":"prompt-api/#running-prompts-with-prompt-executors","title":"Running prompts with prompt executors","text":"<p>While LLM clients provide direct access to providers, prompt executors offer a higher-level abstraction that simplifies common use cases and handles client lifecycle management. They are ideal when you need to:</p> <ul> <li>Quickly prototype without managing client configuration.</li> <li>Work with multiple providers through a unified interface.</li> <li>Simplify dependency injection in larger applications.</li> <li>Abstract away provider-specific details.</li> </ul>"},{"location":"prompt-api/#executor-types","title":"Executor types","text":"<p>Koog provides two main prompt executors:</p> Name Description <code>SingleLLMPromptExecutor</code> Wraps a single LLM client for one provider. Use this executor if your agent only requires the ability to switch between models within a single LLM provider. <code>MultiLLMPromptExecutor</code> Routes to multiple LLM clients by a provider, with optional fallbacks for each provider to be used when a requested provider is not available. Use this executor if your agent needs to switch between models from different providers. <p>These are implementations of the <code>PromtExecutor</code> interface for executing prompts with LLMs.</p>"},{"location":"prompt-api/#creating-a-single-provider-executor","title":"Creating a single provider executor","text":"<p>To create a prompt executor for a specific LLM provider, perform the following:</p> <ol> <li>Configure an LLM client for a specific provider with the corresponding API key:</li> </ol> <pre><code>val openAIClient = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\n</code></pre> <ol> <li>Create a prompt executor using <code>SingleLLMPromptExecutor</code>:</li> </ol> <pre><code>val promptExecutor = SingleLLMPromptExecutor(openAIClient)\n</code></pre>"},{"location":"prompt-api/#creating-a-multi-provider-executor","title":"Creating a multi-provider executor","text":"<p>To create a prompt executor that works with multiple LLM providers, do the following:</p> <ol> <li>Configure clients for the required LLM providers with the corresponding API keys:</li> </ol> <pre><code>val openAIClient = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\nval ollamaClient = OllamaClient()\n</code></pre> <ol> <li>Pass the configured clients to the <code>MultiLLMPromptExecutor</code> class constructor to create a prompt executor with multiple LLM providers:</li> </ol> <pre><code>val multiExecutor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to openAIClient,\n    LLMProvider.Ollama to ollamaClient\n)\n</code></pre>"},{"location":"prompt-api/#pre-defined-prompt-executors","title":"Pre-defined prompt executors","text":"<p>For faster setup, Koog provides the following ready-to-use executor implementations for common providers:</p> <ul> <li> <p>Single provider executors that return <code>SingleLLMPromptExecutor</code> configured with a certain LLM client:</p> <ul> <li><code>simpleOpenAIExecutor</code> for executing prompts with OpenAI models.</li> <li><code>simpleAzureOpenAIExecutor</code> for executing prompts using Azure OpenAI Service.</li> <li><code>simpleAnthropicExecutor</code> for executing prompts with Anthropic models.</li> <li><code>simpleGoogleAIExecutor</code> for executing prompts with Google models.</li> <li><code>simpleOpenRouterExecutor</code> for executing prompts with OpenRouter.</li> <li><code>simpleOllamaExecutor</code> for executing prompts with Ollama.</li> </ul> </li> <li> <p>Multi-provider executor:</p> <ul> <li><code>DefaultMultiLLMPromptExecutor</code> which is an implementation of <code>MultiLLMPromptExecutor</code> that supports OpenAI, Anthropic, and Google providers.</li> </ul> </li> </ul> <p>Here is an example of creating pre-defined single and multi-provider executors:</p> <pre><code>// Create an OpenAI executor\nval promptExecutor = simpleOpenAIExecutor(\"OPENAI_KEY\")\n\n// Create a DefaultMultiLLMPromptExecutor with OpenAI, Anthropic, and Google LLM clients\nval openAIClient = OpenAILLMClient(\"OPENAI_KEY\")\nval anthropicClient = AnthropicLLMClient(\"ANTHROPIC_KEY\")\nval googleClient = GoogleLLMClient(\"GOOGLE_KEY\")\nval multiExecutor = DefaultMultiLLMPromptExecutor(openAIClient, anthropicClient, googleClient)\n</code></pre>"},{"location":"prompt-api/#executing-a-prompt","title":"Executing a prompt","text":"<p>The prompt executors provide methods to run prompts using various capabilities, such as streaming, multiple choices generation, and content moderation.</p> <p>Here is an example of how to run a prompt with a specific LLM using the <code>execute</code> method:</p> <pre><code>// Execute a prompt\nval response = promptExecutor.execute(\n    prompt = prompt,\n    model = OpenAIModels.Chat.GPT4o\n)\n</code></pre> <p>This will run the prompt with the <code>GPT4o</code> model and return the response.</p> <p>Note</p> <p>The prompt executors let you stream responses, generate multiple choices, and run content moderation. For more information, refer to the API Reference for the specific executor. To learn more about content moderation, see Content moderation.</p>"},{"location":"prompt-api/#cached-prompt-executors","title":"Cached prompt executors","text":"<p>For repeated requests, you can cache LLM responses to optimize performance and reduce costs. Koog provides the <code>CachedPromptExecutor</code>, which is a wrapper around the <code>PromptExecutor</code> that adds caching functionality. It lets you store responses from previously executed prompts and retrieve them when the same prompts are run again.</p> <p>To create a cached prompt executor, perform the following:</p> <ol> <li>Create a prompt executor for which you want to cache responses:</li> </ol> <pre><code>val client = OpenAILLMClient(System.getenv(\"OPENAI_KEY\"))\nval promptExecutor = SingleLLMPromptExecutor(client)\n</code></pre> <ol> <li>Create a <code>CachedPromptExecutor</code> instance with the desired cache and provide the created prompt executor:</li> </ol> <pre><code>val cachedExecutor = CachedPromptExecutor(\n    cache = FilePromptCache(Path(\"/cache_directory\")),\n    nested = promptExecutor\n)\n</code></pre> <ol> <li>Run the cached prompt executor with the desired prompt and model:</li> </ol> <pre><code>val response = cachedExecutor.execute(prompt, OpenAIModels.Chat.GPT4o)\n</code></pre> <p>Now you can run the same prompt with the same model multiple times, the response will be retrieved from the cache.</p> <p>Note</p> <ul> <li>If you call <code>executeStreaming()</code> with the cached prompt executor, it produces a response as a single chunk.</li> <li>If you call <code>moderate()</code> with the cached prompt executor, it forwards the request to the nested prompt executor and does not use the cache.</li> <li>Caching of multiple choice responses is not supported.</li> </ul>"},{"location":"prompt-api/#retry-functionality","title":"Retry functionality","text":"<p>When working with LLM providers, you may encounter transient errors like rate limits or temporary service unavailability. The <code>RetryingLLMClient</code> decorator adds automatic retry logic to any LLM client.</p>"},{"location":"prompt-api/#basic-usage","title":"Basic usage","text":"<p>Wrap any existing client with retry capability:</p> <pre><code>// Wrap any client with retry capability\nval client = OpenAILLMClient(apiKey)\nval resilientClient = RetryingLLMClient(client)\n\n// Now all operations will automatically retry on transient errors\nval response = resilientClient.execute(prompt, OpenAIModels.Chat.GPT4o)\n</code></pre>"},{"location":"prompt-api/#configuring-retry-behavior","title":"Configuring retry behavior","text":"<p>Koog provides several predefined retry configurations:</p> Configuration Max Attempts Initial Delay Max Delay Use Case <code>DISABLED</code> 1 (no retry) - - Development and testing <code>CONSERVATIVE</code> 3 2s 30s Normal production use <code>AGGRESSIVE</code> 5 500ms 20s Critical operations <code>PRODUCTION</code> 3 1s 20s Recommended default <p>You can use them directly or create custom configurations:</p> <pre><code>// Use the predefined configuration\nval conservativeClient = RetryingLLMClient(\n    delegate = client,\n    config = RetryConfig.CONSERVATIVE\n)\n\n// Or create a custom configuration\nval customClient = RetryingLLMClient(\n    delegate = client,\n    config = RetryConfig(\n        maxAttempts = 5,\n        initialDelay = 1.seconds,\n        maxDelay = 30.seconds,\n        backoffMultiplier = 2.0,\n        jitterFactor = 0.2\n    )\n)\n</code></pre>"},{"location":"prompt-api/#retryable-error-patterns","title":"Retryable error patterns","text":"<p>By default, the retry mechanism recognizes common transient errors:</p> <ul> <li> <p>HTTP status codes:</p> <ul> <li><code>429</code>: Rate limit</li> <li><code>500</code>: Internal server error</li> <li><code>502</code>: Bag gateway</li> <li><code>503</code>: Service unavailable</li> <li><code>504</code>: Gateway timeout</li> <li><code>529</code>: Anthropic overloaded</li> </ul> </li> <li> <p>Error keywords:</p> <ul> <li>rate limit</li> <li>too many requests</li> <li>request timeout</li> <li>connection timeout</li> <li>read timeout</li> <li>write timeout</li> <li>connection reset by peer</li> <li>connection refused</li> <li>temporarily unavailable</li> <li>service unavailable</li> </ul> </li> </ul> <p>You can define custom patterns for your specific needs:</p> <pre><code>val config = RetryConfig(\n    retryablePatterns = listOf(\n        RetryablePattern.Status(429),           // Specific status code\n        RetryablePattern.Keyword(\"quota\"),      // Keyword in error message\n        RetryablePattern.Regex(Regex(\"ERR_\\\\d+\")), // Custom regex pattern\n        RetryablePattern.Custom { error -&gt;      // Custom logic\n            error.contains(\"temporary\") &amp;&amp; error.length &gt; 20\n        }\n    )\n)\n</code></pre>"},{"location":"prompt-api/#retry-with-prompt-executors","title":"Retry with prompt executors","text":"<p>When working with prompt executors, you can wrap the underlying LLM client with a retry mechanism before creating the executor:</p> <pre><code>// Single provider executor with retry\nval resilientClient = RetryingLLMClient(\n    OpenAILLMClient(System.getenv(\"OPENAI_KEY\")),\n    RetryConfig.PRODUCTION\n)\nval executor = SingleLLMPromptExecutor(resilientClient)\n\n// Multi-provider executor with flexible client configuration\nval multiExecutor = MultiLLMPromptExecutor(\n    LLMProvider.OpenAI to RetryingLLMClient(\n        OpenAILLMClient(System.getenv(\"OPENAI_KEY\")),\n        RetryConfig.CONSERVATIVE\n    ),\n    LLMProvider.Anthropic to RetryingLLMClient(\n        AnthropicLLMClient(System.getenv(\"ANTHROPIC_API_KEY\")),\n        RetryConfig.AGGRESSIVE  \n    ),\n    // The Bedrock client already has a built-in AWS SDK retry \n    LLMProvider.Bedrock to BedrockLLMClient(\n        awsAccessKeyId = System.getenv(\"AWS_ACCESS_KEY_ID\"),\n        awsSecretAccessKey = System.getenv(\"AWS_SECRET_ACCESS_KEY\"),\n        awsSessionToken = System.getenv(\"AWS_SESSION_TOKEN\"),\n    ))\n</code></pre>"},{"location":"prompt-api/#streaming-with-retry","title":"Streaming with retry","text":"<p>Streaming operations can optionally be retried. This feature is disabled by default.</p> <pre><code>val config = RetryConfig(\n    maxAttempts = 3\n)\n\nval client = RetryingLLMClient(baseClient, config)\nval stream = client.executeStreaming(prompt, OpenAIModels.Chat.GPT4o)\n</code></pre> <p>Note</p> <p>Streaming retry only applies to the connection failures before the first token is received. Once streaming begins, errors are passed through to preserve content integrity.</p>"},{"location":"prompt-api/#timeout-configuration","title":"Timeout configuration","text":"<p>All LLM clients support timeout configuration to prevent hanging requests:</p> <pre><code>val client = OpenAILLMClient(\n    apiKey = apiKey,\n    settings = OpenAIClientSettings(\n        timeoutConfig = ConnectionTimeoutConfig(\n            connectTimeoutMillis = 5000,    // 5 seconds to establish connection\n            requestTimeoutMillis = 60000    // 60 seconds for the entire request\n        )\n    )\n)\n</code></pre>"},{"location":"prompt-api/#error-handling","title":"Error handling","text":"<p>When working with LLMs in production, you need to imlement error-handling strategies:</p> <ul> <li>Use try-catch blocks to handle unexpected errors.</li> <li>Log errors with context for debugging.</li> <li>Implement fallback strategies for critical operations.</li> <li>Monitor retry patterns to identify recurring or systemic issues.</li> </ul> <p>Here is an example of comprehensive error handling:</p> <pre><code>try {\n    val response = resilientClient.execute(prompt, model)\n    processResponse(response)\n} catch (e: Exception) {\n    logger.error(\"LLM operation failed\", e)\n\n    when {\n        e.message?.contains(\"rate limit\") == true -&gt; {\n            // Handle rate limiting specifically\n            scheduleRetryLater()\n        }\n        e.message?.contains(\"invalid api key\") == true -&gt; {\n            // Handle authentication errors\n            notifyAdministrator()\n        }\n        else -&gt; {\n            // Fall back to an alternative solution\n            useDefaultResponse()\n        }\n    }\n}\n</code></pre>"},{"location":"ranked-document-storage/","title":"Document storage","text":"<p>To let you provide up-to-date and searchable information sources for use with Large Language Models (LLMs), Koog supports Resource-Augmented Generation (RAG) to store and retrieve information from documents.</p>"},{"location":"ranked-document-storage/#key-rag-features","title":"Key RAG features","text":"<p>The core components of a common RAG system include:</p> <ul> <li>Document storage: a repository of documents, files, or text chunks that contain information.</li> <li>Vector embeddings: numerical representations of a text that capture semantic meaning. For more information on embeddings in Koog, see Embeddings.</li> <li>Retrieval mechanism: a system that finds the most relevant documents based on a query.</li> <li>Generation component: an LLM that uses the retrieved information to generate responses.</li> </ul> <p>RAG addresses several limitations of traditional LLMs:</p> <ul> <li>Knowledge cutoff: RAG can access the most recent information, not limited to training data.</li> <li>Hallucinations: by grounding responses in retrieved documents, RAG reduces fabricated information.</li> <li>Domain specificity: RAG can be tailored to specific domains by curating the knowledge base.</li> <li>Transparency: the sources of information can be cited, making the system more explainable.</li> </ul>"},{"location":"ranked-document-storage/#finding-information-in-a-rag-system","title":"Finding information in a RAG system","text":"<p>Finding relevant information in a RAG system involves storing documents as vector embeddings and ranking them based on their similarity to a user's query. This approach works with various document types, including PDFs, images, text files, or even individual text chunks.</p> <p>The process involves:</p> <ol> <li>Document embedding: converting documents into vector representations that capture their semantic meaning.</li> <li>Vector storage: storing these embeddings efficiently for quick retrieval.</li> <li>Similarity search: finding documents whose embeddings are most similar to the query embedding.</li> <li>Ranking: ordering documents by their relevance score.</li> </ol>"},{"location":"ranked-document-storage/#implementing-a-rag-system-in-koog","title":"Implementing a RAG system in Koog","text":"<p>To implement a RAG system in Koog, follow the steps below:</p> <ol> <li>Create an embedder using Ollama or OpenAI. The embedder is an instance of the <code>LLMEmbedder</code> class that takes an LLM client instance and model as parameters. For more information, see Embeddings.</li> <li>Create a document embedder based on the created general embedder.</li> <li>Create a document storage.</li> <li>Add documents to the storage.</li> <li>Find the most relevant documents using a defined query.</li> </ol> <p>This sequence of steps represents a relevance search flow that returns the most relevant documents for a given user query. Here is a code sample showing how to implement the entire sequence of steps described above:</p> <pre><code>// Create an embedder using Ollama\nval embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\n// You may also use OpenAI embeddings with:\n// val embedder = LLMEmbedder(OpenAILLMClient(\"API_KEY\"), OpenAIModels.Embeddings.TextEmbeddingAda3Large)\n\n// Create a JVM-specific document embedder\nval documentEmbedder = JVMTextDocumentEmbedder(embedder)\n\n// Create a ranked document storage using in-memory vector storage\nval rankedDocumentStorage = EmbeddingBasedDocumentStorage(documentEmbedder, InMemoryVectorStorage())\n\n// Store documents in the storage\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc1.txt\"))\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc2.txt\"))\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc3.txt\"))\n// ... store more documents as needed\nrankedDocumentStorage.store(Path.of(\"./my/documents/doc100.txt\"))\n\n// Find the most relevant documents for a user query\nval query = \"I want to open a bank account but I'm getting a 404 when I open your website. I used to be your client with a different account 5 years ago before you changed your firm name\"\nval relevantFiles = rankedDocumentStorage.mostRelevantDocuments(query, count = 3)\n\n// Process the relevant files\nrelevantFiles.forEach { file -&gt;\n    println(\"Relevant file: ${file.toAbsolutePath()}\")\n    // Process the file content as needed\n}\n</code></pre>"},{"location":"ranked-document-storage/#providing-relevance-search-for-use-by-ai-agents","title":"Providing relevance search for use by AI agents","text":"<p>Once you have a ranked document storage system, you can use it to provide relevant context to an AI agent for answering user queries. This enhances the agent's ability to provide accurate and contextually appropriate responses.</p> <p>Here is an example of how to implement the defined RAG system for an AI agent to be able to answer queries by getting information from the document storage: </p> <pre><code>suspend fun solveUserRequest(query: String) {\n    // Retrieve top-5 documents from the document provider\n    val relevantDocuments = rankedDocumentStorage.mostRelevantDocuments(query, count = 5)\n\n    // Create an AI Agent with the relevant context\n    val agentConfig = AIAgentConfig(\n        prompt = prompt(\"context\") {\n            system(\"You are a helpful assistant. Use the provided context to answer the user's question accurately.\")\n            user {\n                \"Relevant context\"\n                attachments {\n                    relevantDocuments.forEach {\n                        file(it.pathString, \"text/plain\")\n                    }\n                }\n            }\n        },\n        model = OpenAIModels.Chat.GPT4o, // Or a different model of your choice\n        maxAgentIterations = 100,\n    )\n\n    val agent = AIAgent(\n        executor = simpleOpenAIExecutor(apiKey),\n        llmModel = OpenAIModels.Chat.GPT4o\n    )\n\n\n    // Run the agent to get a response\n    val response = agent.run(query)\n\n    // Return or process the response\n    println(\"Agent response: $response\")\n}\n</code></pre>"},{"location":"ranked-document-storage/#providing-relevance-search-as-a-tool","title":"Providing relevance search as a tool","text":"<p>Instead of directly providing document content as context, you can also implement a tool that allows the agent to perform relevance searches on demand. This gives the agent more flexibility in deciding when and how to use the document storage.</p> <p>Here is an example of how to implement a relevance search tool:</p> <pre><code>@Tool\n@LLMDescription(\"Search for relevant documents about any topic (if exists). Returns the content of the most relevant documents.\")\nsuspend fun searchDocuments(\n    @LLMDescription(\"Query to search relevant documents about\")\n    query: String,\n    @LLMDescription(\"Maximum number of documents\")\n    count: Int\n): String {\n    val relevantDocuments =\n        rankedDocumentStorage.mostRelevantDocuments(query, count = count, similarityThreshold = 0.9).toList()\n\n    if (!relevantDocuments.isEmpty()) {\n        return \"No relevant documents found for the query: $query\"\n    }\n\n    val result = StringBuilder(\"Found ${relevantDocuments.size} relevant documents:\\n\\n\")\n\n    relevantDocuments.forEachIndexed { index, document -&gt;\n        val content = Files.readString(document)\n        result.append(\"Document ${index + 1}: ${document.fileName}\\n\")\n        result.append(\"Content: $content\\n\\n\")\n    }\n\n    return result.toString()\n}\n\nfun main() {\n    runBlocking {\n        val tools = ToolRegistry {\n            tool(::searchDocuments.asTool())\n        }\n\n        val agent = AIAgent(\n            toolRegistry = tools,\n            executor = simpleOpenAIExecutor(apiKey),\n            llmModel = OpenAIModels.Chat.GPT4o\n        )\n\n        val response = agent.run(\"How to make a cake?\")\n        println(\"Agent response: $response\")\n\n    }\n}\n</code></pre> <p>With this approach, the agent can decide when to use the search tool based on your query. This is particularly useful for complex queries that may require information from multiple documents or when the agent needs to search for specific details.</p>"},{"location":"ranked-document-storage/#existing-implementations-of-vector-storage-and-document-embedding-providers","title":"Existing implementations of vector storage and document embedding providers","text":"<p>For convenience and easier implementation of a RAG system, Koog provides several out-of-the-box implementations for vector storage, document embedding, and combined embedding and storage components.</p>"},{"location":"ranked-document-storage/#vector-storage","title":"Vector storage","text":""},{"location":"ranked-document-storage/#inmemoryvectorstorage","title":"InMemoryVectorStorage","text":"<p>A simple in-memory implementation that stores documents and their vector embeddings in memory. Suitable for testing or small-scale applications.</p> <pre><code>val inMemoryStorage = InMemoryVectorStorage&lt;Path&gt;()\n</code></pre> <p>For more information, see the InMemoryVectorStorage reference.</p>"},{"location":"ranked-document-storage/#filevectorstorage","title":"FileVectorStorage","text":"<p>A file-based implementation that stores documents and their vector embeddings on disk. Suitable for persistent storage across application restarts.</p> <pre><code>val fileStorage = FileVectorStorage&lt;Document, Path&gt;(\n   documentReader = documentProvider,\n   fs = fileSystemProvider,\n   root = rootPath\n)\n</code></pre> <p>For more information, see the FileVectorStorage reference.</p>"},{"location":"ranked-document-storage/#jvmfilevectorstorage","title":"JVMFileVectorStorage","text":"<p>A JVM-specific implementation of <code>FileVectorStorage</code> that works with <code>java.nio.file.Path</code>.</p> <pre><code>val jvmFileStorage = JVMFileVectorStorage(root = Path.of(\"/path/to/storage\"))\n</code></pre> <p>For more information, see the JVMFileVectorStorage reference.</p>"},{"location":"ranked-document-storage/#document-embedder","title":"Document embedder","text":""},{"location":"ranked-document-storage/#textdocumentembedder","title":"TextDocumentEmbedder","text":"<p>A generic implementation that works with any document type that can be converted to text.</p> <pre><code>val textEmbedder = TextDocumentEmbedder&lt;Document, Path&gt;(\n   documentReader = documentProvider,\n   embedder = embedder\n)\n</code></pre> <p>For more information, see the TextDocumentEmbedder reference.</p>"},{"location":"ranked-document-storage/#jvmtextdocumentembedder","title":"JVMTextDocumentEmbedder","text":"<p>A JVM-specific implementation that works with <code>java.nio.file.Path</code>.</p> <pre><code>val embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\nval jvmTextEmbedder = JVMTextDocumentEmbedder(embedder = embedder)\n</code></pre> <p>For more information, see the JVMTextDocumentEmbedder reference.</p>"},{"location":"ranked-document-storage/#combined-storage-implementations","title":"Combined storage implementations","text":""},{"location":"ranked-document-storage/#embeddingbaseddocumentstorage","title":"EmbeddingBasedDocumentStorage","text":"<p>Combines a document embedder and a vector storage to provide a complete solution for storing and ranking documents.</p> <pre><code>val embeddingStorage = EmbeddingBasedDocumentStorage(\n    embedder = documentEmbedder,\n    storage = vectorStorage\n)\n</code></pre> <p>For more information, see the EmbeddingBasedDocumentStorage reference.</p>"},{"location":"ranked-document-storage/#inmemorydocumentembeddingstorage","title":"InMemoryDocumentEmbeddingStorage","text":"<p>An in-memory implementation of <code>EmbeddingBasedDocumentStorage</code>.</p> <pre><code>val inMemoryEmbeddingStorage = InMemoryDocumentEmbeddingStorage&lt;Document&gt;(\n    embedder = documentEmbedder\n)\n</code></pre> <p>For more information, see the InMemoryDocumentEmbeddingStorage reference.</p>"},{"location":"ranked-document-storage/#filedocumentembeddingstorage","title":"FileDocumentEmbeddingStorage","text":"<p>A file-based implementation of <code>EmbeddingBasedDocumentStorage</code>.</p> <pre><code>val fileEmbeddingStorage = FileDocumentEmbeddingStorage&lt;Document, Path&gt;(\n   embedder = documentEmbedder,\n   documentProvider = documentProvider,\n   fs = fileSystemProvider,\n   root = rootPath\n)\n</code></pre> <p>For more information, see the FileDocumentEmbeddingStorage reference.</p>"},{"location":"ranked-document-storage/#jvmfiledocumentembeddingstorage","title":"JVMFileDocumentEmbeddingStorage","text":"<p>A JVM-specific implementation of <code>FileDocumentEmbeddingStorage</code>.</p> <pre><code>val jvmFileEmbeddingStorage = JVMFileDocumentEmbeddingStorage(\n   embedder = documentEmbedder,\n   root = Path.of(\"/path/to/storage\")\n)\n</code></pre> <p>For more information, see the JVMFileDocumentEmbeddingStorage reference.</p>"},{"location":"ranked-document-storage/#jvmtextfiledocumentembeddingstorage","title":"JVMTextFileDocumentEmbeddingStorage","text":"<p>A JVM-specific implementation that combines <code>JVMTextDocumentEmbedder</code> and <code>JVMFileVectorStorage</code>.</p> <pre><code>val jvmTextFileEmbeddingStorage = JVMTextFileDocumentEmbeddingStorage(\n   embedder = embedder,\n   root = Path.of(\"/path/to/storage\")\n)\n</code></pre> <p>For more information, see the JVMTextFileDocumentEmbeddingStorage reference.</p> <p>These implementations provide a flexible and extensible framework for working with document embeddings and vector storage in various environments.</p>"},{"location":"ranked-document-storage/#implementing-your-own-vector-storage-and-document-embedder","title":"Implementing your own vector storage and document embedder","text":"<p>You can extend Koog's vector storage framework by implementing your own custom document embedders and vector storage solutions. This is particularly useful when working with specialized document types or storage requirements.</p> <p>Here's an example of implementing a custom document embedder for PDF documents:</p> <pre><code>// Define a PDFDocument class\nclass PDFDocument(private val path: Path) {\n    fun readText(): String {\n        // Use a PDF library to extract text from the PDF\n        return \"Text extracted from PDF at $path\"\n    }\n}\n\n// Implement a DocumentProvider for PDFDocument\nclass PDFDocumentProvider : DocumentProvider&lt;Path, PDFDocument&gt; {\n    override suspend fun document(path: Path): PDFDocument? {\n        return if (path.toString().endsWith(\".pdf\")) {\n            PDFDocument(path)\n        } else {\n            null\n        }\n    }\n\n    override suspend fun text(document: PDFDocument): CharSequence {\n        return document.readText()\n    }\n}\n\n// Implement a DocumentEmbedder for PDFDocument\nclass PDFDocumentEmbedder(private val embedder: Embedder) : DocumentEmbedder&lt;PDFDocument&gt; {\n    override suspend fun embed(document: PDFDocument): Vector {\n        val text = document.readText()\n        return embed(text)\n    }\n\n    override suspend fun embed(text: String): Vector {\n        return embedder.embed(text)\n    }\n\n    override fun diff(embedding1: Vector, embedding2: Vector): Double {\n        return embedder.diff(embedding1, embedding2)\n    }\n}\n\n// Create a custom vector storage for PDF documents\nclass PDFVectorStorage(\n    private val pdfProvider: PDFDocumentProvider,\n    private val embedder: PDFDocumentEmbedder,\n    private val storage: VectorStorage&lt;PDFDocument&gt;\n) : RankedDocumentStorage&lt;PDFDocument&gt; {\n    override fun rankDocuments(query: String): Flow&lt;RankedDocument&lt;PDFDocument&gt;&gt; = flow {\n        val queryVector = embedder.embed(query)\n        storage.allDocumentsWithPayload().collect { (document, documentVector) -&gt;\n            emit(\n                RankedDocument(\n                    document = document,\n                    similarity = 1.0 - embedder.diff(queryVector, documentVector)\n                )\n            )\n        }\n    }\n\n    override suspend fun store(document: PDFDocument, data: Unit): String {\n        val vector = embedder.embed(document)\n        return storage.store(document, vector)\n    }\n\n    override suspend fun delete(documentId: String): Boolean {\n        return storage.delete(documentId)\n    }\n\n    override suspend fun read(documentId: String): PDFDocument? {\n        return storage.read(documentId)\n    }\n\n    override fun allDocuments(): Flow&lt;PDFDocument&gt; = flow {\n        storage.allDocumentsWithPayload().collect {\n            emit(it.document)\n        }\n    }\n}\n\n// Usage example\nsuspend fun main() {\n    val pdfProvider = PDFDocumentProvider()\n    val embedder = LLMEmbedder(OllamaClient(), OllamaEmbeddingModels.NOMIC_EMBED_TEXT)\n    val pdfEmbedder = PDFDocumentEmbedder(embedder)\n    val storage = InMemoryVectorStorage&lt;PDFDocument&gt;()\n    val pdfStorage = PDFVectorStorage(pdfProvider, pdfEmbedder, storage)\n\n    // Store PDF documents\n    val pdfDocument = PDFDocument(Path.of(\"./documents/sample.pdf\"))\n    pdfStorage.store(pdfDocument)\n\n    // Query for relevant PDF documents\n    val relevantPDFs = pdfStorage.mostRelevantDocuments(\"information about climate change\", count = 3)\n\n}\n</code></pre>"},{"location":"ranked-document-storage/#implementing-custom-non-embedding-based-rankeddocumentstorage","title":"Implementing custom non-embedding-based RankedDocumentStorage","text":"<p>While embedding-based document ranking is powerful, there are scenarios where you might want to implement a custom ranking mechanism that does not rely on embeddings. For example, you might want to rank documents based on:</p> <ul> <li>PageRank-like algorithms</li> <li>Keyword frequency</li> <li>Recency of documents</li> <li>User interaction history</li> <li>Domain-specific heuristics</li> </ul> <p>Here's an example of implementing a custom <code>RankedDocumentStorage</code> that uses a simple keyword-based ranking approach:</p> <pre><code>class KeywordBasedDocumentStorage&lt;Document&gt;(\n    private val documentProvider: DocumentProvider&lt;Path, Document&gt;,\n    private val storage: DocumentStorage&lt;Document&gt;\n) : RankedDocumentStorage&lt;Document&gt; {\n\n    override fun rankDocuments(query: String): Flow&lt;RankedDocument&lt;Document&gt;&gt; = flow {\n        // Split the query into keywords\n        val keywords = query.lowercase().split(Regex(\"\\\\W+\")).filter { it.length &gt; 2 }\n\n        // Process each document\n        storage.allDocuments().collect { document -&gt;\n            // Get the document text\n            val documentText = documentProvider.text(document).toString().lowercase()\n\n            // Calculate a simple similarity score based on keyword frequency\n            var similarity = 0.0\n            for (keyword in keywords) {\n                val count = countOccurrences(documentText, keyword)\n                if (count &gt; 0) {\n                    similarity += count.toDouble() / documentText.length * 1000\n                }\n            }\n\n            // Emit the document with its similarity score\n            emit(RankedDocument(document, similarity))\n        }\n    }\n\n    private fun countOccurrences(text: String, keyword: String): Int {\n        var count = 0\n        var index = 0\n        while (index != -1) {\n            index = text.indexOf(keyword, index)\n            if (index != -1) {\n                count++\n                index += keyword.length\n            }\n        }\n        return count\n    }\n\n    override suspend fun store(document: Document, data: Unit): String {\n        return storage.store(document)\n    }\n\n    override suspend fun delete(documentId: String): Boolean {\n        return storage.delete(documentId)\n    }\n\n    override suspend fun read(documentId: String): Document? {\n        return storage.read(documentId)\n    }\n\n    override fun allDocuments(): Flow&lt;Document&gt; {\n        return storage.allDocuments()\n    }\n}\n</code></pre> <p>This implementation ranks documents based on the frequency of keywords from the query appearing in the document text. You could extend this approach with more sophisticated algorithms like TF-IDF (Term Frequency-Inverse Document Frequency) or BM25.</p> <p>Another example is a time-based ranking system that prioritizes recent documents:</p> <pre><code>class TimeBasedDocumentStorage&lt;Document&gt;(\n    private val storage: DocumentStorage&lt;Document&gt;,\n    private val getDocumentTimestamp: (Document) -&gt; Long\n) : RankedDocumentStorage&lt;Document&gt; {\n\n    override fun rankDocuments(query: String): Flow&lt;RankedDocument&lt;Document&gt;&gt; = flow {\n        val currentTime = System.currentTimeMillis()\n\n        storage.allDocuments().collect { document -&gt;\n            val timestamp = getDocumentTimestamp(document)\n            val ageInHours = (currentTime - timestamp) / (1000.0 * 60 * 60)\n\n            // Calculate a decay factor based on age (newer documents get higher scores)\n            val decayFactor = Math.exp(-0.01 * ageInHours)\n\n            emit(RankedDocument(document, decayFactor))\n        }\n    }\n\n    // Implement other required methods from RankedDocumentStorage\n    override suspend fun store(document: Document, data: Unit): String {\n        return storage.store(document)\n    }\n\n    override suspend fun delete(documentId: String): Boolean {\n        return storage.delete(documentId)\n    }\n\n    override suspend fun read(documentId: String): Document? {\n        return storage.read(documentId)\n    }\n\n    override fun allDocuments(): Flow&lt;Document&gt; {\n        return storage.allDocuments()\n    }\n}\n</code></pre> <p>By implementing the <code>RankedDocumentStorage</code> interface, you can create custom ranking mechanisms tailored to your specific use case while still leveraging the rest of the RAG infrastructure.</p> <p>The flexibility of Koog's design allows you to mix and match different storage and ranking strategies to build a system that meets your specific requirements.</p>"},{"location":"sessions/","title":"LLM sessions and manual history management","text":"<p>This page provides detailed information about LLM sessions, including how to work with read and write sessions, manage conversation history, and make requests to language models.</p>"},{"location":"sessions/#introduction","title":"Introduction","text":"<p>LLM sessions are a fundamental concept that provides a structured way to interact with language models (LLMs).  They manage the conversation history, handle requests to the LLM, and provide a consistent interface for running tools and processing responses.</p>"},{"location":"sessions/#understanding-llm-sessions","title":"Understanding LLM sessions","text":"<p>An LLM session represents a context for interacting with a language model. It encapsulates:</p> <ul> <li>The conversation history (prompt)</li> <li>Available tools</li> <li>Methods for making requests to the LLM</li> <li>Methods for updating the conversation history</li> <li>Methods for running tools</li> </ul> <p>Sessions are managed by the <code>AIAgentLLMContext</code> class, which provides methods for creating read and write sessions.</p>"},{"location":"sessions/#session-types","title":"Session types","text":"<p>The Koog framework provides two types of sessions:</p> <ol> <li> <p>Write Sessions (<code>AIAgentLLMWriteSession</code>): Allow modifying the prompt and tools, making LLM requests, and    running tools. Changes made in a write session are persisted back to the LLM context.</p> </li> <li> <p>Read Sessions (<code>AIAgentLLMReadSession</code>): Provide read-only access to the prompt and tools. They are useful for    inspecting the current state without making changes.</p> </li> </ol> <p>The key difference is that write sessions can modify the conversation history, while read sessions cannot.</p>"},{"location":"sessions/#session-lifecycle","title":"Session lifecycle","text":"<p>Sessions have a defined lifecycle:</p> <ol> <li>Creation: a session is created using <code>llm.writeSession { ... }</code> or <code>llm.readSession { ... }</code>.</li> <li>Active phase: the session is active while the lambda block is executing.</li> <li>Termination: the session is automatically closed when the lambda block completes.</li> </ol> <p>Sessions implement the <code>AutoCloseable</code> interface, ensuring they are properly cleaned up even if an exception occurs.</p>"},{"location":"sessions/#working-with-llm-sessions","title":"Working with LLM sessions","text":""},{"location":"sessions/#creating-sessions","title":"Creating sessions","text":"<p>Sessions are created using extension functions on the <code>AIAgentLLMContext</code> class:</p> <pre><code>// Creating a write session\nllm.writeSession {\n    // Session code here\n}\n\n// Creating a read session\nllm.readSession {\n    // Session code here\n}\n</code></pre> <p>These functions take a lambda block that runs within the context of the session. The session is automatically closed when the block completes.</p>"},{"location":"sessions/#session-scope-and-thread-safety","title":"Session scope and thread safety","text":"<p>Sessions use a read-write lock to ensure thread safety:</p> <ul> <li>Multiple read sessions can be active simultaneously.</li> <li>Only one write session can be active at a time.</li> <li>A write session blocks all other sessions (both read and write).</li> </ul> <p>This ensures that the conversation history is not corrupted by concurrent modifications.</p>"},{"location":"sessions/#accessing-session-properties","title":"Accessing session properties","text":"<p>Within a session, you can access the prompt and tools:</p> <pre><code>llm.readSession {\n    val messageCount = prompt.messages.size\n    val availableTools = tools.map { it.name }\n}\n</code></pre> <p>In a write session, you can also modify these properties:</p> <pre><code>llm.writeSession {\n    // Modify the prompt\n    updatePrompt {\n        user(\"New user message\")\n    }\n\n    // Modify the tools\n    tools = newTools\n}\n</code></pre> <p>For more information, see the detailed API reference for AIAgentLLMReadSession and AIAgentLLMWriteSession.</p>"},{"location":"sessions/#making-llm-requests","title":"Making LLM requests","text":""},{"location":"sessions/#basic-request-methods","title":"Basic request methods","text":"<p>The most common methods for making LLM requests are:</p> <ol> <li> <p><code>requestLLM()</code>: makes a request to the LLM with the current prompt and tools, returning a single response.</p> </li> <li> <p><code>requestLLMMultiple()</code>: makes a request to the LLM with the current prompt and tools, returning multiple    responses.</p> </li> <li> <p><code>requestLLMWithoutTools()</code>: makes a request to the LLM with the current prompt but without any tools, returning a    single response.</p> </li> <li> <p><code>requestLLMForceOneTool</code>: makes a request to the LLM with the current prompt and tools, forcing the use of one tool.</p> </li> <li> <p><code>requestLLMOnlyCallingTools</code>: makes a request to the LLM that should be processed by only using tools.</p> </li> </ol> <p>Example:</p> <pre><code>llm.writeSession {\n    // Make a request with tools enabled\n    val response = requestLLM()\n\n    // Make a request without tools\n    val responseWithoutTools = requestLLMWithoutTools()\n\n    // Make a request that returns multiple responses\n    val responses = requestLLMMultiple()\n}\n</code></pre>"},{"location":"sessions/#how-requests-work","title":"How requests work","text":"<p>LLM requests are made when you explicitly call one of the request methods. The key points to understand are:</p> <ol> <li>Explicit invocation: requests only happen when you call methods like <code>requestLLM()</code>, <code>requestLLMWithoutTools()</code> and so on.</li> <li>Immediate execution: when you call a request method, the request is made immediately, and the method blocks until a response is received.</li> <li>Automatic history update: in a write session, the response is automatically added to the conversation history.</li> <li>No implicit requests: the system does not make implicit requests; you need to explicitly call a request method.</li> </ol>"},{"location":"sessions/#request-methods-with-tools","title":"Request methods with tools","text":"<p>When making requests with tools enabled, the LLM may respond with a tool call instead of a text response. The request methods handle this transparently:</p> <pre><code>llm.writeSession {\n    val response = requestLLM()\n\n    // The response might be a tool call or a text response\n    if (response is Message.Tool.Call) {\n        // Handle tool call\n    } else {\n        // Handle text response\n    }\n}\n</code></pre> <p>In practice, you typically do not need to check the response type manually, as the agent graph handles this routing automatically.</p>"},{"location":"sessions/#structured-and-streaming-requests","title":"Structured and streaming requests","text":"<p>For more advanced use cases, the platform provides methods for structured and streaming requests:</p> <ol> <li> <p><code>requestLLMStructured()</code>: requests the LLM to provide a response in a specific structured format.</p> </li> <li> <p><code>requestLLMStructuredOneShot()</code>: similar to <code>requestLLMStructured()</code> but without retries or corrections.</p> </li> <li> <p><code>requestLLMStreaming()</code>: makes a streaming request to the LLM, returning a flow of response chunks.</p> </li> </ol> <p>Example:</p> <pre><code>llm.writeSession {\n    // Make a structured request\n    val structuredResponse = requestLLMStructured&lt;JokeRating&gt;()\n\n    // Make a streaming request\n    val responseStream = requestLLMStreaming()\n    responseStream.collect { chunk -&gt;\n        // Process each chunk as it arrives\n    }\n}\n</code></pre>"},{"location":"sessions/#managing-conversation-history","title":"Managing conversation history","text":""},{"location":"sessions/#updating-the-prompt","title":"Updating the prompt","text":"<p>In a write session, you can update the prompt (conversation history) using the <code>updatePrompt</code> method:</p> <pre><code>llm.writeSession {\n    updatePrompt {\n        // Add a system message\n        system(\"You are a helpful assistant.\")\n\n        // Add a user message\n        user(\"Hello, can you help me with a coding question?\")\n\n        // Add an assistant message\n        assistant(\"Of course! What's your question?\")\n\n        // Add a tool result\n        tool {\n            result(myToolResult)\n        }\n    }\n}\n</code></pre> <p>You can also completely rewrite the prompt using the <code>rewritePrompt</code> method:</p> <pre><code>llm.writeSession {\n    rewritePrompt { oldPrompt -&gt;\n        // Create a new prompt based on the old one\n        oldPrompt.copy(messages = filteredMessages)\n    }\n}\n</code></pre>"},{"location":"sessions/#automatic-history-update-on-response","title":"Automatic history update on response","text":"<p>When you make an LLM request in a write session, the response is automatically added to the conversation history:</p> <pre><code>llm.writeSession {\n    // Add a user message\n    updatePrompt {\n        user(\"What's the capital of France?\")\n    }\n\n    // Make a request \u2013 the response is automatically added to the history\n    val response = requestLLM()\n\n    // The prompt now includes both the user message and the model's response\n}\n</code></pre> <p>This automatic history update is the key feature of write sessions, ensuring that the conversation flows naturally.</p>"},{"location":"sessions/#history-compression","title":"History compression","text":"<p>For long-running conversations, the history can grow large and consume a lot of tokens. The platform provides methods for compressing history:</p> <pre><code>llm.writeSession {\n    // Compress the history using a TLDR approach\n    replaceHistoryWithTLDR(HistoryCompressionStrategy.WholeHistory, preserveMemory = true)\n}\n</code></pre> <p>You can also use the <code>nodeLLMCompressHistory</code> node in a strategy graph to compress history at specific points.</p> <p>For more information about history compression and compression strategies, see History compression.</p>"},{"location":"sessions/#running-tools-in-sessions","title":"Running tools in sessions","text":""},{"location":"sessions/#calling-tools","title":"Calling tools","text":"<p>Write sessions provide several methods for calling tools:</p> <ol> <li> <p><code>callTool(tool, args)</code>: calls a tool by reference.</p> </li> <li> <p><code>callTool(toolName, args)</code>: calls a tool by name.</p> </li> <li> <p><code>callTool(toolClass, args)</code>: calls a tool by class.</p> </li> <li> <p><code>callToolRaw(toolName, args)</code>: calls a tool by name and returns the raw string result.</p> </li> </ol> <p>Example:</p> <pre><code>llm.writeSession {\n    // Call a tool by reference\n    val result = callTool(myTool, myArgs)\n\n    // Call a tool by name\n    val result2 = callTool(\"myToolName\", myArgs)\n\n    // Call a tool by class\n    val result3 = callTool(MyTool::class, myArgs)\n\n    // Call a tool and get the raw result\n    val rawResult = callToolRaw(\"myToolName\", myArgs)\n}\n</code></pre>"},{"location":"sessions/#parallel-tool-runs","title":"Parallel tool runs","text":"<p>To run multiple tools in parallel, write sessions provide extension functions on <code>Flow</code>:</p> <pre><code>llm.writeSession {\n    // Run tools in parallel\n    parseDataToArgs(data).toParallelToolCalls(MyTool::class).collect { result -&gt;\n        // Process each result\n    }\n\n    // Run tools in parallel and get raw results\n    parseDataToArgs(data).toParallelToolCallsRaw(MyTool::class).collect { rawResult -&gt;\n        // Process each raw result\n    }\n}\n</code></pre> <p>This is useful for processing large amounts of data efficiently.</p>"},{"location":"sessions/#best-practices","title":"Best practices","text":"<p>When working with LLM sessions, follow these best practices:</p> <ol> <li> <p>Use the right session type: Use write sessions when you need to modify the conversation history and read    sessions when you only need to read it.</p> </li> <li> <p>Keep sessions short: Sessions should be focused on a specific task and closed as soon as possible to release    resources.</p> </li> <li> <p>Handle exceptions: Make sure to handle exceptions within sessions to prevent resource leaks.</p> </li> <li> <p>Manage history size: For long-running conversations, use history compression to reduce token usage.</p> </li> <li> <p>Prefer high-Level abstractions: When possible, use the node-based API. For example, <code>nodeLLMRequest</code> instead of directly working with sessions.</p> </li> <li> <p>Be mindful of thread safety: Remember that write sessions block other sessions, so keep write operations as short    as possible.</p> </li> <li> <p>Use structured requests for complex data: When you need the LLM to return structured data, use    <code>requestLLMStructured</code> instead of parsing free-form text.</p> </li> <li> <p>Use streaming for long responses: For long responses, use <code>requestLLMStreaming</code> to process the response as it    arrives.</p> </li> </ol>"},{"location":"sessions/#troubleshooting","title":"Troubleshooting","text":""},{"location":"sessions/#session-already-closed","title":"Session already closed","text":"<p>If you see an error such as <code>Cannot use session after it was closed</code>, you are trying to use a session after its lambda  block has completed. Make sure all session operations are performed within the session block.</p>"},{"location":"sessions/#history-too-large","title":"History too large","text":"<p>If your history becomes too large and consumes too many tokens, use history compression techniques:</p> <pre><code>llm.writeSession {\n    replaceHistoryWithTLDR(HistoryCompressionStrategy.FromLastNMessages(10), preserveMemory = true)\n}\n</code></pre> <p>For more information, see History compression</p>"},{"location":"sessions/#tool-not-found","title":"Tool not found","text":"<p>If you see errors about tools not being found, check that:</p> <ul> <li>The tool is correctly registered in the tool registry.</li> <li>You are using the correct tool name or class.</li> </ul>"},{"location":"sessions/#api-documentation","title":"API documentation","text":"<p>For more information, see the full AIAgentLLMSession and AIAgentLLMContext reference.</p>"},{"location":"single-run-agents/","title":"Single-run agents","text":"<p>The <code>AIAgent</code> class is the core component that lets you create AI agents in your Kotlin applications.</p> <p>You can build simple agents with minimal configuration or create sophisticated agents with advanced capabilities by defining custom strategies, tools, configurations, and custom input/output types.</p> <p>This page guides you through the steps necessary to create a single-run agent with customizable tools and configurations.</p> <p>A single-run agent processes a single input and provides a response. It operates within a single cycle of tool-calling to complete its task and provide a response. This agent can return either a message or a tool result. The tool result is returned if the tool registry is provided to the agent.</p> <p>If your goal is to build a simple agent to experiment with, you can provide only a prompt executor and LLM when creating it. But if you want more flexibility and customization, you can pass optional parameters to configure the agent. To learn more about configuration options, see API reference.</p>"},{"location":"single-run-agents/#prerequisites","title":"Prerequisites","text":"<ul> <li>You have a valid API key from the LLM provider used to implement an AI agent. For a list of all available providers, see Overview.</li> </ul> <p>Tip</p> <p>Use environment variables or a secure configuration management system to store your API keys. Avoid hardcoding API keys directly in your source code.</p>"},{"location":"single-run-agents/#creating-a-single-run-agent","title":"Creating a single-run agent","text":""},{"location":"single-run-agents/#1-add-dependencies","title":"1. Add dependencies","text":"<p>To use the <code>AIAgent</code> functionality, include all necessary dependencies in your build configuration:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-agents:VERSION\")\n}\n</code></pre> <p>For all available installation methods, see Installation.</p>"},{"location":"single-run-agents/#2-create-an-agent","title":"2. Create an agent","text":"<p>To create an agent, create an instance of the <code>AIAgent</code> class and provide the <code>executor</code> and <code>llmModel</code> parameters:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Chat.GPT4o\n)\n</code></pre>"},{"location":"single-run-agents/#3-add-a-system-prompt","title":"3. Add a system prompt","text":"<p>A system prompt is used to define agent behavior. To provide the prompt, use the <code>systemPrompt</code> parameter:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o\n)\n</code></pre>"},{"location":"single-run-agents/#4-configure-llm-output","title":"4. Configure LLM output","text":"<p>Provide a temperature of LLM output generation using the <code>temperature</code> parameter:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7\n)\n</code></pre>"},{"location":"single-run-agents/#5-add-tools","title":"5. Add tools","text":"<p>Agents use tools to complete specific tasks. You can use the built-in tools or implement your own custom tools if needed.</p> <p>To configure tools, use the <code>toolRegistry</code> parameter that defines the tools available to the agent:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7,\n    toolRegistry = ToolRegistry {\n        tool(SayToUser)\n    }\n)\n</code></pre> <p>In the example, <code>SayToUser</code> is the built-in tool. To learn how to create a custom tool, see Tools.</p>"},{"location":"single-run-agents/#6-adjust-agent-iterations","title":"6. Adjust agent iterations","text":"<p>Provide the maximum number of steps the agent can take before it is forced to stop using the <code>maxIterations</code> parameter:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"YOUR_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7,\n    toolRegistry = ToolRegistry {\n        tool(SayToUser)\n    },\n    maxIterations = 30\n)\n</code></pre>"},{"location":"single-run-agents/#7-handle-events-during-agent-runtime","title":"7. Handle events during agent runtime","text":"<p>Single-run agents support custom event handlers. While having an event handler is not required for creating an agent, it might be helpful for testing, debugging, or making hooks for chained agent interactions.</p> <p>For more information on how to use the <code>EventHandler</code> feature for monitoring your agent interactions, see Agent events.</p>"},{"location":"single-run-agents/#8-run-the-agent","title":"8. Run the agent","text":"<p>To run the agent, use the <code>run()</code> function:</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant. Answer user questions concisely.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    temperature = 0.7,\n    toolRegistry = ToolRegistry {\n        tool(SayToUser)\n    },\n    maxIterations = 100\n)\n\nfun main() = runBlocking {\n    val result = agent.run(\"Hello! How can you help me?\")\n}\n</code></pre> <p>The agent produces the following output:</p> <pre><code>Agent says: Hello! I'm here to assist you with a variety of tasks. Whether you have questions, need information, or require help with specific tasks, feel free to ask. How can I assist you today?\n</code></pre>"},{"location":"spring-boot/","title":"Spring Boot Integration","text":"<p>Koog provides seamless Spring Boot integration through its auto-configuration starter, making it easy to incorporate AI agents into your Spring Boot applications with minimal setup.</p>"},{"location":"spring-boot/#overview","title":"Overview","text":"<p>The <code>koog-spring-boot-starter</code> automatically configures LLM clients based on your application properties and provides ready-to-use beans for dependency injection. It supports all major LLM providers including OpenAI, Anthropic, Google, OpenRouter, DeepSeek, and Ollama.</p>"},{"location":"spring-boot/#getting-started","title":"Getting Started","text":""},{"location":"spring-boot/#1-add-dependency","title":"1. Add Dependency","text":"<p>Add the Spring Boot starter to your <code>build.gradle.kts</code>:</p> <pre><code>dependencies {\n    implementation(\"ai.koog:koog-spring-boot-starter:$koogVersion\")\n}\n</code></pre>"},{"location":"spring-boot/#2-configure-providers","title":"2. Configure Providers","text":"<p>Configure your preferred LLM providers in <code>application.properties</code>:</p> <pre><code># OpenAI Configuration\nai.koog.openai.api-key=${OPENAI_API_KEY}\nai.koog.openai.base-url=https://api.openai.com\n# Anthropic Configuration  \nai.koog.anthropic.api-key=${ANTHROPIC_API_KEY}\nai.koog.anthropic.base-url=https://api.anthropic.com\n# Google Configuration\nai.koog.google.api-key=${GOOGLE_API_KEY}\nai.koog.google.base-url=https://generativelanguage.googleapis.com\n# OpenRouter Configuration\nai.koog.openrouter.api-key=${OPENROUTER_API_KEY}\nai.koog.openrouter.base-url=https://openrouter.ai\n# DeepSeek Configuration\nai.koog.deepseek.api-key=${DEEPSEEK_API_KEY}\nai.koog.deepseek.base-url=https://api.deepseek.com\n# Ollama Configuration (local - no API key required)\nai.koog.ollama.base-url=http://localhost:11434\n</code></pre> <p>Or using YAML format (<code>application.yml</code>):</p> <pre><code>ai:\n    koog:\n        openai:\n            api-key: ${OPENAI_API_KEY}\n            base-url: https://api.openai.com\n        anthropic:\n            api-key: ${ANTHROPIC_API_KEY}\n            base-url: https://api.anthropic.com\n        google:\n            api-key: ${GOOGLE_API_KEY}\n            base-url: https://generativelanguage.googleapis.com\n        openrouter:\n            api-key: ${OPENROUTER_API_KEY}\n            base-url: https://openrouter.ai\n        deepseek:\n            api-key: ${DEEPSEEK_API_KEY}\n            base-url: https://api.deepseek.com\n        ollama:\n            base-url: http://localhost:11434\n</code></pre> <p>Environment Variables</p> <p>It's recommended to use environment variables for API keys to keep them secure and out of version control.</p>"},{"location":"spring-boot/#3-inject-and-use","title":"3. Inject and Use","text":"<p>Inject the auto-configured executors into your services:</p> <pre><code>@Service\nclass AIService(\n    private val openAIExecutor: SingleLLMPromptExecutor?,\n    private val anthropicExecutor: SingleLLMPromptExecutor?\n) {\n\n    suspend fun generateResponse(input: String): String {\n        val prompt = prompt {\n            system(\"You are a helpful AI assistant\")\n            user(input)\n        }\n\n        return when {\n            openAIExecutor != null -&gt; {\n                val result = openAIExecutor.execute(prompt)\n                result.text\n            }\n            anthropicExecutor != null -&gt; {\n                val result = anthropicExecutor.execute(prompt)\n                result.text\n            }\n            else -&gt; throw IllegalStateException(\"No LLM provider configured\")\n        }\n    }\n}\n</code></pre>"},{"location":"spring-boot/#advanced-usage","title":"Advanced Usage","text":""},{"location":"spring-boot/#rest-controller-example","title":"REST Controller Example","text":"<p>Create a chat endpoint using auto-configured executors:</p> <pre><code>@RestController\n@RequestMapping(\"/api/chat\")\nclass ChatController(\n    private val anthropicExecutor: SingleLLMPromptExecutor?\n) {\n\n    @PostMapping\n    suspend fun chat(@RequestBody request: ChatRequest): ResponseEntity&lt;ChatResponse&gt; {\n        return if (anthropicExecutor != null) {\n            try {\n                val prompt = prompt {\n                    system(\"You are a helpful assistant\")\n                    user(request.message)\n                }\n\n                val result = anthropicExecutor.execute(prompt)\n                ResponseEntity.ok(ChatResponse(result.text))\n            } catch (e: Exception) {\n                ResponseEntity.status(HttpStatus.INTERNAL_SERVER_ERROR)\n                    .body(ChatResponse(\"Error processing request\"))\n            }\n        } else {\n            ResponseEntity.status(HttpStatus.SERVICE_UNAVAILABLE)\n                .body(ChatResponse(\"AI service not configured\"))\n        }\n    }\n}\n\ndata class ChatRequest(val message: String)\ndata class ChatResponse(val response: String)\n</code></pre>"},{"location":"spring-boot/#multiple-provider-support","title":"Multiple Provider Support","text":"<p>Handle multiple providers with fallback logic:</p> <pre><code>@Service\nclass RobustAIService(\n    private val openAIExecutor: SingleLLMPromptExecutor?,\n    private val anthropicExecutor: SingleLLMPromptExecutor?,\n    private val openRouterExecutor: SingleLLMPromptExecutor?\n) {\n\n    suspend fun generateWithFallback(input: String): String {\n        val prompt = prompt {\n            system(\"You are a helpful AI assistant\")\n            user(input)\n        }\n\n        val executors = listOfNotNull(openAIExecutor, anthropicExecutor, openRouterExecutor)\n\n        for (executor in executors) {\n            try {\n                val result = executor.execute(prompt)\n                return result.text\n            } catch (e: Exception) {\n                logger.warn(\"Executor failed, trying next: ${e.message}\")\n                continue\n            }\n        }\n\n        throw IllegalStateException(\"All AI providers failed\")\n    }\n\n    companion object {\n        private val logger = LoggerFactory.getLogger(RobustAIService::class.java)\n    }\n}\n</code></pre>"},{"location":"spring-boot/#configuration-properties","title":"Configuration Properties","text":"<p>You can also inject configuration properties for custom logic:</p> <pre><code>@Service\nclass ConfigurableAIService(\n    private val openAIExecutor: SingleLLMPromptExecutor?,\n    @Value(\"\\${ai.koog.openai.api-key:}\") private val openAIKey: String\n) {\n\n    fun isOpenAIConfigured(): Boolean = openAIKey.isNotBlank() &amp;&amp; openAIExecutor != null\n\n    suspend fun processIfConfigured(input: String): String? {\n        return if (isOpenAIConfigured()) {\n            val result = openAIExecutor!!.execute(prompt { user(input) })\n            result.text\n        } else {\n            null\n        }\n    }\n}\n</code></pre>"},{"location":"spring-boot/#configuration-reference","title":"Configuration Reference","text":""},{"location":"spring-boot/#available-properties","title":"Available Properties","text":"Property Description Bean Condition Default <code>ai.koog.openai.api-key</code> OpenAI API key Required for <code>openAIExecutor</code> bean - <code>ai.koog.openai.base-url</code> OpenAI base URL Optional <code>https://api.openai.com</code> <code>ai.koog.anthropic.api-key</code> Anthropic API key Required for <code>anthropicExecutor</code> bean - <code>ai.koog.anthropic.base-url</code> Anthropic base URL Optional <code>https://api.anthropic.com</code> <code>ai.koog.google.api-key</code> Google API key Required for <code>googleExecutor</code> bean - <code>ai.koog.google.base-url</code> Google base URL Optional <code>https://generativelanguage.googleapis.com</code> <code>ai.koog.openrouter.api-key</code> OpenRouter API key Required for <code>openRouterExecutor</code> bean - <code>ai.koog.openrouter.base-url</code> OpenRouter base URL Optional <code>https://openrouter.ai</code> <code>ai.koog.deepseek.api-key</code> DeepSeek API key Required for <code>deepSeekExecutor</code> bean - <code>ai.koog.deepseek.base-url</code> DeepSeek base URL Optional <code>https://api.deepseek.com</code> <code>ai.koog.ollama.base-url</code> Ollama base URL Any <code>ai.koog.ollama.*</code> property activates <code>ollamaExecutor</code> bean <code>http://localhost:11434</code>"},{"location":"spring-boot/#bean-names","title":"Bean Names","text":"<p>The auto-configuration creates the following beans (when configured):</p> <ul> <li><code>openAIExecutor</code> - OpenAI executor (requires <code>ai.koog.openai.api-key</code>)</li> <li><code>anthropicExecutor</code> - Anthropic executor (requires <code>ai.koog.anthropic.api-key</code>)</li> <li><code>googleExecutor</code> - Google executor (requires <code>ai.koog.google.api-key</code>)</li> <li><code>openRouterExecutor</code> - OpenRouter executor (requires <code>ai.koog.openrouter.api-key</code>)</li> <li><code>deepSeekExecutor</code> - DeepSeek executor (requires <code>ai.koog.deepseek.api-key</code>)</li> <li><code>ollamaExecutor</code> - Ollama executor (requires any <code>ai.koog.ollama.*</code> property)</li> </ul>"},{"location":"spring-boot/#troubleshooting","title":"Troubleshooting","text":""},{"location":"spring-boot/#common-issues","title":"Common Issues","text":"<p>Bean not found error:</p> <pre><code>No qualifying bean of type 'SingleLLMPromptExecutor' available\n</code></pre> <p>Solution: Ensure you have configured at least one provider in your properties file.</p> <p>Multiple beans error:</p> <pre><code>Multiple qualifying beans of type 'SingleLLMPromptExecutor' available\n</code></pre> <p>Solution: Use <code>@Qualifier</code> to specify which bean you want:</p> <pre><code>@Service\nclass MyService(\n    @Qualifier(\"openAIExecutor\") private val openAIExecutor: SingleLLMPromptExecutor,\n    @Qualifier(\"anthropicExecutor\") private val anthropicExecutor: SingleLLMPromptExecutor\n) {\n    // ...\n}\n</code></pre> <p>API key not loaded:</p> <pre><code>API key is required but not provided\n</code></pre> <p>Solution: Check that your environment variables are properly set and accessible to your Spring Boot application.</p>"},{"location":"spring-boot/#best-practices","title":"Best Practices","text":"<ol> <li>Environment Variables: Always use environment variables for API keys</li> <li>Nullable Injection: Use nullable types (<code>SingleLLMPromptExecutor?</code>) to handle cases where providers aren't    configured</li> <li>Fallback Logic: Implement fallback mechanisms when using multiple providers</li> <li>Error Handling: Always wrap executor calls in try-catch blocks for production code</li> <li>Testing: Use mocks in tests to avoid making actual API calls</li> <li>Configuration Validation: Check if executors are available before using them</li> </ol>"},{"location":"spring-boot/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Single Run Agents to build basic AI workflows</li> <li>Explore Complex Workflow Agents for advanced use cases</li> <li>See Tools Overview to extend your agents' capabilities</li> <li>Check out Examples for real-world implementations</li> <li>Read Key Concepts to understand the framework better</li> </ul>"},{"location":"streaming-api/","title":"Streaming API","text":""},{"location":"streaming-api/#introduction","title":"Introduction","text":"<p>The Streaming API in the Koog framework lets you process structured data from Large Language Models  (LLMs) as it arrives, rather than waiting for the entire response. This page explains how to use the Streaming API to efficiently handle structured data in Markdown format.</p>"},{"location":"streaming-api/#streaming-api-overview","title":"Streaming API overview","text":"<p>The Streaming API enables real-time processing of structured data from LLM responses. Instead of waiting for the complete response, you can:</p> <ul> <li>Process data as it arrives in chunks</li> <li>Parse structured information on the fly</li> <li>Emit structured objects as they are completed</li> <li>Handle these objects immediately (collect them or pass to tools)</li> </ul> <p>This approach is particularly useful as it provides the following benefits:</p> <ul> <li>Improving responsiveness in user interfaces</li> <li>Processing large responses efficiently</li> <li>Implementing real-time data processing pipelines</li> </ul> <p>The Streaming API allows parsing the output as structured data from the .md format or as a set of plain text chunks.</p>"},{"location":"streaming-api/#working-with-a-raw-string-stream","title":"Working with a raw string stream","text":"<p>It is important to note that you can parse the output by working directly with a raw string stream. This approach gives you more flexibility and control over the parsing process.</p> <p>Here is a raw string stream with the Markdown definition of the output structure:</p> <pre><code>fun markdownBookDefinition(): MarkdownStructuredDataDefinition {\n    return MarkdownStructuredDataDefinition(\"name\", schema = { /*...*/ })\n}\n\nval mdDefinition = markdownBookDefinition()\n\nllm.writeSession {\n    val stream = requestLLMStreaming(mdDefinition)\n    // Access the raw string chunks directly\n    stream.collect { chunk -&gt;\n        // Process each chunk of text as it arrives\n        println(\"Received chunk: $chunk\") // The chunks together will be structured as a text following the mdDefinition schema\n    }\n}\n</code></pre> <p>This is an example of a raw string stream without the definition:</p> <pre><code>llm.writeSession {\n    val stream = requestLLMStreaming()\n    // Access the raw string chunks directly\n    stream.collect { chunk -&gt;\n        // Process each chunk of text as it arrives\n        println(\"Received chunk: $chunk\") // The chunks will not be structured in a specific way\n    }\n}\n</code></pre>"},{"location":"streaming-api/#working-with-a-stream-of-structured-data","title":"Working with a stream of structured data","text":"<p>Although it is possible to work with a raw string stream, it is often more convenient to work with structured data.</p> <p>The structured data approach includes the following key components:</p> <ol> <li>MarkdownStructuredDataDefinition: a class to help you define the schema and examples for structured data in    Markdown format.</li> <li>markdownStreamingParser: a function to create a parser that processes a stream of Markdown chunks and emits    events.</li> </ol> <p>The sections below provide step-by-step instructions and code samples related to processing a stream of structured data. </p>"},{"location":"streaming-api/#1-define-your-data-structure","title":"1. Define your data structure","text":"<p>First, define a data class to represent your structured data:</p> <pre><code>@Serializable\ndata class Book(\n    val title: String,\n    val author: String,\n    val description: String\n)\n</code></pre>"},{"location":"streaming-api/#2-define-the-markdown-structure","title":"2. Define the Markdown structure","text":"<p>Create a definition that specifies how your data should be structured in Markdown with the <code>MarkdownStructuredDataDefinition</code> class:</p> <pre><code>fun markdownBookDefinition(): MarkdownStructuredDataDefinition {\n    return MarkdownStructuredDataDefinition(\"bookList\", schema = {\n        markdown {\n            header(1, \"title\")\n            bulleted {\n                item(\"author\")\n                item(\"description\")\n            }\n        }\n    }, examples = {\n        markdown {\n            header(1, \"The Great Gatsby\")\n            bulleted {\n                item(\"F. Scott Fitzgerald\")\n                item(\"A novel set in the Jazz Age that tells the story of Jay Gatsby's unrequited love for Daisy Buchanan.\")\n            }\n        }\n    })\n}\n</code></pre>"},{"location":"streaming-api/#3-create-a-parser-for-your-data-structure","title":"3. Create a parser for your data structure","text":"<p>The <code>markdownStreamingParser</code> provides several handlers for different Markdown elements:</p> <pre><code>markdownStreamingParser {\n    // Handle level 1 headings\n    // The heading level can be from 1 to 6\n    onHeader(1) { headerText -&gt;\n        // Process heading text\n    }\n\n    // Handle bullet points\n    onBullet { bulletText -&gt;\n        // Process bullet text\n    }\n\n    // Handle code blocks\n    onCodeBlock { codeBlockContent -&gt;\n        // Process code block content\n    }\n\n    // Handle lines matching a regex pattern\n    onLineMatching(Regex(\"pattern\")) { line -&gt;\n        // Process matching lines\n    }\n\n    // Handle the end of the stream\n    onFinishStream { remainingText -&gt;\n        // Process any remaining text or perform cleanup\n    }\n}\n</code></pre> <p>Using the defined handlers, you can implement a function that parses the Markdown stream and emits your data objects  with the <code>markdownStreamingParser</code> function.</p> <pre><code>fun parseMarkdownStreamToBooks(markdownStream: Flow&lt;String&gt;): Flow&lt;Book&gt; {\n   return flow {\n      markdownStreamingParser {\n         var currentBookTitle = \"\"\n         val bulletPoints = mutableListOf&lt;String&gt;()\n\n         // Handle the event of receiving the Markdown header in the response stream\n         onHeader(1) { headerText -&gt;\n            // If there was a previous book, emit it\n            if (currentBookTitle.isNotEmpty() &amp;&amp; bulletPoints.isNotEmpty()) {\n               val author = bulletPoints.getOrNull(0) ?: \"\"\n               val description = bulletPoints.getOrNull(1) ?: \"\"\n               emit(Book(currentBookTitle, author, description))\n            }\n\n            currentBookTitle = headerText\n            bulletPoints.clear()\n         }\n\n         // Handle the event of receiving the Markdown bullets list in the response stream\n         onBullet { bulletText -&gt;\n            bulletPoints.add(bulletText)\n         }\n\n         // Handle the end of the response stream\n         onFinishStream {\n            // Emit the last book, if present\n            if (currentBookTitle.isNotEmpty() &amp;&amp; bulletPoints.isNotEmpty()) {\n               val author = bulletPoints.getOrNull(0) ?: \"\"\n               val description = bulletPoints.getOrNull(1) ?: \"\"\n               emit(Book(currentBookTitle, author, description))\n            }\n         }\n      }.parseStream(markdownStream)\n   }\n}\n</code></pre>"},{"location":"streaming-api/#4-use-the-parser-in-your-agent-strategy","title":"4. Use the parser in your agent strategy","text":"<pre><code>val agentStrategy = strategy&lt;String, List&lt;Book&gt;&gt;(\"library-assistant\") {\n   // Describe the node containing the output stream parsing\n   val getMdOutput by node&lt;String, List&lt;Book&gt;&gt; { booksDescription -&gt;\n      val books = mutableListOf&lt;Book&gt;()\n      val mdDefinition = markdownBookDefinition()\n\n      llm.writeSession {\n         updatePrompt { user(booksDescription) }\n         // Initiate the response stream in the form of the definition `mdDefinition`\n         val markdownStream = requestLLMStreaming(mdDefinition)\n         // Call the parser with the result of the response stream and perform actions with the result\n         parseMarkdownStreamToBooks(markdownStream).collect { book -&gt;\n            books.add(book)\n            println(\"Parsed Book: ${book.title} by ${book.author}\")\n         }\n      }\n\n      books\n   }\n   // Describe the agent's graph making sure the node is accessible\n   edge(nodeStart forwardTo getMdOutput)\n   edge(getMdOutput forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"streaming-api/#advanced-usage-streaming-with-tools","title":"Advanced usage: Streaming with tools","text":"<p>You can also use the Streaming API with tools to process data as it arrives. The following sections provide a brief step-by-step guide on how to define a tool and use it with streaming data.</p>"},{"location":"streaming-api/#1-define-a-tool-for-your-data-structure","title":"1. Define a tool for your data structure","text":"<pre><code>@Serializable\ndata class Book(\n   val title: String,\n   val author: String,\n   val description: String\n) : ToolArgs\n\nclass BookTool(): SimpleTool&lt;Book&gt;() {\n   companion object {\n      const val NAME = \"book\"\n   }\n\n   override suspend fun doExecute(args: Book): String {\n      println(\"${args.title} by ${args.author}:\\n ${args.description}\")\n      return \"Done\"\n   }\n\n   override val argsSerializer: KSerializer&lt;Book&gt;\n      get() = Book.serializer()\n   override val descriptor: ToolDescriptor\n      get() = ToolDescriptor(\n         name = NAME,\n         description = \"A tool to parse book information from Markdown\",\n         requiredParameters = listOf(),\n         optionalParameters = listOf()\n      )\n}\n</code></pre>"},{"location":"streaming-api/#2-use-the-tool-with-streaming-data","title":"2. Use the tool with streaming data","text":"<pre><code>val agentStrategy = strategy&lt;String, Unit&gt;(\"library-assistant\") {\n   val getMdOutput by node&lt;String, Unit&gt; { input -&gt;\n      val mdDefinition = markdownBookDefinition()\n\n      llm.writeSession {\n         updatePrompt { user(input) }\n         val markdownStream = requestLLMStreaming(mdDefinition)\n\n         parseMarkdownStreamToBooks(markdownStream).collect { book -&gt;\n            callToolRaw(BookTool.NAME, book as ToolArgs)\n            /* Other possible options:\n                callTool(BookTool::class, book)\n                callTool&lt;BookTool&gt;(book)\n                findTool(BookTool::class).execute(book)\n            */\n         }\n\n         // We can make parallel tool calls\n         parseMarkdownStreamToBooks(markdownStream).toParallelToolCallsRaw(toolClass=BookTool::class).collect {\n            println(\"Tool call result: $it\")\n         }\n      }\n   }\n\n   edge(nodeStart forwardTo getMdOutput)\n   edge(getMdOutput forwardTo nodeFinish)\n }\n</code></pre>"},{"location":"streaming-api/#3-register-the-tool-in-your-agent-configuration","title":"3. Register the tool in your agent configuration","text":"<pre><code>val toolRegistry = ToolRegistry {\n   tool(BookTool())\n}\n\nval runner = AIAgent(\n   promptExecutor = simpleOpenAIExecutor(token),\n   toolRegistry = toolRegistry,\n   strategy = agentStrategy,\n   agentConfig = agentConfig\n)\n</code></pre>"},{"location":"streaming-api/#best-practices","title":"Best practices","text":"<ol> <li> <p>Define clear structures: create clear and unambiguous markdown structures for your data.</p> </li> <li> <p>Provide good examples: include comprehensive examples in your <code>MarkdownStructuredDataDefinition</code> to guide the LLM.</p> </li> <li> <p>Handle incomplete data: always check for null or empty values when parsing data from the stream.</p> </li> <li> <p>Clean up resources: use the <code>onFinishStream</code> handler to clean up resources and process any remaining data.</p> </li> <li> <p>Handle errors: implement proper error handling for malformed Markdown or unexpected data.</p> </li> <li> <p>Testing: test your parser with various input scenarios, including partial chunks and malformed input.</p> </li> <li> <p>Parallel processing: for independent data items, consider using parallel tool calls for better performance.</p> </li> </ol>"},{"location":"structured-data/","title":"Structured data processing","text":""},{"location":"structured-data/#introduction","title":"Introduction","text":"<p>The Structured Data Processing API provides a way to ensure that responses from Large Language Models (LLMs)  conform to specific data structures. This is crucial for building reliable AI applications where you need predictable, well-formatted data rather than free-form text.</p> <p>This page explains how to use the Structured Data Processing API to define data structures, generate schemas, and  request structured responses from LLMs.</p>"},{"location":"structured-data/#key-components-and-concepts","title":"Key components and concepts","text":"<p>The Structured Data Processing API consists of several key components:</p> <ol> <li>Data structure definition: Kotlin data classes annotated with kotlinx.serialization and LLM-specific annotations.</li> <li>JSON Schema generation: tools to generate JSON schemas from Kotlin data classes.</li> <li>Structured LLM requests: methods to request responses from LLMs that conform to the defined structures.</li> <li>Response handling: processing and validating the structured responses.</li> </ol>"},{"location":"structured-data/#defining-data-structures","title":"Defining data structures","text":"<p>The first step in using the Structured Data Processing API is to define your data structures using Kotlin data classes.</p>"},{"location":"structured-data/#basic-structure","title":"Basic structure","text":"<pre><code>@Serializable\n@SerialName(\"WeatherForecast\")\n@LLMDescription(\"Weather forecast for a given location\")\ndata class WeatherForecast(\n    @property:LLMDescription(\"Temperature in Celsius\")\n    val temperature: Int,\n    @property:LLMDescription(\"Weather conditions (e.g., sunny, cloudy, rainy)\")\n    val conditions: String,\n    @property:LLMDescription(\"Chance of precipitation in percentage\")\n    val precipitation: Int\n)\n</code></pre>"},{"location":"structured-data/#key-annotations","title":"Key annotations","text":"<ul> <li><code>@Serializable</code>: required for kotlinx.serialization to work with the class.</li> <li><code>@SerialName</code>: specifies the name to use during serialization.</li> <li><code>@LLMDescription</code>: provides a description of the class for the LLM. For field annotations, use <code>@property:LLMDescription</code>.</li> </ul>"},{"location":"structured-data/#supported-features","title":"Supported features","text":"<p>The API supports a wide range of data structure features:</p>"},{"location":"structured-data/#nested-classes","title":"Nested classes","text":"<pre><code>@Serializable\n@SerialName(\"WeatherForecast\")\ndata class WeatherForecast(\n    // Other fields\n    @property:LLMDescription(\"Coordinates of the location\")\n    val latLon: LatLon\n) {\n    @Serializable\n    @SerialName(\"LatLon\")\n    data class LatLon(\n        @property:LLMDescription(\"Latitude of the location\")\n        val lat: Double,\n        @property:LLMDescription(\"Longitude of the location\")\n        val lon: Double\n    )\n}\n</code></pre>"},{"location":"structured-data/#collections-lists-and-maps","title":"Collections (lists and maps)","text":"<pre><code>@Serializable\n@SerialName(\"WeatherForecast\")\ndata class WeatherForecast(\n    // Other fields\n    @property:LLMDescription(\"List of news articles\")\n    val news: List&lt;WeatherNews&gt;,\n    @property:LLMDescription(\"Map of weather sources\")\n    val sources: Map&lt;String, WeatherSource&gt;\n)\n</code></pre>"},{"location":"structured-data/#enums","title":"Enums","text":"<pre><code>@Serializable\n@SerialName(\"Pollution\")\nenum class Pollution { Low, Medium, High }\n</code></pre>"},{"location":"structured-data/#polymorphism-with-sealed-classes","title":"Polymorphism with sealed classes","text":"<pre><code>@Serializable\n@SerialName(\"WeatherAlert\")\nsealed class WeatherAlert {\n    abstract val severity: Severity\n    abstract val message: String\n\n    @Serializable\n    @SerialName(\"Severity\")\n    enum class Severity { Low, Moderate, Severe, Extreme }\n\n    @Serializable\n    @SerialName(\"StormAlert\")\n    data class StormAlert(\n        override val severity: Severity,\n        override val message: String,\n        @property:LLMDescription(\"Wind speed in km/h\")\n        val windSpeed: Double\n    ) : WeatherAlert()\n\n    @Serializable\n    @SerialName(\"FloodAlert\")\n    data class FloodAlert(\n        override val severity: Severity,\n        override val message: String,\n        @property:LLMDescription(\"Expected rainfall in mm\")\n        val expectedRainfall: Double\n    ) : WeatherAlert()\n}\n</code></pre>"},{"location":"structured-data/#generating-json-schemas","title":"Generating JSON schemas","text":"<p>Once you have defined your data structures, you can generate JSON schemas from them using the <code>JsonStructuredData</code> class:</p> <pre><code>val weatherForecastStructure = JsonStructuredData.createJsonStructure&lt;WeatherForecast&gt;(\n    schemaGenerator = BasicJsonSchemaGenerator.Default,\n    examples = exampleForecasts\n)\n</code></pre>"},{"location":"structured-data/#schema-format-options","title":"Schema format options","text":"<ul> <li><code>JsonSchema</code>: standard JSON Schema format.</li> <li><code>Simple</code>: a simplified schema format that may work better with some models but has limitations such as no  polymorphism support.</li> </ul>"},{"location":"structured-data/#schema-type-options","title":"Schema type options","text":"<p>The following schema types are supported</p> <ul> <li> <p><code>SIMPLE</code>: a simplified schema type:</p> <ul> <li>Supports only standard JSON fields</li> <li>Does not support definitions, URL references, and recursive checks</li> <li>Does not support polymorphism</li> <li>Supported by a larger number of language models</li> <li>Used for simpler data structures</li> </ul> </li> <li> <p><code>FULL</code>: a more comprehensive schema type:</p> <ul> <li>Supports advanced JSON Schema capabilities, including definitions, URL references, and recursive checks</li> <li>Supports polymorphism: can work with sealed classes or interfaces and their implementations</li> <li>Supported by fewer language models</li> <li>Used for complex data structures with inheritance hierarchies</li> </ul> </li> </ul>"},{"location":"structured-data/#providing-examples","title":"Providing examples","text":"<p>You can provide examples to help the LLM understand the expected format:</p> <pre><code>val exampleForecasts = listOf(\n  WeatherForecast(\n    news = listOf(WeatherNews(0.0), WeatherNews(5.0)),\n    sources = mutableMapOf(\n      \"openweathermap\" to WeatherSource(Url(\"https://api.openweathermap.org/data/2.5/weather\")),\n      \"googleweather\" to WeatherSource(Url(\"https://weather.google.com\"))\n    )\n    // Other fields\n  ),\n  WeatherForecast(\n    news = listOf(WeatherNews(25.0), WeatherNews(35.0)),\n    sources = mutableMapOf(\n      \"openweathermap\" to WeatherSource(Url(\"https://api.openweathermap.org/data/2.5/weather\")),\n      \"googleweather\" to WeatherSource(Url(\"https://weather.google.com\"))\n    )\n  )\n)\n</code></pre>"},{"location":"structured-data/#requesting-structured-responses","title":"Requesting structured responses","text":"<p>There are two ways to request structured responses in Koog:</p> <ul> <li>Make a single LLM call using a prompt executor and its <code>executeStructured</code> or <code>executeStructuredOneShot</code> methods.</li> <li>Create structured output requests for agent use cases and integration into agent strategies. </li> </ul>"},{"location":"structured-data/#using-a-prompt-executor","title":"Using a prompt executor","text":"<p>To make a single LLM call that returns a structured output, use a prompt executor and its <code>executeStructured</code> method. This method executes a prompt and ensures the response is properly structured by applying automatic output coercion. The  method enhances structured output parsing reliability by:</p> <ul> <li>Injecting structured output instructions into the original prompt.</li> <li>Executing the enriched prompt to receive a raw response.</li> <li>Using a separate LLM call to parse or coerce the response if direct parsing fails.</li> </ul> <p>Unlike <code>[execute(prompt, structure)]</code> which simply attempts to parse the raw response and fails if the format does not match exactly, this method actively works to transform unstructured or malformed outputs into the expected structure through additional LLM processing.</p> <p>Here is an example of using the <code>executeStructured</code> method:</p> <pre><code>// Define a simple, single-provider prompt executor\nval promptExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_KEY\"))\n\n// Make an LLM call that returns a structured response\nval structuredResponse = promptExecutor.executeStructured(\n        // Define the prompt (both system and user messages)\n        prompt = prompt(\"structured-data\") {\n            system(\n                \"\"\"\n                You are a weather forecasting assistant.\n                When asked for a weather forecast, provide a realistic but fictional forecast.\n                \"\"\".trimIndent()\n            )\n            user(\n              \"What is the weather forecast for Amsterdam?\"\n            )\n        },\n        // Define the main model that will execute the request\n        model = OpenAIModels.CostOptimized.GPT4oMini,\n        // Provide the structured data configuration\n        config = StructuredOutputConfig(\n            default = StructuredOutput.Manual(weatherForecastStructure),\n            fixingParser = StructureFixingParser(\n                fixingModel = OpenAIModels.Chat.GPT4o,\n                retries = 3\n            )\n        )\n    )\n</code></pre> <p>The example relies on an already generated JSON schema named <code>weatherForecastStructure</code> that is based on a defined data structure and examples.</p> <p>The <code>executeStructured</code> method takes the following arguments:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt to execute. For more information, see Prompt API. <code>structure</code> StructuredData Yes The structured data definition with schema and parsing logic. For more information, see Defining data structures. <code>mainModel</code> LLModel Yes The main model to execute the prompt. <code>retries</code> Integer No <code>1</code> The number of attempts to parse the response into a proper structured output. <code>fixingModel</code> LLModel No <code>OpenAIModels.Chat.GPT4o</code> The model that handles output coercion - transformation of malformed outputs into the expected structure. <p>In addition to <code>executeStructured</code>, you can also use the <code>executeStructuredOneShot</code> method with a prompt executor. The  main difference is that <code>executeStructuredOneShot</code> does not handle coercion automatically, so you would have to manually transform malformed outputs into proper structured ones.</p> <p>The <code>executeStructuredOneShot</code> method takes the following arguments:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt to execute. <code>structure</code> StructuredData Yes The structured data definition with schema and parsing logic. <code>model</code> LLModel Yes The model to execute the prompt."},{"location":"structured-data/#structured-data-responses-for-agent-use-cases","title":"Structured data responses for agent use cases","text":"<p>To request a structured response from an LLM, use the <code>requestLLMStructured</code> method within a <code>writeSession</code>:</p> <pre><code>val structuredResponse = llm.writeSession {\n    this.requestLLMStructured(\n        config = StructuredOutputConfig(\n            default = StructuredOutput.Manual(weatherForecastStructure),\n            fixingParser = StructureFixingParser(\n                fixingModel = OpenAIModels.Chat.GPT4o,\n                retries = 3\n            )\n        )\n    )\n}\n</code></pre> <p>The <code>fixingModel</code> parameter specifies the language model to use for reparsing or error correction during retries. This helps ensure that you always get a valid response.</p>"},{"location":"structured-data/#integrating-with-agent-strategies","title":"Integrating with agent strategies","text":"<p>You can integrate structured data processing into your agent strategies:</p> <pre><code>val agentStrategy = strategy(\"weather-forecast\") {\n    val setup by nodeLLMRequest()\n\n    val getStructuredForecast by node&lt;Message.Response, String&gt; { _ -&gt;\n        val structuredResponse = llm.writeSession {\n            this.requestLLMStructured(\n                config = StructuredOutputConfig(\n                    default = StructuredOutput.Manual(weatherForecastStructure),\n                    fixingParser = StructureFixingParser(\n                        fixingModel = OpenAIModels.Chat.GPT4o,\n                        retries = 3\n                    )\n                )\n            )\n        }\n\n        \"\"\"\n        Response structure:\n        $structuredResponse\n        \"\"\".trimIndent()\n    }\n\n    edge(nodeStart forwardTo setup)\n    edge(setup forwardTo getStructuredForecast)\n    edge(getStructuredForecast forwardTo nodeFinish)\n}\n</code></pre>"},{"location":"structured-data/#full-code-sample","title":"Full code sample","text":"<p>Here is a full example of using the Structured Data Processing API:</p> <pre><code>// Note: Import statements are omitted for brevity\n@Serializable\n@SerialName(\"SimpleWeatherForecast\")\n@LLMDescription(\"Simple weather forecast for a location\")\ndata class SimpleWeatherForecast(\n    @property:LLMDescription(\"Location name\")\n    val location: String,\n    @property:LLMDescription(\"Temperature in Celsius\")\n    val temperature: Int,\n    @property:LLMDescription(\"Weather conditions (e.g., sunny, cloudy, rainy)\")\n    val conditions: String\n)\n\nval token = System.getenv(\"OPENAI_KEY\") ?: error(\"Environment variable OPENAI_KEY is not set\")\n\nfun main(): Unit = runBlocking {\n    // Create sample forecasts\n    val exampleForecasts = listOf(\n        SimpleWeatherForecast(\n            location = \"New York\",\n            temperature = 25,\n            conditions = \"Sunny\"\n        ),\n        SimpleWeatherForecast(\n            location = \"London\",\n            temperature = 18,\n            conditions = \"Cloudy\"\n        )\n    )\n\n    // Generate JSON Schema\n    val forecastStructure = JsonStructuredData.createJsonStructure&lt;SimpleWeatherForecast&gt;(\n        schemaGenerator = BasicJsonSchemaGenerator.Default,\n        examples = exampleForecasts\n    )\n\n    // Define the agent strategy\n    val agentStrategy = strategy(\"weather-forecast\") {\n        val setup by nodeLLMRequest()\n\n        val getStructuredForecast by node&lt;Message.Response, String&gt; { _ -&gt;\n            val structuredResponse = llm.writeSession {\n                this.requestLLMStructured&lt;SimpleWeatherForecast&gt;()\n            }\n\n            \"\"\"\n            Response structure:\n            $structuredResponse\n            \"\"\".trimIndent()\n        }\n\n        edge(nodeStart forwardTo setup)\n        edge(setup forwardTo getStructuredForecast)\n        edge(getStructuredForecast forwardTo nodeFinish)\n    }\n\n\n    // Configure and run the agent\n    val agentConfig = AIAgentConfig(\n        prompt = prompt(\"weather-forecast-prompt\") {\n            system(\n                \"\"\"\n                You are a weather forecasting assistant.\n                When asked for a weather forecast, provide a realistic but fictional forecast.\n                \"\"\".trimIndent()\n            )\n        },\n        model = OpenAIModels.Chat.GPT4o,\n        maxAgentIterations = 5\n    )\n\n    val runner = AIAgent(\n        promptExecutor = simpleOpenAIExecutor(token),\n        toolRegistry = ToolRegistry.EMPTY,\n        strategy = agentStrategy,\n        agentConfig = agentConfig\n    )\n\n    runner.run(\"Get weather forecast for Paris\")\n}\n</code></pre>"},{"location":"structured-data/#best-practices","title":"Best practices","text":"<ol> <li> <p>Use clear descriptions: provide clear and detailed descriptions using <code>@LLMDescription</code> annotations to help the LLM understand the expected data.</p> </li> <li> <p>Provide examples: include examples of valid data structures to guide the LLM.</p> </li> <li> <p>Handle errors gracefully: implement proper error handling to deal with cases where the LLM might not produce a valid structure.</p> </li> <li> <p>Use appropriate schema types: select the appropriate schema format and type based on your needs and the capabilities of the LLM you are using.</p> </li> <li> <p>Test with different models: different LLMs may have varying abilities to follow structured formats, so test with multiple models if possible.</p> </li> <li> <p>Start simple: begin with simple structures and gradually add complexity as needed.</p> </li> <li> <p>Use polymorphism Carefully: while the API supports polymorphism with sealed classes, be aware that it can be more challenging for LLMs to handle correctly.</p> </li> </ol>"},{"location":"subgraphs-overview/","title":"Overview","text":"<p>This page provides detailed information about subgraphs in the Koog framework. Understanding these concepts is crucial for creating complex agent workflows that maintain context across multiple processing steps.</p>"},{"location":"subgraphs-overview/#introduction","title":"Introduction","text":"<p>Subgraphs are a fundamental concept in the Koog framework that lets you break down complex agent workflows into manageable, sequential steps. Each subgraph represents a phase of processing, with its context, responsibilities, and an optional subset of tools.</p> <p>Subgraphs are integral parts of strategies, which are graphs that represent the overall agent workflow. For more information about strategies, see Custom strategy graphs.</p>"},{"location":"subgraphs-overview/#understanding-subgraphs","title":"Understanding subgraphs","text":"<p>A subgraph is a self-contained unit of processing within an agent strategy. Each subgraph:</p> <ul> <li>Has a unique name</li> <li>Contains a graph of nodes or subgraphs connected by edges</li> <li>Can use any tool or a subset of tools from the tool registry</li> <li>Receives input from the previous subgraph (or the initial user input)</li> <li>Produces output that is passed to the next subgraph (or the output)</li> </ul> <p>To define a sequence of subgraphs in a graph, use edge connections or define sequences using the <code>then</code> keyword. For more information, see Custom strategy graphs.</p>"},{"location":"subgraphs-overview/#subgraph-context","title":"Subgraph context","text":"<p>Each subgraph executes within a context that provides access to:</p> <ul> <li>The environment</li> <li>Agent input</li> <li>The agent configuration</li> <li>The LLM context (including the conversation history)</li> <li>The state manager</li> <li>The storage</li> <li>Session and strategy</li> </ul> <p>The context is passed to each node within the subgraph and provides the necessary resources for the node to perform its operations.</p>"},{"location":"testing/","title":"Testing","text":""},{"location":"testing/#overview","title":"Overview","text":"<p>The Testing feature provides a comprehensive framework for testing AI agent pipelines, subgraphs, and tool interactions in the Koog framework. It enables developers to create controlled test environments with mock LLM (Large Language Model) executors, tool registries, and agent environments.</p>"},{"location":"testing/#purpose","title":"Purpose","text":"<p>The primary purpose of this feature is to facilitate testing of agent-based AI features by:</p> <ul> <li>Mocking LLM responses to specific prompts</li> <li>Simulating tool calls and their results</li> <li>Testing agent pipeline subgraphs and their structures</li> <li>Verifying the correct flow of data through agent nodes</li> <li>Providing assertions for expected behaviors</li> </ul>"},{"location":"testing/#configuration-and-initialization","title":"Configuration and initialization","text":""},{"location":"testing/#setting-up-test-dependencies","title":"Setting up test dependencies","text":"<p>Before setting up a test environment, make sure that you have added the following dependencies:</p> <pre><code>// build.gradle.kts\ndependencies {\n   testImplementation(\"ai.koog:agents-test:LATEST_VERSION\")\n   testImplementation(kotlin(\"test\"))\n}\n</code></pre>"},{"location":"testing/#mocking-llm-responses","title":"Mocking LLM responses","text":"<p>The basic form of testing involves mocking LLM responses to ensure deterministic behavior. You can do this using  <code>MockLLMBuilder</code> and related utilities.</p> <pre><code>// Create a mock LLM executor\nval mockLLMApi = getMockExecutor(toolRegistry) {\n  // Mock a simple text response\n  mockLLMAnswer(\"Hello!\") onRequestContains \"Hello\"\n\n  // Mock a default response\n  mockLLMAnswer(\"I don't know how to answer that.\").asDefaultResponse\n}\n</code></pre>"},{"location":"testing/#mocking-tool-calls","title":"Mocking tool calls","text":"<p>You can mock the LLM to call specific tools based on input patterns:</p> <pre><code>// Mock a tool call response\nmockLLMToolCall(CreateTool, CreateTool.Args(\"solve\")) onRequestEquals \"Solve task\"\n\n// Mock tool behavior - simplest form without lambda\nmockTool(PositiveToneTool) alwaysReturns \"The text has a positive tone.\"\n\n// Using lambda when you need to perform extra actions\nmockTool(NegativeToneTool) alwaysTells {\n  // Perform some extra action\n  println(\"Negative tone tool called\")\n\n  // Return the result\n  \"The text has a negative tone.\"\n}\n\n// Mock tool behavior based on specific arguments\nmockTool(AnalyzeTool) returns AnalyzeTool.Result(\"Detailed analysis\") onArguments AnalyzeTool.Args(\"analyze deeply\")\n\n// Mock tool behavior with conditional argument matching\nmockTool(SearchTool) returns SearchTool.Result(\"Found results\") onArgumentsMatching { args -&gt;\n  args.query.contains(\"important\")\n}\n</code></pre> <p>The examples above demonstrate different ways to mock tools, from simple to more complex ones:</p> <ol> <li><code>alwaysReturns</code>: the simplest form, directly returns a value without a lambda.</li> <li><code>alwaysTells</code>: uses a lambda when you need to perform additional actions.</li> <li><code>returns...onArguments</code>: returns specific results for exact argument matches.</li> <li><code>returns...onArgumentsMatching</code>: returns results based on custom argument conditions.</li> </ol>"},{"location":"testing/#enabling-testing-mode","title":"Enabling testing mode","text":"<p>To enable the testing mode on an agent, use the <code>withTesting()</code> function within the AIAgent constructor block:</p> <pre><code>// Create the agent with testing enabled\nAIAgent(\n    executor = mockLLMApi,\n    toolRegistry = toolRegistry,\n    llmModel = llmModel\n) {\n    // Enable testing mode\n    withTesting()\n}\n</code></pre>"},{"location":"testing/#advanced-testing","title":"Advanced testing","text":""},{"location":"testing/#testing-the-graph-structure","title":"Testing the graph structure","text":"<p>Before testing the detailed node behavior and edge connections, it is important to verify the overall structure of your agent's graph. This includes checking that all required nodes exist and are properly connected in the expected subgraphs.</p> <p>The Testing feature provides a comprehensive way to test your agent's graph structure. This approach is particularly valuable for complex agents with multiple subgraphs and interconnected nodes.</p>"},{"location":"testing/#basic-structure-testing","title":"Basic structure testing","text":"<p>Start by validating the fundamental structure of your agent's graph:</p> <pre><code>AIAgent(\n    // Constructor arguments\n    executor = mockLLMApi,\n    toolRegistry = toolRegistry,\n    llmModel = llmModel\n) {\n    testGraph&lt;String, String&gt;(\"test\") {\n        val firstSubgraph = assertSubgraphByName&lt;String, String&gt;(\"first\")\n        val secondSubgraph = assertSubgraphByName&lt;String, String&gt;(\"second\")\n\n        // Assert subgraph connections\n        assertEdges {\n            startNode() alwaysGoesTo firstSubgraph\n            firstSubgraph alwaysGoesTo secondSubgraph\n            secondSubgraph alwaysGoesTo finishNode()\n        }\n\n        // Verify the first subgraph\n        verifySubgraph(firstSubgraph) {\n            val start = startNode()\n            val finish = finishNode()\n\n            // Assert nodes by name\n            val askLLM = assertNodeByName&lt;String, Message.Response&gt;(\"callLLM\")\n            val callTool = assertNodeByName&lt;ToolArgs, ToolResult&gt;(\"executeTool\")\n\n            // Assert node reachability\n            assertReachable(start, askLLM)\n            assertReachable(askLLM, callTool)\n        }\n    }\n}\n</code></pre>"},{"location":"testing/#testing-node-behavior","title":"Testing node behavior","text":"<p>Node behavior testing lets you verify that nodes in your agent's graph produce the expected outputs for the given inputs.  This is crucial for ensuring that your agent's logic works correctly under different scenarios.</p>"},{"location":"testing/#basic-node-testing","title":"Basic node testing","text":"<p>Start with simple input and output validations for individual nodes:</p> <pre><code>assertNodes {\n\n    // Test basic text responses\n    askLLM withInput \"Hello\" outputs assistantMessage(\"Hello!\")\n\n    // Test tool call responses\n    askLLM withInput \"Solve task\" outputs toolCallMessage(CreateTool, CreateTool.Args(\"solve\"))\n}\n</code></pre> <p>The example above shows how to test the following behavior: 1. When the LLM node receives <code>Hello</code> as the input, it responds with a simple text message. 2. When it receives <code>Solve task</code>, it responds with a tool call.</p>"},{"location":"testing/#testing-tool-run-nodes","title":"Testing tool run nodes","text":"<p>You can also test nodes that run tools:</p> <pre><code>assertNodes {\n    // Test tool runs with specific arguments\n    callTool withInput toolCallMessage(\n        SolveTool,\n        SolveTool.Args(\"solve\")\n    ) outputs toolResult(SolveTool, \"solved\")\n}\n</code></pre> <p>This verifies that when the tool execution node receives a specific tool call signature, it produces the expected tool result.</p>"},{"location":"testing/#advanced-node-testing","title":"Advanced node testing","text":"<p>For more complex scenarios, you can test nodes with structured inputs and outputs:</p> <pre><code>assertNodes {\n    // Test with different inputs to the same node\n    askLLM withInput \"Simple query\" outputs assistantMessage(\"Simple response\")\n\n    // Test with complex parameters\n    askLLM withInput \"Complex query with parameters\" outputs toolCallMessage(\n        AnalyzeTool,\n        AnalyzeTool.Args(query = \"parameters\", depth = 3)\n    )\n}\n</code></pre> <p>You can also test complex tool call scenarios with detailed result structures:</p> <pre><code>assertNodes {\n    // Test a complex tool call with a structured result\n    callTool withInput toolCallMessage(\n        AnalyzeTool,\n        AnalyzeTool.Args(query = \"complex\", depth = 5)\n    ) outputs toolResult(AnalyzeTool, AnalyzeTool.Result(\n        analysis = \"Detailed analysis\",\n        confidence = 0.95,\n        metadata = mapOf(\"source\" to \"database\", \"timestamp\" to \"2023-06-15\")\n    ))\n}\n</code></pre> <p>These advanced tests help ensure that your nodes handle complex data structures correctly, which is essential for sophisticated agent behaviors.</p>"},{"location":"testing/#testing-edge-connections","title":"Testing edge connections","text":"<p>Edge connections testing allows you to verify that your agent's graph correctly routes outputs from one node to the appropriate next node. This ensures that your agent follows the intended workflow paths based on different outputs.</p>"},{"location":"testing/#basic-edge-testing","title":"Basic edge testing","text":"<p>Start with simple edge connection tests:</p> <pre><code>assertEdges {\n    // Test text message routing\n    askLLM withOutput assistantMessage(\"Hello!\") goesTo giveFeedback\n\n    // Test tool call routing\n    askLLM withOutput toolCallMessage(CreateTool, CreateTool.Args(\"solve\")) goesTo callTool\n}\n</code></pre> <p>This example verifies the following behavior: 1. When the LLM node outputs a simple text message, the flow is directed to the <code>giveFeedback</code> node. 2. When it outputs a tool call, the flow is directed to the <code>callTool</code> node.</p>"},{"location":"testing/#testing-conditional-routing","title":"Testing conditional routing","text":"<p>You can test a more complex routing logic based on the content of outputs:</p> <pre><code>assertEdges {\n    // Different text responses can route to different nodes\n    askLLM withOutput assistantMessage(\"Need more information\") goesTo askForInfo\n    askLLM withOutput assistantMessage(\"Ready to proceed\") goesTo processRequest\n}\n</code></pre>"},{"location":"testing/#advanced-edge-testing","title":"Advanced edge testing","text":"<p>For sophisticated agents, you can test conditional routing based on structured data in tool results:</p> <pre><code>assertEdges {\n    // Test routing based on tool result content\n    callTool withOutput toolResult(\n        AnalyzeTool, \n        AnalyzeTool.Result(analysis = \"Needs more processing\", confidence = 0.5)\n    ) goesTo processResult\n}\n</code></pre> <p>You can also test complex decision paths based on different result properties:</p> <pre><code>assertEdges {\n    // Route to different nodes based on confidence level\n    callTool withOutput toolResult(\n        AnalyzeTool, \n        AnalyzeTool.Result(analysis = \"Complete\", confidence = 0.9)\n    ) goesTo finish\n\n    callTool withOutput toolResult(\n        AnalyzeTool, \n        AnalyzeTool.Result(analysis = \"Uncertain\", confidence = 0.3)\n    ) goesTo verifyResult\n}\n</code></pre> <p>These advanced edge tests help ensure that your agent makes the correct decisions based on the content and structure of node outputs, which is essential for creating intelligent, context-aware workflows.</p>"},{"location":"testing/#complete-testing-example","title":"Complete testing example","text":"<p>Here is a user story that demonstrates a complete testing scenario:</p> <p>You are developing a tone analysis agent that analyzes the tone of the text and provides feedback. The agent uses tools for detecting positive, negative, and neutral tones.</p> <p>Here is how you can test this agent:</p> <pre><code>@Test\nfun testToneAgent() = runTest {\n    // Create a list to track tool calls\n    var toolCalls = mutableListOf&lt;String&gt;()\n    var result: String? = null\n\n    // Create a tool registry\n    val toolRegistry = ToolRegistry {\n        // A special tool, required with this type of agent\n        tool(SayToUser)\n\n        with(ToneTools) {\n            tools()\n        }\n    }\n\n    // Create an event handler\n    val eventHandler = EventHandler {\n        onToolCall { tool, args -&gt;\n            println(\"[DEBUG_LOG] Tool called: tool ${tool.name}, args $args\")\n            toolCalls.add(tool.name)\n        }\n\n        handleError {\n            println(\"[DEBUG_LOG] An error occurred: ${it.message}\\n${it.stackTraceToString()}\")\n            true\n        }\n\n        handleResult {\n            println(\"[DEBUG_LOG] Result: $it\")\n            result = it\n        }\n    }\n\n    val positiveText = \"I love this product!\"\n    val negativeText = \"Awful service, hate the app.\"\n    val defaultText = \"I don't know how to answer this question.\"\n\n    val positiveResponse = \"The text has a positive tone.\"\n    val negativeResponse = \"The text has a negative tone.\"\n    val neutralResponse = \"The text has a neutral tone.\"\n\n    val mockLLMApi = getMockExecutor(toolRegistry, eventHandler) {\n        // Set up LLM responses for different input texts\n        mockLLMToolCall(NeutralToneTool, ToneTool.Args(defaultText)) onRequestEquals defaultText\n        mockLLMToolCall(PositiveToneTool, ToneTool.Args(positiveText)) onRequestEquals positiveText\n        mockLLMToolCall(NegativeToneTool, ToneTool.Args(negativeText)) onRequestEquals negativeText\n\n        // Mock the behavior where the LLM responds with just tool responses when the tools return results\n        mockLLMAnswer(positiveResponse) onRequestContains positiveResponse\n        mockLLMAnswer(negativeResponse) onRequestContains negativeResponse\n        mockLLMAnswer(neutralResponse) onRequestContains neutralResponse\n\n        mockLLMAnswer(defaultText).asDefaultResponse\n\n        // Tool mocks\n        mockTool(PositiveToneTool) alwaysTells {\n            toolCalls += \"Positive tone tool called\"\n            positiveResponse\n        }\n        mockTool(NegativeToneTool) alwaysTells {\n            toolCalls += \"Negative tone tool called\"\n            negativeResponse\n        }\n        mockTool(NeutralToneTool) alwaysTells {\n            toolCalls += \"Neutral tone tool called\"\n            neutralResponse\n        }\n    }\n\n    // Create a strategy\n    val strategy = toneStrategy(\"tone_analysis\")\n\n    // Create an agent configuration\n    val agentConfig = AIAgentConfig(\n        prompt = prompt(\"test-agent\") {\n            system(\n                \"\"\"\n                You are an question answering agent with access to the tone analysis tools.\n                You need to answer 1 question with the best of your ability.\n                Be as concise as possible in your answers.\n                DO NOT ANSWER ANY QUESTIONS THAT ARE BESIDES PERFORMING TONE ANALYSIS!\n                DO NOT HALLUCINATE!\n            \"\"\".trimIndent()\n            )\n        },\n        model = mockk&lt;LLModel&gt;(relaxed = true),\n        maxAgentIterations = 10\n    )\n\n    // Create an agent with testing enabled\n    val agent = AIAgent(\n        promptExecutor = mockLLMApi,\n        toolRegistry = toolRegistry,\n        strategy = strategy,\n        eventHandler = eventHandler,\n        agentConfig = agentConfig,\n    ) {\n        withTesting()\n    }\n\n    // Test the positive text\n    agent.run(positiveText)\n    assertEquals(\"The text has a positive tone.\", result, \"Positive tone result should match\")\n    assertEquals(1, toolCalls.size, \"One tool is expected to be called\")\n\n    // Test the negative text\n    agent.run(negativeText)\n    assertEquals(\"The text has a negative tone.\", result, \"Negative tone result should match\")\n    assertEquals(2, toolCalls.size, \"Two tools are expected to be called\")\n\n    //Test the neutral text\n    agent.run(defaultText)\n    assertEquals(\"The text has a neutral tone.\", result, \"Neutral tone result should match\")\n    assertEquals(3, toolCalls.size, \"Three tools are expected to be called\")\n}\n</code></pre> <p>For more complex agents with multiple subgraphs, you can also test the graph structure:</p> <pre><code>@Test\nfun testMultiSubgraphAgentStructure() = runTest {\n    val strategy = strategy(\"test\") {\n        val firstSubgraph by subgraph(\n            \"first\",\n            tools = listOf(DummyTool, CreateTool, SolveTool)\n        ) {\n            val callLLM by nodeLLMRequest(allowToolCalls = false)\n            val executeTool by nodeExecuteTool()\n            val sendToolResult by nodeLLMSendToolResult()\n            val giveFeedback by node&lt;String, String&gt; { input -&gt;\n                llm.writeSession {\n                    updatePrompt {\n                        user(\"Call tools! Don't chat!\")\n                    }\n                }\n                input\n            }\n\n            edge(nodeStart forwardTo callLLM)\n            edge(callLLM forwardTo executeTool onToolCall { true })\n            edge(callLLM forwardTo giveFeedback onAssistantMessage { true })\n            edge(giveFeedback forwardTo giveFeedback onAssistantMessage { true })\n            edge(giveFeedback forwardTo executeTool onToolCall { true })\n            edge(executeTool forwardTo nodeFinish transformed { it.content })\n        }\n\n        val secondSubgraph by subgraph&lt;String, String&gt;(\"second\") {\n            edge(nodeStart forwardTo nodeFinish)\n        }\n\n        edge(nodeStart forwardTo firstSubgraph)\n        edge(firstSubgraph forwardTo secondSubgraph)\n        edge(secondSubgraph forwardTo nodeFinish)\n    }\n\n    val toolRegistry = ToolRegistry {\n        tool(DummyTool)\n        tool(CreateTool)\n        tool(SolveTool)\n    }\n\n    val mockLLMApi = getMockExecutor(toolRegistry) {\n        mockLLMAnswer(\"Hello!\") onRequestContains \"Hello\"\n        mockLLMToolCall(CreateTool, CreateTool.Args(\"solve\")) onRequestEquals \"Solve task\"\n    }\n\n    val basePrompt = prompt(\"test\") {}\n\n    AIAgent(\n        toolRegistry = toolRegistry,\n        strategy = strategy,\n        eventHandler = EventHandler {},\n        agentConfig = AIAgentConfig(prompt = basePrompt, model = OpenAIModels.Chat.GPT4o, maxAgentIterations = 100),\n        promptExecutor = mockLLMApi,\n    ) {\n        testGraph(\"test\") {\n            val firstSubgraph = assertSubgraphByName&lt;String, String&gt;(\"first\")\n            val secondSubgraph = assertSubgraphByName&lt;String, String&gt;(\"second\")\n\n            assertEdges {\n                startNode() alwaysGoesTo firstSubgraph\n                firstSubgraph alwaysGoesTo secondSubgraph\n                secondSubgraph alwaysGoesTo finishNode()\n            }\n\n            verifySubgraph(firstSubgraph) {\n                val start = startNode()\n                val finish = finishNode()\n\n                val askLLM = assertNodeByName&lt;String, Message.Response&gt;(\"callLLM\")\n                val callTool = assertNodeByName&lt;Message.Tool.Call, ReceivedToolResult&gt;(\"executeTool\")\n                val giveFeedback = assertNodeByName&lt;Any?, Any?&gt;(\"giveFeedback\")\n\n                assertReachable(start, askLLM)\n                assertReachable(askLLM, callTool)\n\n                assertNodes {\n                    askLLM withInput \"Hello\" outputs Message.Assistant(\"Hello!\")\n                    askLLM withInput \"Solve task\" outputs toolCallMessage(CreateTool, CreateTool.Args(\"solve\"))\n\n                    callTool withInput toolCallSignature(\n                        SolveTool,\n                        SolveTool.Args(\"solve\")\n                    ) outputs toolResult(SolveTool, \"solved\")\n\n                    callTool withInput toolCallSignature(\n                        CreateTool,\n                        CreateTool.Args(\"solve\")\n                    ) outputs toolResult(CreateTool, \"created\")\n                }\n\n                assertEdges {\n                    askLLM withOutput Message.Assistant(\"Hello!\") goesTo giveFeedback\n                    askLLM withOutput toolCallMessage(CreateTool, CreateTool.Args(\"solve\")) goesTo callTool\n                }\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"testing/#api-reference","title":"API reference","text":"<p>For a complete API reference related to the Testing feature, see the reference documentation for the agents-test module.</p>"},{"location":"testing/#faq-and-troubleshooting","title":"FAQ and troubleshooting","text":""},{"location":"testing/#how-do-i-mock-a-specific-tool-response","title":"How do I mock a specific tool response?","text":"<p>Use the <code>mockTool</code> method in <code>MockLLMBuilder</code>:</p> <pre><code>val mockExecutor = getMockExecutor {\n    mockTool(myTool) alwaysReturns myResult\n\n    // Or with conditions\n    mockTool(myTool) returns myResult onArguments myArgs\n}\n</code></pre>"},{"location":"testing/#how-can-i-test-complex-graph-structures","title":"How can I test complex graph structures?","text":"<p>Use the subgraph assertions, <code>verifySubgraph</code>, and node references:</p> <pre><code>testGraph&lt;Unit, String&gt;(\"test\") {\n    val mySubgraph = assertSubgraphByName&lt;Unit, String&gt;(\"mySubgraph\")\n\n    verifySubgraph(mySubgraph) {\n        // Get references to nodes\n        val nodeA = assertNodeByName&lt;Unit, String&gt;(\"nodeA\")\n        val nodeB = assertNodeByName&lt;String, String&gt;(\"nodeB\")\n\n        // Assert reachability\n        assertReachable(nodeA, nodeB)\n\n        // Assert edge connections\n        assertEdges {\n            nodeA.withOutput(\"result\") goesTo nodeB\n        }\n    }\n}\n</code></pre>"},{"location":"testing/#how-do-i-simulate-different-llm-responses-based-on-input","title":"How do I simulate different LLM responses based on input?","text":"<p>Use pattern matching methods:</p> <pre><code>getMockExecutor {\n    mockLLMAnswer(\"Response A\") onRequestContains \"topic A\"\n    mockLLMAnswer(\"Response B\") onRequestContains \"topic B\"\n    mockLLMAnswer(\"Exact response\") onRequestEquals \"exact question\"\n    mockLLMAnswer(\"Conditional response\") onCondition { it.contains(\"keyword\") &amp;&amp; it.length &gt; 10 }\n}\n</code></pre>"},{"location":"testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"testing/#mock-executor-always-returns-the-default-response","title":"Mock executor always returns the default response","text":"<p>Check that your pattern matching is correct. Patterns are case-sensitive and must match exactly as specified.</p>"},{"location":"testing/#tool-calls-are-not-being-intercepted","title":"Tool calls are not being intercepted","text":"<p>Ensure that:</p> <ol> <li>The tool registry is properly set up.</li> <li>The tool names match exactly.</li> <li>The tool actions are configured correctly.</li> </ol>"},{"location":"testing/#graph-assertions-are-failing","title":"Graph assertions are failing","text":"<ol> <li>Verify that node names are correct.</li> <li>Check that the graph structure matches your expectations.</li> <li>Use the <code>startNode()</code> and <code>finishNode()</code> methods to get the correct entry and exit points.</li> </ol>"},{"location":"tools-overview/","title":"Overview","text":"<p>Agents use tools to perform specific tasks or access external systems.</p>"},{"location":"tools-overview/#tool-workflow","title":"Tool workflow","text":"<p>The Koog framework offers the following workflow for working with tools:</p> <ol> <li>Create a custom tool or use one of the built-in tools.</li> <li>Add the tool to a tool registry.</li> <li>Pass the tool registry to an agent.</li> <li>Use the tool with the agent.</li> </ol>"},{"location":"tools-overview/#available-tool-types","title":"Available tool types","text":"<p>There are three types of tools in the Koog framework:</p> <ul> <li>Built-in tools that provide functionality for agent-user interaction and conversation management. For details, see Built-in tools.</li> <li>Annotation-based custom tools that let you expose functions as tools to LLMs. For details, see Annotation-based tools.</li> <li>Custom tools that let you control tool parameters, metadata, execution logic, and how it is registered and invoked. For details, see Class-based   tools.</li> </ul>"},{"location":"tools-overview/#tool-registry","title":"Tool registry","text":"<p>Before you can use a tool in an agent, you must add it to a tool registry. The tool registry manages all tools available to the agent.</p> <p>The key features of the tool registry:</p> <ul> <li>Organizes tools.</li> <li>Supports merging of multiple tool registries.</li> <li>Provides methods to retrieve tools by name or type.</li> </ul> <p>To learn more, see ToolRegistry.</p> <p>Here is an example of how to create the tool registry and add the tool to it:</p> <pre><code>val toolRegistry = ToolRegistry {\n    tool(SayToUser)\n}\n</code></pre> <p>To merge multiple tool registries, do the following:</p> <pre><code>val firstToolRegistry = ToolRegistry {\n    tool(FirstSampleTool)\n}\n\nval secondToolRegistry = ToolRegistry {\n    tool(SecondSampleTool)\n}\n\nval newRegistry = firstToolRegistry + secondToolRegistry\n</code></pre>"},{"location":"tools-overview/#passing-tools-to-an-agent","title":"Passing tools to an agent","text":"<p>To enable an agent to use a tool, you need to provide a tool registry that contains this tool as an argument when creating the agent:</p> <pre><code>// Agent initialization\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    systemPrompt = \"You are a helpful assistant with strong mathematical skills.\",\n    llmModel = OpenAIModels.Chat.GPT4o,\n    // Pass your tool registry to the agent\n    toolRegistry = toolRegistry\n)\n</code></pre>"},{"location":"tools-overview/#calling-tools","title":"Calling tools","text":"<p>There are several ways to call tools within your agent code. The recommended approach is to use the provided methods in the agent context rather than calling tools directly, as this ensures proper handling of tool operation within the agent environment.</p> <p>Tip</p> <p>Ensure you have implemented proper error handling in your tools to prevent agent failure.</p> <p>The tools are called within a specific session context represented by <code>AIAgentLLMWriteSession</code>. It provides several methods for calling tools so that you can:</p> <ul> <li>Call a tool with the given arguments.</li> <li>Call a tool by its name and the given arguments.</li> <li>Call a tool by the provided tool class and arguments.</li> <li>Call a tool of the specified type with the given arguments.</li> <li>Call a tool that returns a raw string result.</li> </ul> <p>For more details, see API reference.</p>"},{"location":"tools-overview/#parallel-tool-calls","title":"Parallel tool calls","text":"<p>You can also call tools in parallel using the <code>toParallelToolCallsRaw</code> extension. For example:</p> <pre><code>@Serializable\ndata class Book(\n    val title: String,\n    val author: String,\n    val description: String\n) : ToolArgs\n\nclass BookTool() : SimpleTool&lt;Book&gt;() {\n    companion object {\n        const val NAME = \"book\"\n    }\n\n    override suspend fun doExecute(args: Book): String {\n        println(\"${args.title} by ${args.author}:\\n ${args.description}\")\n        return \"Done\"\n    }\n\n    override val argsSerializer: KSerializer&lt;Book&gt;\n        get() = Book.serializer()\n\n    override val descriptor: ToolDescriptor\n        get() = ToolDescriptor(\n            name = NAME,\n            description = \"A tool to parse book information from Markdown\",\n            requiredParameters = listOf(),\n            optionalParameters = listOf()\n        )\n}\n\nval strategy = strategy&lt;Unit, Unit&gt;(\"strategy-name\") {\n\n    /*...*/\n\n    val myNode by node&lt;Unit, Unit&gt; { _ -&gt;\n        llm.writeSession {\n            flow {\n                emit(Book(\"Book 1\", \"Author 1\", \"Description 1\"))\n            }.toParallelToolCallsRaw(BookTool::class).collect()\n        }\n    }\n}\n</code></pre>"},{"location":"tools-overview/#calling-tools-from-nodes","title":"Calling tools from nodes","text":"<p>When building agent workflows with nodes, you can use special nodes to call tools:</p> <ul> <li> <p>nodeExecuteTool: calls a single tool call and returns its result. For details, see API reference.</p> </li> <li> <p>nodeExecuteSingleTool that calls a specific tool with the provided arguments. For details, see API reference.</p> </li> <li> <p>nodeExecuteMultipleTools that performs multiple tool calls and returns their results. For details, see API reference.</p> </li> <li> <p>nodeLLMSendToolResult that sends a tool result to the LLM and gets a response. For details, see API reference.</p> </li> <li> <p>nodeLLMSendMultipleToolResults that sends multiple tool results to the LLM. For details, see API reference.</p> </li> </ul>"},{"location":"tools-overview/#using-agents-as-tools","title":"Using agents as tools","text":"<p>The framework provides the capability to convert any AI agent into a tool that can be used by other agents.  This powerful feature enables you to create hierarchical agent architectures where specialized agents can be called as tools by higher-level orchestrating agents.</p>"},{"location":"tools-overview/#converting-agents-to-tools","title":"Converting agents to tools","text":"<p>To convert an agent into a tool, use the <code>asTool()</code> extension function:</p> <pre><code>// Create a specialized agent\nval analysisAgent = AIAgent(\n    executor = simpleOpenAIExecutor(apiKey),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    systemPrompt = \"You are a financial analysis specialist.\",\n    toolRegistry = analysisToolRegistry\n)\n\n// Convert the agent to a tool\nval analysisAgentTool = analysisAgent.asTool(\n    agentName = \"analyzeTransactions\",\n    agentDescription = \"Performs financial transaction analysis\",\n    inputDescriptor = ToolParameterDescriptor(\n        name = \"request\",\n        description = \"Transaction analysis request\",\n        type = ToolParameterType.String\n    )\n)\n</code></pre>"},{"location":"tools-overview/#using-agent-tools-in-other-agents","title":"Using agent tools in other agents","text":"<p>Once converted to a tool, you can add the agent tool to another agent's tool registry:</p> <pre><code>// Create a coordinator agent that can use specialized agents as tools\nval coordinatorAgent = AIAgent(\n    executor = simpleOpenAIExecutor(apiKey),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    systemPrompt = \"You coordinate different specialized services.\",\n    toolRegistry = ToolRegistry {\n        tool(analysisAgentTool)\n        // Add other tools as needed\n    }\n)\n</code></pre>"},{"location":"tools-overview/#agent-tool-execution","title":"Agent tool execution","text":"<p>When an agent tool is called:</p> <ol> <li>The arguments are deserialized according to the input descriptor</li> <li>The wrapped agent is executed with the deserialized input</li> <li>The agent's output is serialized and returned as the tool result</li> </ol>"},{"location":"tools-overview/#benefits-of-agents-as-tools","title":"Benefits of agents as tools","text":"<ul> <li>Modularity: Break complex workflows into specialized agents</li> <li>Reusability: Use the same specialized agent across multiple coordinator agents</li> <li>Separation of concerns: Each agent can focus on its specific domain</li> </ul>"},{"location":"tracing/","title":"Tracing","text":"<p>This page includes details about the Tracing feature, which provides comprehensive tracing capabilities for AI agents.</p>"},{"location":"tracing/#feature-overview","title":"Feature overview","text":"<p>The Tracing feature is a powerful monitoring and debugging tool that captures detailed information about agent runs, including:</p> <ul> <li>Strategy execution</li> <li>LLM calls</li> <li>Tool invocations</li> <li>Node execution within the agent graph</li> </ul> <p>This feature operates by intercepting key events in the agent pipeline and forwarding them to configurable message processors. These processors can output the trace information to various destinations such as log files or other types of files in the filesystem, enabling developers to gain insights into agent behavior and troubleshoot issues effectively.</p>"},{"location":"tracing/#event-flow","title":"Event flow","text":"<ol> <li>The Tracing feature intercepts events in the agent pipeline.</li> <li>Events are filtered based on the configured message filter.</li> <li>Filtered events are passed to registered message processors.</li> <li>Message processors format and output the events to their respective destinations.</li> </ol>"},{"location":"tracing/#configuration-and-initialization","title":"Configuration and initialization","text":""},{"location":"tracing/#basic-setup","title":"Basic setup","text":"<p>To use the Tracing feature, you need to:</p> <ol> <li>Have one or more message processors (you can use the existing ones or create your own).</li> <li>Install <code>Tracing</code> in your agent.</li> <li>Configure the message filter (optional).</li> <li>Add the message processors to the feature.</li> </ol> <pre><code>// Defining a logger/file that will be used as a destination of trace messages \nval logger = KotlinLogging.logger { }\nval outputPath = Path(\"/path/to/trace.log\")\n\n// Creating an agent\nval agent = AIAgent(\n   executor = simpleOllamaAIExecutor(),\n   llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n   install(Tracing) {\n      // Configure message processors to handle trace events\n      addMessageProcessor(TraceFeatureMessageLogWriter(logger))\n      addMessageProcessor(\n         TraceFeatureMessageFileWriter(\n            outputPath,\n            { path: Path -&gt; SystemFileSystem.sink(path).buffered() }\n         )\n      )\n\n      // Optionally filter messages\n      messageFilter = { message -&gt;\n         // Only trace LLM calls and tool calls\n         message is AfterLLMCallEvent || message is ToolCallEvent\n      }\n   }\n}\n</code></pre>"},{"location":"tracing/#message-filtering","title":"Message filtering","text":"<p>You can process all existing events or select some of them based on specific criteria. The message filter lets you control which events are processed. This is useful for focusing on specific aspects of agent runs:</p> <pre><code>// Filter for LLM-related events only\nmessageFilter = { message -&gt; \n    message is BeforeLLMCallEvent || message is AfterLLMCallEvent\n}\n\n// Filter for tool-related events only\nmessageFilter = { message -&gt; \n    message is ToolCallEvent ||\n           message is ToolCallResultEvent ||\n           message is ToolValidationErrorEvent ||\n           message is ToolCallFailureEvent\n}\n\n// Filter for node execution events only\nmessageFilter = { message -&gt; \n    message is AIAgentNodeExecutionStartEvent || message is AIAgentNodeExecutionEndEvent\n}\n</code></pre>"},{"location":"tracing/#large-trace-volumes","title":"Large trace volumes","text":"<p>For agents with complex strategies or long-running executions, the volume of trace events can be substantial. Consider using the following methods to manage the volume of events:</p> <ul> <li>Use specific message filters to reduce the number of events.</li> <li>Implement custom message processors with buffering or sampling.</li> <li>Use file rotation for log files to prevent them from growing too large.</li> </ul>"},{"location":"tracing/#dependency-graph","title":"Dependency graph","text":"<p>The Tracing feature has the following dependencies:</p> <pre><code>Tracing\n\u251c\u2500\u2500 AIAgentPipeline (for intercepting events)\n\u251c\u2500\u2500 TraceFeatureConfig\n\u2502   \u2514\u2500\u2500 FeatureConfig\n\u251c\u2500\u2500 Message Processors\n\u2502   \u251c\u2500\u2500 TraceFeatureMessageLogWriter\n\u2502   \u2502   \u2514\u2500\u2500 FeatureMessageLogWriter\n\u2502   \u251c\u2500\u2500 TraceFeatureMessageFileWriter\n\u2502   \u2502   \u2514\u2500\u2500 FeatureMessageFileWriter\n\u2502   \u2514\u2500\u2500 TraceFeatureMessageRemoteWriter\n\u2502       \u2514\u2500\u2500 FeatureMessageRemoteWriter\n\u2514\u2500\u2500 Event Types (from ai.koog.agents.core.feature.model)\n    \u251c\u2500\u2500 AIAgentStartedEvent\n    \u251c\u2500\u2500 AIAgentFinishedEvent\n    \u251c\u2500\u2500 AIAgentRunErrorEvent\n    \u251c\u2500\u2500 AIAgentStrategyStartEvent\n    \u251c\u2500\u2500 AIAgentStrategyFinishedEvent\n    \u251c\u2500\u2500 AIAgentNodeExecutionStartEvent\n    \u251c\u2500\u2500 AIAgentNodeExecutionEndEvent\n    \u251c\u2500\u2500 LLMCallStartEvent\n    \u251c\u2500\u2500 LLMCallWithToolsStartEvent\n    \u251c\u2500\u2500 LLMCallEndEvent\n    \u251c\u2500\u2500 LLMCallWithToolsEndEvent\n    \u251c\u2500\u2500 ToolCallEvent\n    \u251c\u2500\u2500 ToolValidationErrorEvent\n    \u251c\u2500\u2500 ToolCallFailureEvent\n    \u2514\u2500\u2500 ToolCallResultEvent\n</code></pre>"},{"location":"tracing/#examples-and-quickstarts","title":"Examples and quickstarts","text":""},{"location":"tracing/#basic-tracing-to-logger","title":"Basic tracing to logger","text":"<pre><code>// Create a logger\nval logger = KotlinLogging.logger { }\n\nfun main() {\n    runBlocking {\n       // Create an agent with tracing\n       val agent = AIAgent(\n          executor = simpleOllamaAIExecutor(),\n          llmModel = OllamaModels.Meta.LLAMA_3_2,\n       ) {\n          install(Tracing) {\n             addMessageProcessor(TraceFeatureMessageLogWriter(logger))\n          }\n       }\n\n       // Run the agent\n       agent.run(\"Hello, agent!\")\n    }\n}\n</code></pre>"},{"location":"tracing/#error-handling-and-edge-cases","title":"Error handling and edge cases","text":""},{"location":"tracing/#no-message-processors","title":"No message processors","text":"<p>If no message processors are added to the Tracing feature, a warning will be logged:</p> <pre><code>Tracing Feature. No feature out stream providers are defined. Trace streaming has no target.\n</code></pre> <p>The feature will still intercept events, but they will not be processed or output anywhere.</p>"},{"location":"tracing/#resource-management","title":"Resource management","text":"<p>Message processors may hold resources (like file handles) that need to be properly released. Use the <code>use</code> extension function to ensure proper cleanup:</p> <pre><code>// Creating an agent\nval agent = AIAgent(\n    executor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    val writer = TraceFeatureMessageFileWriter(\n        outputPath,\n        { path: Path -&gt; SystemFileSystem.sink(path).buffered() }\n    )\n\n    install(Tracing) {\n        addMessageProcessor(writer)\n    }\n}\n// Run the agent\nagent.run(input)\n// Writer will be automatically closed when the block exits\n</code></pre>"},{"location":"tracing/#tracing-specific-events-to-file","title":"Tracing specific events to file","text":"<pre><code>install(Tracing) {\n    // Only trace LLM calls\n    messageFilter = { message -&gt;\n        message is BeforeLLMCallEvent || message is AfterLLMCallEvent\n    }\n    addMessageProcessor(writer)\n}\n</code></pre>"},{"location":"tracing/#tracing-specific-events-to-remote-endpoint","title":"Tracing specific events to remote endpoint","text":"<p>You use tracing to remote endpoints when you need to send event data via the network. Once initiated, tracing to a remote endpoint launches a light server at the specified port number and sends events via Kotlin Server-Sent Events  (SSE).</p> <pre><code>// Creating an agent\nval agent = AIAgent(\n    executor = simpleOllamaAIExecutor(),\n    llmModel = OllamaModels.Meta.LLAMA_3_2,\n) {\n    val connectionConfig = AIAgentFeatureServerConnectionConfig(host = host, port = port)\n    val writer = TraceFeatureMessageRemoteWriter(\n        connectionConfig = connectionConfig\n    )\n\n    install(Tracing) {\n        addMessageProcessor(writer)\n    }\n}\n// Run the agent\nagent.run(input)\n// Writer will be automatically closed when the block exits\n</code></pre> <p>On the client side, you can use <code>FeatureMessageRemoteClient</code> to receive events and deserialize them.</p> <pre><code>val clientConfig = AIAgentFeatureClientConnectionConfig(host = host, port = port, protocol = URLProtocol.HTTP)\nval agentEvents = mutableListOf&lt;DefinedFeatureEvent&gt;()\n\nval clientJob = launch {\n    FeatureMessageRemoteClient(connectionConfig = clientConfig, scope = this).use { client -&gt;\n        val collectEventsJob = launch {\n            client.receivedMessages.consumeAsFlow().collect { event -&gt;\n                // Collect events from server\n                agentEvents.add(event as DefinedFeatureEvent)\n\n                // Stop collecting events on angent finished\n                if (event is AIAgentFinishedEvent) {\n                    cancel()\n                }\n            }\n        }\n        client.connect()\n        collectEventsJob.join()\n        client.healthCheck()\n    }\n}\n\nlistOf(clientJob).joinAll()\n</code></pre>"},{"location":"tracing/#api-documentation","title":"API documentation","text":"<p>The Tracing feature follows a modular architecture with these key components:</p> <ol> <li>Tracing: the main feature class that intercepts events in the agent pipeline.</li> <li>TraceFeatureConfig: configuration class for customizing feature behavior.</li> <li>Message Processors: components that process and output trace events:<ul> <li>TraceFeatureMessageLogWriter: writes trace events to a logger.</li> <li>TraceFeatureMessageFileWriter: writes trace events to a file.</li> <li>TraceFeatureMessageRemoteWriter: sends trace events to a remote server.</li> </ul> </li> </ol>"},{"location":"tracing/#faq-and-troubleshooting","title":"FAQ and troubleshooting","text":"<p>The following section includes commonly asked questions and answers related to the Tracing feature. </p>"},{"location":"tracing/#how-do-i-trace-only-specific-parts-of-my-agents-execution","title":"How do I trace only specific parts of my agent's execution?","text":"<p>Use the <code>messageFilter</code> property to filter events. For example, to trace only node execution:</p> <pre><code>install(Tracing) {\n   // Only trace LLM calls\n   messageFilter = { message -&gt;\n      message is BeforeLLMCallEvent || message is AfterLLMCallEvent\n   }\n   addMessageProcessor(writer)\n}\n</code></pre>"},{"location":"tracing/#can-i-use-multiple-message-processors","title":"Can I use multiple message processors?","text":"<p>Yes, you can add multiple message processors to trace to different destinations simultaneously:</p> <pre><code>install(Tracing) {\n    addMessageProcessor(TraceFeatureMessageLogWriter(logger))\n    addMessageProcessor(TraceFeatureMessageFileWriter(outputPath, syncOpener))\n    addMessageProcessor(TraceFeatureMessageRemoteWriter(connectionConfig))\n}\n</code></pre>"},{"location":"tracing/#how-can-i-create-a-custom-message-processor","title":"How can I create a custom message processor?","text":"<p>Implement the <code>FeatureMessageProcessor</code> interface:</p> <pre><code>class CustomTraceProcessor : FeatureMessageProcessor() {\n\n    // Current open state of the processor\n    private var _isOpen = MutableStateFlow(false)\n\n    override val isOpen: StateFlow&lt;Boolean&gt;\n        get() = _isOpen.asStateFlow()\n\n    override suspend fun processMessage(message: FeatureMessage) {\n        // Custom processing logic\n        when (message) {\n            is AIAgentNodeExecutionStartEvent -&gt; {\n                // Process node start event\n            }\n\n            is AfterLLMCallEvent -&gt; {\n                // Process LLM call end event\n           }\n            // Handle other event types \n        }\n    }\n\n    override suspend fun close() {\n        // Close connections of established\n    }\n}\n\n// Use your custom processor\ninstall(Tracing) {\n    addMessageProcessor(CustomTraceProcessor())\n}\n</code></pre> <p>For more information about existing event types that can be handled by message processors, see Predefined event types.</p>"},{"location":"tracing/#predefined-event-types","title":"Predefined event types","text":"<p>Koog provides predefined event types that can be used in custom message processors. The predefined events can be classified into several categories, depending on the entity they relate to:</p> <ul> <li>Agent events</li> <li>Strategy events</li> <li>Node events</li> <li>LLM call events</li> <li>Tool call events</li> </ul>"},{"location":"tracing/#agent-events","title":"Agent events","text":""},{"location":"tracing/#aiagentstartedevent","title":"AIAgentStartedEvent","text":"<p>Represents the start of an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>strategyName</code> String Yes The name of the strategy that the agent should follow. <code>eventId</code> String No <code>AIAgentStartedEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#aiagentfinishedevent","title":"AIAgentFinishedEvent","text":"<p>Represents the end of an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>strategyName</code> String Yes The name of the strategy that the agent followed. <code>result</code> String Yes The result of the agent run. Can be <code>null</code> if there is no result. <code>eventId</code> String No <code>AIAgentFinishedEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#aiagentrunerrorevent","title":"AIAgentRunErrorEvent","text":"<p>Represents the occurrence of an error during an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>strategyName</code> String Yes The name of the strategy that the agent followed. <code>error</code> AIAgentError Yes The specific error that occurred during the agent run. For more information, see AIAgentError. <code>eventId</code> String No <code>AIAgentRunErrorEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class. <p> The <code>AIAgentError</code> class provides more details about an error that occurred during an agent run. Includes the following fields:</p> Name Data type Required Default Description <code>message</code> String Yes The message that provides more details about the specific error. <code>stackTrace</code> String Yes The collection of stack records until the last executed code. <code>cause</code> String No null The cause of the error, if available."},{"location":"tracing/#strategy-events","title":"Strategy events","text":""},{"location":"tracing/#aiagentstrategystartevent","title":"AIAgentStrategyStartEvent","text":"<p>Represents the start of a strategy run. Includes the following fields:</p> Name Data type Required Default Description <code>strategyName</code> String Yes The name of the strategy. <code>eventId</code> String No <code>AIAgentStrategyStartEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#aiagentstrategyfinishedevent","title":"AIAgentStrategyFinishedEvent","text":"<p>Represents the end of a strategy run. Includes the following fields:</p> Name Data type Required Default Description <code>strategyName</code> String Yes The name of the strategy. <code>result</code> String Yes The result of the run. <code>eventId</code> String No <code>AIAgentStrategyFinishedEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#node-events","title":"Node events","text":""},{"location":"tracing/#aiagentnodeexecutionstartevent","title":"AIAgentNodeExecutionStartEvent","text":"<p>Represents the start of a node run. Includes the following fields:</p> Name Data type Required Default Description <code>nodeName</code> String Yes The name of the node whose run started. <code>input</code> String Yes The input value for the node. <code>eventId</code> String No <code>AIAgentNodeExecutionStartEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#aiagentnodeexecutionendevent","title":"AIAgentNodeExecutionEndEvent","text":"<p>Represents the end of a node run. Includes the following fields:</p> Name Data type Required Default Description <code>nodeName</code> String Yes The name of the node whose run ended. <code>input</code> String Yes The input value for the node. <code>output</code> String Yes The output value produced by the node. <code>eventId</code> String No <code>AIAgentNodeExecutionEndEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#llm-call-events","title":"LLM call events","text":""},{"location":"tracing/#llmcallstartevent","title":"LLMCallStartEvent","text":"<p>Represents the start of an LLM call. Includes the following fields:</p> Name Data type Required Default Description <code>prompt</code> Prompt Yes The prompt that is sent to the model. For more information, see Prompt. <code>tools</code> List&lt;String&gt; Yes The list of tools that the model can call. <code>eventId</code> String No <code>LLMCallStartEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class. <p> The <code>Prompt</code> class represents a data structure for a prompt, consisting of a list of messages, a unique identifier, and optional parameters for language model settings. Includes the following fields:</p> Name Data type Required Default Description <code>messages</code> List&lt;Message&gt; Yes The list of messages that the prompt consists of. <code>id</code> String Yes The unique identifier for the prompt. <code>params</code> LLMParams No LLMParams() The settings that control the way the LLM generates content."},{"location":"tracing/#llmcallendevent","title":"LLMCallEndEvent","text":"<p>Represents the end of an LLM call. Includes the following fields:</p> Name Data type Required Default Description <code>responses</code> List&lt;Message.Response&gt; Yes One or more responses returned by the model. <code>eventId</code> String No <code>LLMCallEndEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#tool-call-events","title":"Tool call events","text":""},{"location":"tracing/#toolcallevent","title":"ToolCallEvent","text":"<p>Represents the event of a model calling a tool. Includes the following fields:</p> Name Data type Required Default Description <code>toolName</code> String Yes The name of the tool. <code>toolArgs</code> Tool.Args Yes The arguments that are provided to the tool. <code>eventId</code> String No <code>ToolCallEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#toolvalidationerrorevent","title":"ToolValidationErrorEvent","text":"<p>Represents the occurrence of a validation error during a tool call. Includes the following fields:</p> Name Data type Required Default Description <code>toolName</code> String Yes The name of the tool for which validation failed. <code>toolArgs</code> Tool.Args Yes The arguments that are provided to the tool. <code>errorMessage</code> String Yes The validation error message. <code>eventId</code> String No <code>ToolValidationErrorEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#toolcallfailureevent","title":"ToolCallFailureEvent","text":"<p>Represents a failure to call a tool. Includes the following fields:</p> Name Data type Required Default Description <code>toolName</code> String Yes The name of the tool. <code>toolArgs</code> Tool.Args Yes The arguments that are provided to the tool. <code>error</code> AIAgentError Yes The specific error that occurred when trying to call a tool. For more information, see AIAgentError. <code>eventId</code> String No <code>ToolCallFailureEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"tracing/#toolcallresultevent","title":"ToolCallResultEvent","text":"<p>Represents a successful tool call with the return of a result. Includes the following fields:</p> Name Data type Required Default Description <code>toolName</code> String Yes The name of the tool. <code>toolArgs</code> Tool.Args Yes The arguments that are provided to the tool. <code>result</code> ToolResult Yes The result of the tool call. <code>eventId</code> String No <code>ToolCallResultEvent</code> The identifier of the event. Usually the <code>simpleName</code> of the event class."},{"location":"examples/Attachments/","title":"Attachments","text":"<p> Open on GitHub  Download .ipynb</p>"},{"location":"examples/Attachments/#setting-up-the-environment","title":"Setting Up the Environment","text":"<p>Before diving into the code, we make sure our Kotlin Notebook is ready. Here we load the latest descriptors and enable the Koog library, which provides a clean API for working with AI model providers.</p> <pre><code>// Loads the latest descriptors and activates Koog integration for Kotlin Notebook.\n// This makes Koog DSL types and executors available in further cells.\n%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/Attachments/#configuring-api-keys","title":"Configuring API Keys","text":"<p>We read the API key from an environment variable. This keeps secrets out of the notebook file and lets you switch providers. You can set <code>OPENAI_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>, or <code>GEMINI_API_KEY</code>.</p> <pre><code>val apiKey = System.getenv(\"OPENAI_API_KEY\") // or ANTHROPIC_API_KEY, or GEMINI_API_KEY\n</code></pre>"},{"location":"examples/Attachments/#creating-a-simple-openai-executor","title":"Creating a Simple OpenAI Executor","text":"<p>The executor encapsulates authentication, base URLs, and correct defaults. Here we use a simple OpenAI executor, but you can swap it for Anthropic or Gemini without changing the rest of the code.</p> <pre><code>// --- Provider selection ---\n// For OpenAI-compatible models. Alternatives include:\n//   val executor = simpleAnthropicExecutor(System.getenv(\"ANTHROPIC_API_KEY\"))\n//   val executor = simpleGeminiExecutor(System.getenv(\"GEMINI_API_KEY\"))\n// All executors expose the same high\u2011level API.\nval executor = simpleOpenAIExecutor(apiKey)\n</code></pre> <p>Koog\u2019s prompt DSL lets you add structured Markdown and attachments. In this cell we build a prompt that asks the model to generate a short, blog\u2011style \"content card\" and we attach two images from the local <code>images/</code> directory.</p> <pre><code>import ai.koog.prompt.markdown.markdown\nimport kotlinx.io.files.Path\n\nval prompt = prompt(\"images-prompt\") {\n    system(\"You are professional assistant that can write cool and funny descriptions for Instagram posts.\")\n\n    user {\n        markdown {\n            +\"I want to create a new post on Instagram.\"\n            br()\n            +\"Can you write something creative under my instagram post with the following photos?\"\n            br()\n            h2(\"Requirements\")\n            bulleted {\n                item(\"It must be very funny and creative\")\n                item(\"It must increase my chance of becoming an ultra-famous blogger!!!!\")\n                item(\"It not contain explicit content, harassment or bullying\")\n                item(\"It must be a short catching phrase\")\n                item(\"You must include relevant hashtags that would increase the visibility of my post\")\n            }\n        }\n\n        attachments {\n            image(Path(\"images/kodee-loving.png\"))\n            image(Path(\"images/kodee-electrified.png\"))\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Attachments/#execute-and-inspect-the-response","title":"Execute and Inspect the Response","text":"<p>We run the prompt against <code>gpt-4.1</code>, collect the first message, and print its content. If you want streaming, swap to a streaming API in Koog; for tool use, pass your tool list instead of <code>emptyList()</code>.</p> <p>Troubleshooting: * 401/403 \u2014 check your API key/environment variable. * File not found \u2014 verify the <code>images/</code> paths. * Rate limits \u2014 add minimal retry/backoff around the call if needed.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    val response = executor.execute(prompt = prompt, model = OpenAIModels.Chat.GPT4_1, tools = emptyList()).first()\n    println(response.content)\n}\n</code></pre> <pre><code>Caption:\nRunning on cuteness and extra giggle power! Warning: Side effects may include heart-thief vibes and spontaneous dance parties. \ud83d\udc9c\ud83e\udd16\ud83d\udc83\n\nHashtags:  \n#ViralVibes #UltraFamousBlogger #CutieAlert #QuirkyContent #InstaFun #SpreadTheLove #DancingIntoFame #RobotLife #InstaFamous #FeedGoals\n</code></pre> <pre><code>runBlocking {\n    val response = executor.executeStreaming(prompt = prompt, model = OpenAIModels.Chat.GPT4_1)\n    response.collect { print(it) }\n}\n</code></pre> <pre><code>Caption:  \nRunning on good vibes &amp; wi-fi only! \ud83e\udd16\ud83d\udc9c Drop a like if you feel the circuit-joy! #BlogBotInTheWild #HeartDeliveryService #DancingWithWiFi #UltraFamousBlogger #MoreFunThanYourAICat #ViralVibes #InstaFun #BeepBoopFamous\n</code></pre>"},{"location":"examples/Banking/","title":"Building an AI Banking Assistant with Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this tutorial we\u2019ll build a small banking assistant using Koog agents in Kotlin. You\u2019ll learn how to: - Define domain models and sample data - Expose capability-focused tools for money transfers and transaction analytics - Classify user intent (Transfer vs Analytics) - Orchestrate calls in two styles:   1) a graph/subgraph strategy   2) \u201cagents as tools\u201d</p> <p>By the end, you\u2019ll be able to route free-form user requests to the right tools and produce helpful, auditable responses.</p>"},{"location":"examples/Banking/#setup-dependencies","title":"Setup &amp; Dependencies","text":"<p>We\u2019ll use the Kotlin Notebook kernel. Make sure your Koog artifacts are resolvable from Maven Central and your LLM provider key is available via <code>OPENAI_API_KEY</code>.</p> <pre><code>%useLatestDescriptors\n%use datetime\n\n// uncomment this for using koog from Maven Central\n// %use koog\n</code></pre> <pre><code>import ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\n\nval apiKey = System.getenv(\"OPENAI_API_KEY\") ?: error(\"Please set OPENAI_API_KEY environment variable\")\nval openAIExecutor = simpleOpenAIExecutor(apiKey)\n</code></pre>"},{"location":"examples/Banking/#defining-the-system-prompt","title":"Defining the System Prompt","text":"<p>A well-crafted system prompt helps the AI understand its role and constraints. This prompt will guide all our agents' behavior.</p> <pre><code>val bankingAssistantSystemPrompt = \"\"\"\n    |You are a banking assistant interacting with a user (userId=123).\n    |Your goal is to understand the user's request and determine whether it can be fulfilled using the available tools.\n    |\n    |If the task can be accomplished with the provided tools, proceed accordingly,\n    |at the end of the conversation respond with: \"Task completed successfully.\"\n    |If the task cannot be performed with the tools available, respond with: \"Can't perform the task.\"\n\"\"\".trimMargin()\n</code></pre>"},{"location":"examples/Banking/#domain-model-sample-data","title":"Domain model &amp; Sample data","text":"<p>First, let's define our domain models and sample data. We'll use Kotlin's data classes with serialization support.</p> <pre><code>import kotlinx.serialization.Serializable\n\n@Serializable\ndata class Contact(\n    val id: Int,\n    val name: String,\n    val surname: String? = null,\n    val phoneNumber: String\n)\n\nval contactList = listOf(\n    Contact(100, \"Alice\", \"Smith\", \"+1 415 555 1234\"),\n    Contact(101, \"Bob\", \"Johnson\", \"+49 151 23456789\"),\n    Contact(102, \"Charlie\", \"Williams\", \"+36 20 123 4567\"),\n    Contact(103, \"Daniel\", \"Anderson\", \"+46 70 123 45 67\"),\n    Contact(104, \"Daniel\", \"Garcia\", \"+34 612 345 678\"),\n)\n\nval contactById = contactList.associateBy(Contact::id)\n</code></pre>"},{"location":"examples/Banking/#tools-money-transfer","title":"Tools: Money Transfer","text":"<p>Tools should be pure and predictable.</p> <p>We model two \u201csoft contracts\u201d: - <code>chooseRecipient</code> returns candidates when ambiguity is detected. - <code>sendMoney</code> supports a <code>confirmed</code> flag. If <code>false</code>, it asks the agent to confirm with the user.</p> <pre><code>import ai.koog.agents.core.tools.annotations.LLMDescription\nimport ai.koog.agents.core.tools.annotations.Tool\nimport ai.koog.agents.core.tools.reflect.ToolSet\n\n@LLMDescription(\"Tools for money transfer operations.\")\nclass MoneyTransferTools : ToolSet {\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Returns the list of contacts for the given user.\n        The user in this demo is always userId=123.\n        \"\"\"\n    )\n    fun getContacts(\n        @LLMDescription(\"The unique identifier of the user whose contact list is requested.\") userId: Int\n    ): String = buildString {\n        contactList.forEach { c -&gt;\n            appendLine(\"${c.id}: ${c.name} ${c.surname ?: \"\"} (${c.phoneNumber})\")\n        }\n    }.trimEnd()\n\n    @Tool\n    @LLMDescription(\"Returns the current balance (demo value).\")\n    fun getBalance(\n        @LLMDescription(\"The unique identifier of the user.\") userId: Int\n    ): String = \"Balance: 200.00 EUR\"\n\n    @Tool\n    @LLMDescription(\"Returns the default user currency (demo value).\")\n    fun getDefaultCurrency(\n        @LLMDescription(\"The unique identifier of the user.\") userId: Int\n    ): String = \"EUR\"\n\n    @Tool\n    @LLMDescription(\"Returns a demo FX rate between two ISO currencies (e.g. EUR\u2192USD).\")\n    fun getExchangeRate(\n        @LLMDescription(\"Base currency (e.g., EUR).\") from: String,\n        @LLMDescription(\"Target currency (e.g., USD).\") to: String\n    ): String = when (from.uppercase() to to.uppercase()) {\n        \"EUR\" to \"USD\" -&gt; \"1.10\"\n        \"EUR\" to \"GBP\" -&gt; \"0.86\"\n        \"GBP\" to \"EUR\" -&gt; \"1.16\"\n        \"USD\" to \"EUR\" -&gt; \"0.90\"\n        else -&gt; \"No information about exchange rate available.\"\n    }\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Returns a ranked list of possible recipients for an ambiguous name.\n        The agent should ask the user to pick one and then use the selected contact id.\n        \"\"\"\n    )\n    fun chooseRecipient(\n        @LLMDescription(\"An ambiguous or partial contact name.\") confusingRecipientName: String\n    ): String {\n        val matches = contactList.filter { c -&gt;\n            c.name.contains(confusingRecipientName, ignoreCase = true) ||\n                (c.surname?.contains(confusingRecipientName, ignoreCase = true) ?: false)\n        }\n        if (matches.isEmpty()) {\n            return \"No candidates found for '$confusingRecipientName'. Use getContacts and ask the user to choose.\"\n        }\n        return matches.mapIndexed { idx, c -&gt;\n            \"${idx + 1}. ${c.id}: ${c.name} ${c.surname ?: \"\"} (${c.phoneNumber})\"\n        }.joinToString(\"\\n\")\n    }\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Sends money from the user to a contact.\n        If confirmed=false, return \"REQUIRES_CONFIRMATION\" with a human-readable summary.\n        The agent should confirm with the user before retrying with confirmed=true.\n        \"\"\"\n    )\n    fun sendMoney(\n        @LLMDescription(\"Sender user id.\") senderId: Int,\n        @LLMDescription(\"Amount in sender's default currency.\") amount: Double,\n        @LLMDescription(\"Recipient contact id.\") recipientId: Int,\n        @LLMDescription(\"Short purpose/description.\") purpose: String,\n        @LLMDescription(\"Whether the user already confirmed this transfer.\") confirmed: Boolean = false\n    ): String {\n        val recipient = contactById[recipientId] ?: return \"Invalid recipient.\"\n        val summary = \"Transfer \u20ac%.2f to %s %s (%s) for \\\"%s\\\".\"\n            .format(amount, recipient.name, recipient.surname ?: \"\", recipient.phoneNumber, purpose)\n\n        if (!confirmed) {\n            return \"REQUIRES_CONFIRMATION: $summary\"\n        }\n\n        // In a real system this is where you'd call a payment API.\n        return \"Money was sent. $summary\"\n    }\n}\n</code></pre>"},{"location":"examples/Banking/#creating-your-first-agent","title":"Creating Your First Agent","text":"<p>Now let's create an agent that uses our money transfer tools. An agent combines an LLM with tools to accomplish tasks.</p> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.core.tools.ToolRegistry\nimport ai.koog.agents.core.tools.reflect.asTools\nimport ai.koog.agents.ext.tool.AskUser\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport kotlinx.coroutines.runBlocking\n\nval transferAgent = AIAgent(\n    executor = openAIExecutor,\n    llmModel = OpenAIModels.Reasoning.GPT4oMini,\n    systemPrompt = bankingAssistantSystemPrompt,\n    temperature = 0.0,  // Use deterministic responses for financial operations\n    toolRegistry = ToolRegistry {\n        tool(AskUser)\n        tools(MoneyTransferTools().asTools())\n    }\n)\n\n// Test the agent with various scenarios\nprintln(\"Banking Assistant started\")\nval message = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n\n// Other test messages you can try:\n// - \"Send 50 euros to Alice for the concert tickets\"\n// - \"What's my current balance?\"\n// - \"Transfer 100 euros to Bob for the shared vacation expenses\"\n\nrunBlocking {\n    val result = transferAgent.run(message)\n    result\n}\n</code></pre> <pre><code>Banking Assistant started\nThere are two contacts named Daniel. Please confirm which one you would like to send money to:\n1. Daniel Anderson (+46 70 123 45 67)\n2. Daniel Garcia (+34 612 345 678)\nPlease confirm the transfer of \u20ac25.00 to Daniel Garcia (+34 612 345 678) for \"Dinner at the restaurant\".\n\n\n\n\n\nTask completed successfully.\n</code></pre>"},{"location":"examples/Banking/#adding-transaction-analytics","title":"Adding Transaction Analytics","text":"<p>Let's expand our assistant's capabilities with transaction analysis tools. First, we'll define the transaction domain model.</p> <pre><code>@Serializable\nenum class TransactionCategory(val title: String) {\n    FOOD_AND_DINING(\"Food &amp; Dining\"),\n    SHOPPING(\"Shopping\"),\n    TRANSPORTATION(\"Transportation\"),\n    ENTERTAINMENT(\"Entertainment\"),\n    GROCERIES(\"Groceries\"),\n    HEALTH(\"Health\"),\n    UTILITIES(\"Utilities\"),\n    HOME_IMPROVEMENT(\"Home Improvement\");\n\n    companion object {\n        fun fromString(value: String): TransactionCategory? =\n            entries.find { it.title.equals(value, ignoreCase = true) }\n\n        fun availableCategories(): String =\n            entries.joinToString(\", \") { it.title }\n    }\n}\n\n@Serializable\ndata class Transaction(\n    val merchant: String,\n    val amount: Double,\n    val category: TransactionCategory,\n    val date: LocalDateTime\n)\n</code></pre>"},{"location":"examples/Banking/#sample-transaction-data","title":"Sample transaction data","text":"<pre><code>val transactionAnalysisPrompt = \"\"\"\nToday is 2025-05-22.\nAvailable categories for transactions: ${TransactionCategory.availableCategories()}\n\"\"\"\n\nval sampleTransactions = listOf(\n    Transaction(\"Starbucks\", 5.99, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 22, 8, 30, 0, 0)),\n    Transaction(\"Amazon\", 129.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 22, 10, 15, 0, 0)),\n    Transaction(\n        \"Shell Gas Station\",\n        45.50,\n        TransactionCategory.TRANSPORTATION,\n        LocalDateTime(2025, 5, 21, 18, 45, 0, 0)\n    ),\n    Transaction(\"Netflix\", 15.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 21, 12, 0, 0, 0)),\n    Transaction(\"AMC Theaters\", 32.50, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 20, 19, 30, 0, 0)),\n    Transaction(\"Whole Foods\", 89.75, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 20, 16, 20, 0, 0)),\n    Transaction(\"Target\", 67.32, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 20, 14, 30, 0, 0)),\n    Transaction(\"CVS Pharmacy\", 23.45, TransactionCategory.HEALTH, LocalDateTime(2025, 5, 19, 11, 25, 0, 0)),\n    Transaction(\"Subway\", 12.49, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 19, 13, 15, 0, 0)),\n    Transaction(\"Spotify Premium\", 9.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 19, 14, 15, 0, 0)),\n    Transaction(\"AT&amp;T\", 85.00, TransactionCategory.UTILITIES, LocalDateTime(2025, 5, 18, 9, 0, 0, 0)),\n    Transaction(\"Home Depot\", 156.78, TransactionCategory.HOME_IMPROVEMENT, LocalDateTime(2025, 5, 18, 15, 45, 0, 0)),\n    Transaction(\"Amazon\", 129.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 17, 10, 15, 0, 0)),\n    Transaction(\"Starbucks\", 5.99, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 17, 8, 30, 0, 0)),\n    Transaction(\"Whole Foods\", 89.75, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 16, 16, 20, 0, 0)),\n    Transaction(\"CVS Pharmacy\", 23.45, TransactionCategory.HEALTH, LocalDateTime(2025, 5, 15, 11, 25, 0, 0)),\n    Transaction(\"AT&amp;T\", 85.00, TransactionCategory.UTILITIES, LocalDateTime(2025, 5, 14, 9, 0, 0, 0)),\n    Transaction(\"Xbox Game Pass\", 14.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 14, 16, 45, 0, 0)),\n    Transaction(\"Aldi\", 76.45, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 13, 17, 30, 0, 0)),\n    Transaction(\"Chipotle\", 15.75, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 13, 12, 45, 0, 0)),\n    Transaction(\"Best Buy\", 299.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 12, 14, 20, 0, 0)),\n    Transaction(\"Olive Garden\", 89.50, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 12, 19, 15, 0, 0)),\n    Transaction(\"Whole Foods\", 112.34, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 11, 10, 30, 0, 0)),\n    Transaction(\"Old Navy\", 45.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 11, 13, 45, 0, 0)),\n    Transaction(\"Panera Bread\", 18.25, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 10, 11, 30, 0, 0)),\n    Transaction(\"Costco\", 245.67, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 10, 15, 20, 0, 0)),\n    Transaction(\"Five Guys\", 22.50, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 9, 18, 30, 0, 0)),\n    Transaction(\"Macy's\", 156.78, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 9, 14, 15, 0, 0)),\n    Transaction(\"Hulu Plus\", 12.99, TransactionCategory.ENTERTAINMENT, LocalDateTime(2025, 5, 8, 20, 0, 0, 0)),\n    Transaction(\"Whole Foods\", 94.23, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 8, 16, 45, 0, 0)),\n    Transaction(\"Texas Roadhouse\", 78.90, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 8, 19, 30, 0, 0)),\n    Transaction(\"Walmart\", 167.89, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 7, 11, 20, 0, 0)),\n    Transaction(\"Chick-fil-A\", 14.75, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 7, 12, 30, 0, 0)),\n    Transaction(\"Aldi\", 82.45, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 6, 15, 45, 0, 0)),\n    Transaction(\"TJ Maxx\", 67.90, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 6, 13, 20, 0, 0)),\n    Transaction(\"P.F. Chang's\", 95.40, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 5, 19, 15, 0, 0)),\n    Transaction(\"Whole Foods\", 78.34, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 4, 14, 30, 0, 0)),\n    Transaction(\"H&amp;M\", 89.99, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 3, 16, 20, 0, 0)),\n    Transaction(\"Red Lobster\", 112.45, TransactionCategory.FOOD_AND_DINING, LocalDateTime(2025, 5, 2, 18, 45, 0, 0)),\n    Transaction(\"Whole Foods\", 67.23, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 2, 11, 30, 0, 0)),\n    Transaction(\"Marshalls\", 123.45, TransactionCategory.SHOPPING, LocalDateTime(2025, 5, 1, 15, 20, 0, 0)),\n    Transaction(\n        \"Buffalo Wild Wings\",\n        45.67,\n        TransactionCategory.FOOD_AND_DINING,\n        LocalDateTime(2025, 5, 1, 19, 30, 0, 0)\n    ),\n    Transaction(\"Aldi\", 145.78, TransactionCategory.GROCERIES, LocalDateTime(2025, 5, 1, 10, 15, 0, 0))\n)\n</code></pre>"},{"location":"examples/Banking/#transaction-analysis-tools","title":"Transaction Analysis Tools","text":"<pre><code>@LLMDescription(\"Tools for analyzing transaction history\")\nclass TransactionAnalysisTools : ToolSet {\n\n    @Tool\n    @LLMDescription(\n        \"\"\"\n        Retrieves transactions filtered by userId, category, start date, and end date.\n        All parameters are optional. If no parameters are provided, all transactions are returned.\n        Dates should be in the format YYYY-MM-DD.\n        \"\"\"\n    )\n    fun getTransactions(\n        @LLMDescription(\"The ID of the user whose transactions to retrieve.\")\n        userId: String? = null,\n        @LLMDescription(\"The category to filter transactions by (e.g., 'Food &amp; Dining').\")\n        category: String? = null,\n        @LLMDescription(\"The start date to filter transactions by, in the format YYYY-MM-DD.\")\n        startDate: String? = null,\n        @LLMDescription(\"The end date to filter transactions by, in the format YYYY-MM-DD.\")\n        endDate: String? = null\n    ): String {\n        var filteredTransactions = sampleTransactions\n\n        // Validate userId (in production, this would query a real database)\n        if (userId != null &amp;&amp; userId != \"123\") {\n            return \"No transactions found for user $userId.\"\n        }\n\n        // Apply category filter\n        category?.let { cat -&gt;\n            val categoryEnum = TransactionCategory.fromString(cat)\n                ?: return \"Invalid category: $cat. Available: ${TransactionCategory.availableCategories()}\"\n            filteredTransactions = filteredTransactions.filter { it.category == categoryEnum }\n        }\n\n        // Apply date range filters\n        startDate?.let { date -&gt;\n            val startDateTime = parseDate(date, startOfDay = true)\n            filteredTransactions = filteredTransactions.filter { it.date &gt;= startDateTime }\n        }\n\n        endDate?.let { date -&gt;\n            val endDateTime = parseDate(date, startOfDay = false)\n            filteredTransactions = filteredTransactions.filter { it.date &lt;= endDateTime }\n        }\n\n        if (filteredTransactions.isEmpty()) {\n            return \"No transactions found matching the specified criteria.\"\n        }\n\n        return filteredTransactions.joinToString(\"\\n\") { transaction -&gt;\n            \"${transaction.date}: ${transaction.merchant} - \" +\n                \"$${transaction.amount} (${transaction.category.title})\"\n        }\n    }\n\n    @Tool\n    @LLMDescription(\"Calculates the sum of an array of double numbers.\")\n    fun sumArray(\n        @LLMDescription(\"Comma-separated list of double numbers to sum (e.g., '1.5,2.3,4.7').\")\n        numbers: String\n    ): String {\n        val numbersList = numbers.split(\",\")\n            .mapNotNull { it.trim().toDoubleOrNull() }\n        val sum = numbersList.sum()\n        return \"Sum: $%.2f\".format(sum)\n    }\n\n    // Helper function to parse dates\n    private fun parseDate(dateStr: String, startOfDay: Boolean): LocalDateTime {\n        val parts = dateStr.split(\"-\").map { it.toInt() }\n        require(parts.size == 3) { \"Invalid date format. Use YYYY-MM-DD\" }\n\n        return if (startOfDay) {\n            LocalDateTime(parts[0], parts[1], parts[2], 0, 0, 0, 0)\n        } else {\n            LocalDateTime(parts[0], parts[1], parts[2], 23, 59, 59, 999999999)\n        }\n    }\n}\n</code></pre> <pre><code>val analysisAgent = AIAgent(\n    executor = openAIExecutor,\n    llmModel = OpenAIModels.Reasoning.GPT4oMini,\n    systemPrompt = \"$bankingAssistantSystemPrompt\\n$transactionAnalysisPrompt\",\n    temperature = 0.0,\n    toolRegistry = ToolRegistry {\n        tools(TransactionAnalysisTools().asTools())\n    }\n)\n\nprintln(\"Transaction Analysis Assistant started\")\nval analysisMessage = \"How much have I spent on restaurants this month?\"\n\n// Other queries to try:\n// - \"What's my maximum check at a restaurant this month?\"\n// - \"How much did I spend on groceries in the first week of May?\"\n// - \"What's my total spending on entertainment in May?\"\n// - \"Show me all transactions from last week\"\n\nrunBlocking {\n    val result = analysisAgent.run(analysisMessage)\n    result\n}\n</code></pre> <pre><code>Transaction Analysis Assistant started\n\n\n\n\n\nYou have spent a total of $517.64 on restaurants this month.\n\nTask completed successfully.\n</code></pre>"},{"location":"examples/Banking/#building-an-agent-with-graph","title":"Building an Agent with Graph","text":"<p>Now let's combine our specialized agents into a graph agent that can route requests to the appropriate handler.</p>"},{"location":"examples/Banking/#request-classification","title":"Request Classification","text":"<p>First, we need a way to classify incoming requests:</p> <pre><code>import ai.koog.agents.ext.agent.SerializableSubgraphResult\nimport kotlinx.serialization.SerialName\n\n@SerialName(\"UserRequestType\")\n@Serializable\n@LLMDescription(\"Type of user request: Transfer or Analytics\")\nenum class RequestType {\n    Transfer,\n    Analytics\n}\n\n@Serializable\n@LLMDescription(\"The bank request that was classified by the agent.\")\ndata class ClassifiedBankRequest(\n    @LLMDescription(\"Type of request: Transfer or Analytics\")\n    val requestType: RequestType,\n    @LLMDescription(\"Actual request to be performed by the banking application\")\n    val userRequest: String\n) : SerializableSubgraphResult&lt;ClassifiedBankRequest&gt; {\n    override fun getSerializer() = serializer()\n}\n</code></pre>"},{"location":"examples/Banking/#shared-tool-registry","title":"Shared tool registry","text":"<pre><code>import ai.koog.agents.ext.agent.ProvideStringSubgraphResult\n\n// Create a comprehensive tool registry for the multi-agent system\nval toolRegistry = ToolRegistry {\n    tool(AskUser)  // Allow agents to ask for clarification\n    tools(MoneyTransferTools().asTools())\n    tools(TransactionAnalysisTools().asTools())\n    tool(ProvideStringSubgraphResult)\n}\n</code></pre>"},{"location":"examples/Banking/#agent-strategy","title":"Agent Strategy","text":"<p>Now we'll create a strategy that orchestrates multiple nodes:</p> <pre><code>import ai.koog.agents.core.dsl.builder.forwardTo\nimport ai.koog.agents.core.dsl.builder.strategy\nimport ai.koog.agents.core.dsl.extension.*\nimport ai.koog.agents.ext.agent.subgraphWithTask\nimport ai.koog.prompt.structure.StructureFixingParser\n\nval strategy = strategy&lt;String, String&gt;(\"banking assistant\") {\n\n    // Subgraph for classifying user requests\n    val classifyRequest by subgraph&lt;String, ClassifiedBankRequest&gt;(\n        tools = listOf(AskUser)\n    ) {\n        // Use structured output to ensure proper classification\n        val requestClassification by nodeLLMRequestStructured&lt;ClassifiedBankRequest&gt;(\n            examples = listOf(\n                ClassifiedBankRequest(\n                    requestType = RequestType.Transfer,\n                    userRequest = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n                ),\n                ClassifiedBankRequest(\n                    requestType = RequestType.Analytics,\n                    userRequest = \"Provide transaction overview for the last month\"\n                )\n            ),\n            fixingParser = StructureFixingParser(\n                fixingModel = OpenAIModels.CostOptimized.GPT4oMini,\n                retries = 2,\n            )\n        )\n\n        val callLLM by nodeLLMRequest()\n        val callAskUserTool by nodeExecuteTool()\n\n        // Define the flow\n        edge(nodeStart forwardTo requestClassification)\n\n        edge(\n            requestClassification forwardTo nodeFinish\n                onCondition { it.isSuccess }\n                transformed { it.getOrThrow().structure }\n        )\n\n        edge(\n            requestClassification forwardTo callLLM\n                onCondition { it.isFailure }\n                transformed { \"Failed to understand the user's intent\" }\n        )\n\n        edge(callLLM forwardTo callAskUserTool onToolCall { true })\n\n        edge(\n            callLLM forwardTo callLLM onAssistantMessage { true }\n                transformed { \"Please call `${AskUser.name}` tool instead of chatting\" }\n        )\n\n        edge(callAskUserTool forwardTo requestClassification\n            transformed { it.result.toString() })\n    }\n\n    // Subgraph for handling money transfers\n    val transferMoney by subgraphWithTask&lt;ClassifiedBankRequest&gt;(\n        tools = MoneyTransferTools().asTools() + AskUser,\n        llmModel = OpenAIModels.Chat.GPT4o  // Use more capable model for transfers\n    ) { request -&gt;\n        \"\"\"\n        $bankingAssistantSystemPrompt\n        Specifically, you need to help with the following request:\n        ${request.userRequest}\n        \"\"\".trimIndent()\n    }\n\n    // Subgraph for transaction analysis\n    val transactionAnalysis by subgraphWithTask&lt;ClassifiedBankRequest&gt;(\n        tools = TransactionAnalysisTools().asTools() + AskUser,\n    ) { request -&gt;\n        \"\"\"\n        $bankingAssistantSystemPrompt\n        $transactionAnalysisPrompt\n        Specifically, you need to help with the following request:\n        ${request.userRequest}\n        \"\"\".trimIndent()\n    }\n\n    // Connect the subgraphs\n    edge(nodeStart forwardTo classifyRequest)\n\n    edge(classifyRequest forwardTo transferMoney\n        onCondition { it.requestType == RequestType.Transfer })\n\n    edge(classifyRequest forwardTo transactionAnalysis\n        onCondition { it.requestType == RequestType.Analytics })\n\n    // Route results to finish node\n    edge(transferMoney forwardTo nodeFinish transformed { it.result })\n    edge(transactionAnalysis forwardTo nodeFinish transformed { it.result })\n}\n</code></pre> <pre><code>import ai.koog.agents.core.agent.config.AIAgentConfig\nimport ai.koog.prompt.dsl.prompt\n\nval agentConfig = AIAgentConfig(\n    prompt = prompt(id = \"banking assistant\") {\n        system(\"$bankingAssistantSystemPrompt\\n$transactionAnalysisPrompt\")\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 50  // Allow for complex multi-step operations\n)\n\nval agent = AIAgent&lt;String, String&gt;(\n    promptExecutor = openAIExecutor,\n    strategy = strategy,\n    agentConfig = agentConfig,\n    toolRegistry = toolRegistry,\n)\n</code></pre>"},{"location":"examples/Banking/#run-graph-agent","title":"Run graph agent","text":"<pre><code>println(\"Banking Assistant started\")\nval testMessage = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n\n// Test various scenarios:\n// Transfer requests:\n//   - \"Send 50 euros to Alice for the concert tickets\"\n//   - \"Transfer 100 to Bob for groceries\"\n//   - \"What's my current balance?\"\n//\n// Analytics requests:\n//   - \"How much have I spent on restaurants this month?\"\n//   - \"What's my maximum check at a restaurant this month?\"\n//   - \"How much did I spend on groceries in the first week of May?\"\n//   - \"What's my total spending on entertainment in May?\"\n\nrunBlocking {\n    val result = agent.run(testMessage)\n    \"Result: $result\"\n}\n</code></pre> <pre><code>Banking Assistant started\nI found multiple contacts with the name Daniel. Please choose the correct one:\n1. Daniel Anderson (+46 70 123 45 67)\n2. Daniel Garcia (+34 612 345 678)\nPlease specify the number of the correct recipient.\nPlease confirm if you would like to proceed with sending \u20ac25 to Daniel Garcia for \"dinner at the restaurant.\"\n\n\n\n\n\nResult: Task completed successfully.\n</code></pre>"},{"location":"examples/Banking/#agent-composition-using-agents-as-tools","title":"Agent Composition \u2014 Using Agents as Tools","text":"<p>Koog allows you to use agents as tools within other agents, enabling powerful composition patterns.</p> <pre><code>import ai.koog.agents.core.agent.asTool\nimport ai.koog.agents.core.tools.ToolParameterDescriptor\nimport ai.koog.agents.core.tools.ToolParameterType\n\nval classifierAgent = AIAgent(\n    executor = openAIExecutor,\n    llmModel = OpenAIModels.Reasoning.GPT4oMini,\n    toolRegistry = ToolRegistry {\n        tool(AskUser)\n\n        // Convert agents into tools\n        tool(\n            transferAgent.asTool(\n                agentName = \"transferMoney\",\n                agentDescription = \"Transfers money and handles all related operations\",\n                inputDescriptor = ToolParameterDescriptor(\n                    name = \"request\",\n                    description = \"Transfer request from the user\",\n                    type = ToolParameterType.String\n                )\n            )\n        )\n\n        tool(\n            analysisAgent.asTool(\n                agentName = \"analyzeTransactions\",\n                agentDescription = \"Performs analytics on user transactions\",\n                inputDescriptor = ToolParameterDescriptor(\n                    name = \"request\",\n                    description = \"Transaction analytics request\",\n                    type = ToolParameterType.String\n                )\n            )\n        )\n    },\n    systemPrompt = \"$bankingAssistantSystemPrompt\\n$transactionAnalysisPrompt\"\n)\n</code></pre>"},{"location":"examples/Banking/#run-composed-agent","title":"Run composed agent","text":"<pre><code>println(\"Banking Assistant started\")\nval composedMessage = \"Send 25 euros to Daniel for dinner at the restaurant.\"\n\nrunBlocking {\n    val result = classifierAgent.run(composedMessage)\n    \"Result: $result\"\n}\n</code></pre> <pre><code>Banking Assistant started\nThere are two contacts named Daniel. Please confirm which one you would like to send money to:\n1. Daniel Anderson (+46 70 123 45 67)\n2. Daniel Garcia (+34 612 345 678)\nPlease confirm the transfer of \u20ac25.00 to Daniel Anderson (+46 70 123 45 67) for \"Dinner at the restaurant\".\n\n\n\n\n\nResult: Can't perform the task.\n</code></pre>"},{"location":"examples/Banking/#summary","title":"Summary","text":"<p>In this tutorial, you've learned how to:</p> <ol> <li>Create LLM-powered tools with clear descriptions that help the AI understand when and how to use them</li> <li>Build single-purpose agents that combine LLMs with tools to accomplish specific tasks</li> <li>Implement graph agent using strategies and subgraphs for complex workflows</li> <li>Compose agents by using them as tools within other agents</li> <li>Handle user interactions including confirmations and disambiguation</li> </ol>"},{"location":"examples/Banking/#best-practices","title":"Best Practices","text":"<ol> <li>Clear tool descriptions: Write detailed LLMDescription annotations to help the AI understand tool usage</li> <li>Idiomatic Kotlin: Use Kotlin features like data classes, extension functions, and scope functions</li> <li>Error handling: Always validate inputs and provide meaningful error messages</li> <li>User experience: Include confirmation steps for critical operations like money transfers</li> <li>Modularity: Separate concerns into different tools and agents for better maintainability</li> </ol>"},{"location":"examples/BedrockAgent/","title":"Building AI Agents with AWS Bedrock and Koog Framework","text":"<p> Open on GitHub  Download .ipynb</p> <p>Welcome to this comprehensive guide on creating intelligent AI agents using the Koog framework with AWS Bedrock integration. In this notebook, we'll walk through building a functional agent that can control a simple switch device through natural language commands.</p>"},{"location":"examples/BedrockAgent/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to define custom tools for AI agents using Kotlin annotations</li> <li>Setting up AWS Bedrock integration for LLM-powered agents</li> <li>Creating tool registries and connecting them to agents</li> <li>Building interactive agents that can understand and execute commands</li> </ul>"},{"location":"examples/BedrockAgent/#prerequisites","title":"Prerequisites","text":"<ul> <li>AWS Bedrock access with appropriate permissions</li> <li>AWS credentials configured (access key and secret key)</li> <li>Basic understanding of Kotlin coroutines</li> </ul> <p>Let's dive into building our first Bedrock-powered AI agent!</p> <pre><code>%useLatestDescriptors\n// %use koog\n</code></pre> <pre><code>import ai.koog.agents.core.tools.annotations.LLMDescription\nimport ai.koog.agents.core.tools.annotations.Tool\nimport ai.koog.agents.core.tools.reflect.ToolSet\n\n// Simple state-holding device that our agent will control\nclass Switch {\n    private var state: Boolean = false\n\n    fun switch(on: Boolean) {\n        state = on\n    }\n\n    fun isOn(): Boolean {\n        return state\n    }\n}\n\n/**\n * ToolSet implementation that exposes switch operations to the AI agent.\n *\n * Key concepts:\n * - @Tool annotation marks methods as callable by the agent\n * - @LLMDescription provides natural language descriptions for the LLM\n * - ToolSet interface allows grouping related tools together\n */\nclass SwitchTools(val switch: Switch) : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Switches the state of the switch to on or off\")\n    fun switchState(state: Boolean): String {\n        switch.switch(state)\n        return \"Switch turned ${if (state) \"on\" else \"off\"} successfully\"\n    }\n\n    @Tool\n    @LLMDescription(\"Returns the current state of the switch (on or off)\")\n    fun getCurrentState(): String {\n        return \"Switch is currently ${if (switch.isOn()) \"on\" else \"off\"}\"\n    }\n}\n</code></pre> <pre><code>import ai.koog.agents.core.tools.ToolRegistry\nimport ai.koog.agents.core.tools.reflect.asTools\n\n// Create our switch instance\nval switch = Switch()\n\n// Build the tool registry with our switch tools\nval toolRegistry = ToolRegistry {\n    // Convert our ToolSet to individual tools and register them\n    tools(SwitchTools(switch).asTools())\n}\n\nprintln(\"\u2705 Tool registry created with ${toolRegistry.tools.size} tools:\")\ntoolRegistry.tools.forEach { tool -&gt;\n    println(\"  - ${tool.name}\")\n}\n</code></pre> <pre><code>\u2705 Tool registry created with 2 tools:\n  - getCurrentState\n  - switchState\n</code></pre> <pre><code>import ai.koog.prompt.executor.clients.bedrock.BedrockClientSettings\nimport ai.koog.prompt.executor.clients.bedrock.BedrockRegions\n\nval region = BedrockRegions.US_WEST_2.regionCode\nval maxRetries = 3\n\n// Configure Bedrock client settings\nval bedrockSettings = BedrockClientSettings(\n    region = region, // Choose your preferred AWS region\n    maxRetries = maxRetries // Number of retry attempts for failed requests\n)\n\nprintln(\"\ud83c\udf10 Bedrock configured for region: $region\")\nprintln(\"\ud83d\udd04 Max retries set to: $maxRetries\")\n</code></pre> <pre><code>\ud83c\udf10 Bedrock configured for region: us-west-2\n\ud83d\udd04 Max retries set to: 3\n</code></pre> <pre><code>import ai.koog.prompt.executor.llms.all.simpleBedrockExecutor\n\n// Create the Bedrock LLM executor with credentials from environment\nval executor = simpleBedrockExecutor(\n    awsAccessKeyId = System.getenv(\"AWS_BEDROCK_ACCESS_KEY\")\n        ?: throw IllegalStateException(\"AWS_BEDROCK_ACCESS_KEY environment variable not set\"),\n    awsSecretAccessKey = System.getenv(\"AWS_BEDROCK_SECRET_ACCESS_KEY\")\n        ?: throw IllegalStateException(\"AWS_BEDROCK_SECRET_ACCESS_KEY environment variable not set\"),\n    settings = bedrockSettings\n)\n\nprintln(\"\ud83d\udd10 Bedrock executor initialized successfully\")\nprintln(\"\ud83d\udca1 Pro tip: Set AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables\")\n</code></pre> <pre><code>\ud83d\udd10 Bedrock executor initialized successfully\n\ud83d\udca1 Pro tip: Set AWS_BEDROCK_ACCESS_KEY and AWS_BEDROCK_SECRET_ACCESS_KEY environment variables\n</code></pre> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.prompt.executor.clients.bedrock.BedrockModels\n\nval agent = AIAgent(\n    executor = executor,\n    llmModel = BedrockModels.AnthropicClaude35SonnetV2, // State-of-the-art reasoning model\n    systemPrompt = \"\"\"\n        You are a helpful assistant that controls a switch device.\n\n        You can:\n        - Turn the switch on or off when requested\n        - Check the current state of the switch\n        - Explain what you're doing\n\n        Always be clear about the switch's current state and confirm actions taken.\n    \"\"\".trimIndent(),\n    temperature = 0.1, // Low temperature for consistent, focused responses\n    toolRegistry = toolRegistry\n)\n\nprintln(\"\ud83e\udd16 AI Agent created successfully!\")\nprintln(\"\ud83d\udccb System prompt configured\")\nprintln(\"\ud83d\udee0\ufe0f  Tools available: ${toolRegistry.tools.size}\")\nprintln(\"\ud83c\udfaf Model: ${BedrockModels.AnthropicClaude35SonnetV2}\")\nprintln(\"\ud83c\udf21\ufe0f  Temperature: 0.1 (focused responses)\")\n</code></pre> <pre><code>\ud83e\udd16 AI Agent created successfully!\n\ud83d\udccb System prompt configured\n\ud83d\udee0\ufe0f  Tools available: 2\n\ud83c\udfaf Model: LLModel(provider=Bedrock, id=us.anthropic.claude-3-5-sonnet-20241022-v2:0, capabilities=[Temperature, Tools, ToolChoice, Image, Document, Completion], contextLength=200000, maxOutputTokens=8192)\n\ud83c\udf21\ufe0f  Temperature: 0.1 (focused responses)\n</code></pre> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"\ud83c\udf89 Bedrock Agent with Switch Tools - Ready to Go!\")\nprintln(\"\ud83d\udcac You can ask me to:\")\nprintln(\"   \u2022 Turn the switch on/off\")\nprintln(\"   \u2022 Check the current switch state\")\nprintln(\"   \u2022 Ask questions about the switch\")\nprintln()\nprintln(\"\ud83d\udca1 Example: 'Please turn on the switch' or 'What's the current state?'\")\nprintln(\"\ud83d\udcdd Type your request:\")\n\nval input = readln()\nprintln(\"\\n\ud83e\udd16 Processing your request...\")\n\nrunBlocking {\n    val response = agent.run(input)\n    println(\"\\n\u2728 Agent response:\")\n    println(response)\n}\n</code></pre> <pre><code>\ud83c\udf89 Bedrock Agent with Switch Tools - Ready to Go!\n\ud83d\udcac You can ask me to:\n   \u2022 Turn the switch on/off\n   \u2022 Check the current switch state\n   \u2022 Ask questions about the switch\n\n\ud83d\udca1 Example: 'Please turn on the switch' or 'What's the current state?'\n\ud83d\udcdd Type your request:\n\n\n\nThe execution was interrupted\n</code></pre>"},{"location":"examples/BedrockAgent/#what-just-happened","title":"What Just Happened? \ud83c\udfaf","text":"<p>When you run the agent, here's the magic that occurs behind the scenes:</p> <ol> <li>Natural Language Processing: Your input is sent to Claude 3.5 Sonnet via Bedrock</li> <li>Intent Recognition: The model understands what you want to do with the switch</li> <li>Tool Selection: Based on your request, the agent decides which tools to call</li> <li>Action Execution: The appropriate tool methods are invoked on your switch object</li> <li>Response Generation: The agent formulates a natural language response about what happened</li> </ol> <p>This demonstrates the core power of the Koog framework - seamless integration between natural language understanding and programmatic actions.</p>"},{"location":"examples/BedrockAgent/#next-steps-extensions","title":"Next Steps &amp; Extensions","text":"<p>Ready to take this further? Here are some ideas to explore:</p>"},{"location":"examples/BedrockAgent/#enhanced-tools","title":"\ud83d\udd27 Enhanced Tools","text":"<pre><code>@Tool\n@LLMDescription(\"Sets a timer to automatically turn off the switch after specified seconds\")\nfun setAutoOffTimer(seconds: Int): String\n\n@Tool\n@LLMDescription(\"Gets the switch usage statistics and history\")\nfun getUsageStats(): String\n</code></pre>"},{"location":"examples/BedrockAgent/#multiple-devices","title":"\ud83c\udf10 Multiple Devices","text":"<pre><code>class HomeAutomationTools : ToolSet {\n    @Tool fun controlLight(room: String, on: Boolean): String\n    @Tool fun setThermostat(temperature: Double): String\n    @Tool fun lockDoor(doorName: String): String\n}\n</code></pre>"},{"location":"examples/BedrockAgent/#memory-context","title":"\ud83e\udde0 Memory &amp; Context","text":"<pre><code>val agent = AIAgent(\n    executor = executor,\n    // ... other config\n    features = listOf(\n        MemoryFeature(), // Remember past interactions\n        LoggingFeature()  // Track all actions\n    )\n)\n</code></pre>"},{"location":"examples/BedrockAgent/#advanced-workflows","title":"\ud83d\udd04 Advanced Workflows","text":"<pre><code>// Multi-step workflows with conditional logic\n@Tool\n@LLMDescription(\"Executes evening routine: dims lights, locks doors, sets thermostat\")\nfun eveningRoutine(): String\n</code></pre>"},{"location":"examples/BedrockAgent/#key-takeaways","title":"Key Takeaways","text":"<p>\u2705 Tools are functions: Any Kotlin function can become an agent capability \u2705 Annotations drive behavior: @Tool and @LLMDescription make functions discoverable \u2705 ToolSets organize capabilities: Group related tools together logically \u2705 Registries are toolboxes: ToolRegistry contains all available agent capabilities \u2705 Agents orchestrate everything: AIAgent brings LLM intelligence + tools together</p> <p>The Koog framework makes it incredibly straightforward to build sophisticated AI agents that can understand natural language and take real-world actions. Start simple, then expand your agent's capabilities by adding more tools and features as needed.</p> <p>Happy agent building! \ud83d\ude80</p>"},{"location":"examples/BedrockAgent/#testing-the-agent","title":"Testing the Agent","text":"<p>Time to see our agent in action! The agent can now understand natural language requests and use the tools we've provided to control the switch.</p> <p>Try these commands: - \"Turn on the switch\" - \"What's the current state?\" - \"Switch it off please\" - \"Is the switch on or off?\"</p>"},{"location":"examples/Calculator/","title":"Building a Tool-Calling Calculator Agent with Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this mini-tutorial we\u2019ll build a calculator agent powered by Koog tool-calling. You\u2019ll learn how to: - Design small, pure tools for arithmetic - Orchestrate parallel tool calls with Koog\u2019s multiple-call strategy - Add lightweight event logging for transparency - Run with OpenAI (and optionally Ollama)</p> <p>We\u2019ll keep the API tidy and idiomatic Kotlin, returning predictable results and handling edge cases (like division by zero) gracefully.</p>"},{"location":"examples/Calculator/#setup","title":"Setup","text":"<p>We assume you\u2019re in a Kotlin Notebook environment with Koog available. Provide an LLM executor</p> <pre><code>%useLatestDescriptors\n%use koog\n\n\nval OPENAI_API_KEY = System.getenv(\"OPENAI_API_KEY\")\n    ?: error(\"Please set the OPENAI_API_KEY environment variable\")\n\nval executor = simpleOpenAIExecutor(OPENAI_API_KEY)\n</code></pre>"},{"location":"examples/Calculator/#calculator-tools","title":"Calculator Tools","text":"<p>Tools are small, pure functions with clear contracts. We\u2019ll use <code>Double</code> for better precision and format outputs consistently.</p> <pre><code>import ai.koog.agents.core.tools.annotations.Tool\n\n// Format helper: integers render cleanly, decimals keep reasonable precision.\nprivate fun Double.pretty(): String =\n    if (abs(this % 1.0) &lt; 1e-9) this.toLong().toString() else \"%.10g\".format(this)\n\n@LLMDescription(\"Tools for basic calculator operations\")\nclass CalculatorTools : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Adds two numbers and returns the sum as text.\")\n    fun plus(\n        @LLMDescription(\"First addend.\") a: Double,\n        @LLMDescription(\"Second addend.\") b: Double\n    ): String = (a + b).pretty()\n\n    @Tool\n    @LLMDescription(\"Subtracts the second number from the first and returns the difference as text.\")\n    fun minus(\n        @LLMDescription(\"Minuend.\") a: Double,\n        @LLMDescription(\"Subtrahend.\") b: Double\n    ): String = (a - b).pretty()\n\n    @Tool\n    @LLMDescription(\"Multiplies two numbers and returns the product as text.\")\n    fun multiply(\n        @LLMDescription(\"First factor.\") a: Double,\n        @LLMDescription(\"Second factor.\") b: Double\n    ): String = (a * b).pretty()\n\n    @Tool\n    @LLMDescription(\"Divides the first number by the second and returns the quotient as text. Returns an error message on division by zero.\")\n    fun divide(\n        @LLMDescription(\"Dividend.\") a: Double,\n        @LLMDescription(\"Divisor (must not be zero).\") b: Double\n    ): String = if (abs(b) &lt; 1e-12) {\n        \"ERROR: Division by zero\"\n    } else {\n        (a / b).pretty()\n    }\n}\n</code></pre>"},{"location":"examples/Calculator/#tool-registry","title":"Tool Registry","text":"<p>Expose our tools (plus two built-ins for interaction/logging).</p> <pre><code>val toolRegistry = ToolRegistry {\n    tool(AskUser)   // enables explicit user clarification when needed\n    tool(SayToUser) // allows the agent to present the final message to the user\n    tools(CalculatorTools())\n}\n</code></pre>"},{"location":"examples/Calculator/#strategy-multiple-tool-calls-with-optional-compression","title":"Strategy: Multiple Tool Calls (with Optional Compression)","text":"<p>This strategy lets the LLM propose multiple tool calls at once (e.g., <code>plus</code>, <code>minus</code>, <code>multiply</code>, <code>divide</code>) and then sends the results back. If the token usage grows too large, we compress the history of tool results before continuing.</p> <pre><code>import ai.koog.agents.core.environment.ReceivedToolResult\n\nobject CalculatorStrategy {\n    private const val MAX_TOKENS_THRESHOLD = 1000\n\n    val strategy = strategy&lt;String, String&gt;(\"test\") {\n        val callLLM by nodeLLMRequestMultiple()\n        val executeTools by nodeExecuteMultipleTools(parallelTools = true)\n        val sendToolResults by nodeLLMSendMultipleToolResults()\n        val compressHistory by nodeLLMCompressHistory&lt;List&lt;ReceivedToolResult&gt;&gt;()\n\n        edge(nodeStart forwardTo callLLM)\n\n        // If the assistant produced a final answer, finish.\n        edge((callLLM forwardTo nodeFinish) transformed { it.first() } onAssistantMessage { true })\n\n        // Otherwise, run the tools LLM requested (possibly several in parallel).\n        edge((callLLM forwardTo executeTools) onMultipleToolCalls { true })\n\n        // If we\u2019re getting large, compress past tool results before continuing.\n        edge(\n            (executeTools forwardTo compressHistory)\n                onCondition { llm.readSession { prompt.latestTokenUsage &gt; MAX_TOKENS_THRESHOLD } }\n        )\n        edge(compressHistory forwardTo sendToolResults)\n\n        // Normal path: send tool results back to the LLM.\n        edge(\n            (executeTools forwardTo sendToolResults)\n                onCondition { llm.readSession { prompt.latestTokenUsage &lt;= MAX_TOKENS_THRESHOLD } }\n        )\n\n        // LLM might request more tools after seeing results.\n        edge((sendToolResults forwardTo executeTools) onMultipleToolCalls { true })\n\n        // Or it can produce the final answer.\n        edge((sendToolResults forwardTo nodeFinish) transformed { it.first() } onAssistantMessage { true })\n    }\n}\n</code></pre>"},{"location":"examples/Calculator/#agent-configuration","title":"Agent Configuration","text":"<p>A minimal, tool-forward prompt works well. Keep temperature low for deterministic math.</p> <pre><code>val agentConfig = AIAgentConfig(\n    prompt = prompt(\"calculator\") {\n        system(\"You are a calculator. Always use the provided tools for arithmetic.\")\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 50\n)\n</code></pre> <pre><code>import ai.koog.agents.features.eventHandler.feature.handleEvents\n\nval agent = AIAgent(\n    promptExecutor = executor,\n    strategy = CalculatorStrategy.strategy,\n    agentConfig = agentConfig,\n    toolRegistry = toolRegistry\n) {\n    handleEvents {\n        onToolCall { e -&gt;\n            println(\"Tool called: ${e.tool.name}, args=${e.toolArgs}\")\n        }\n        onAgentRunError { e -&gt;\n            println(\"Agent error: ${e.throwable.message}\")\n        }\n        onAgentFinished { e -&gt;\n            println(\"Final result: ${e.result}\")\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Calculator/#try-it","title":"Try It","text":"<p>The agent should decompose the expression into parallel tool calls and return a neatly formatted result.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    agent.run(\"(10 + 20) * (5 + 5) / (2 - 11)\")\n}\n// Expected final value \u2248 -33.333...\n</code></pre> <pre><code>Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=10.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=20.0})\nTool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0})\nTool called: minus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=2.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=11.0})\nTool called: multiply, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=10.0})\nTool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=1.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})\nTool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=300.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})\nFinal result: The result of the expression \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-33.33\\).\n\n\n\n\n\nThe result of the expression \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-33.33\\).\n</code></pre>"},{"location":"examples/Calculator/#try-forcing-parallel-calls","title":"Try Forcing Parallel Calls","text":"<p>Ask the model to call all needed tools at once. You should still see a correct plan and stable execution.</p> <pre><code>runBlocking {\n    agent.run(\"Use tools to calculate (10 + 20) * (5 + 5) / (2 - 11). Please call all the tools at once.\")\n}\n</code></pre> <pre><code>Tool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=10.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=20.0})\nTool called: plus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.plus(kotlin.Double, kotlin.Double): kotlin.String=5.0})\nTool called: minus, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=2.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.minus(kotlin.Double, kotlin.Double): kotlin.String=11.0})\nTool called: multiply, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.multiply(kotlin.Double, kotlin.Double): kotlin.String=10.0})\nTool called: divide, args=VarArgs(args={parameter #1 a of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=30.0, parameter #2 b of fun Line_4_jupyter.CalculatorTools.divide(kotlin.Double, kotlin.Double): kotlin.String=-9.0})\nFinal result: The result of \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-3.33\\).\n\n\n\n\n\nThe result of \\((10 + 20) * (5 + 5) / (2 - 11)\\) is approximately \\(-3.33\\).\n</code></pre>"},{"location":"examples/Calculator/#running-with-ollama","title":"Running with Ollama","text":"<p>Swap the executor and model if you prefer local inference.</p> <pre><code>val ollamaExecutor: PromptExecutor = simpleOllamaAIExecutor()\n\nval ollamaAgentConfig = AIAgentConfig(\n    prompt = prompt(\"calculator\", LLMParams(temperature = 0.0)) {\n        system(\"You are a calculator. Always use the provided tools for arithmetic.\")\n    },\n    model = OllamaModels.Meta.LLAMA_3_2,\n    maxAgentIterations = 50\n)\n\n\nval ollamaAgent = AIAgent(\n    promptExecutor = ollamaExecutor,\n    strategy = CalculatorStrategy.strategy,\n    agentConfig = ollamaAgentConfig,\n    toolRegistry = toolRegistry\n)\n\nrunBlocking {\n    ollamaAgent.run(\"(10 + 20) * (5 + 5) / (2 - 11)\")\n}\n</code></pre> <pre><code>Agent says: The result of the expression (10 + 20) * (5 + 5) / (2 - 11) is approximately -33.33.\n\n\n\n\n\nIf you have any more questions or need further assistance, feel free to ask!\n</code></pre>"},{"location":"examples/Chess/","title":"Building an AI Chess Player with Koog Framework","text":"<p> Open on GitHub  Download .ipynb</p> <p>This tutorial demonstrates how to build an intelligent chess-playing agent using the Koog framework. We'll explore key concepts including tool integration, agent strategies, memory optimization, and interactive AI decision-making.</p>"},{"location":"examples/Chess/#what-youll-learn","title":"What You'll Learn","text":"<ul> <li>How to model domain-specific data structures for complex games</li> <li>Creating custom tools that agents can use to interact with the environment</li> <li>Implementing efficient agent strategies with memory management</li> <li>Building interactive AI systems with choice selection capabilities</li> <li>Optimizing agent performance for turn-based games</li> </ul>"},{"location":"examples/Chess/#setup","title":"Setup","text":"<p>First, let's import the Koog framework and set up our development environment:</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/Chess/#modeling-the-chess-domain","title":"Modeling the Chess Domain","text":"<p>Creating a robust domain model is essential for any game AI. In chess, we need to represent players, pieces, and their relationships. Let's start by defining our core data structures:</p>"},{"location":"examples/Chess/#core-enums-and-types","title":"Core Enums and Types","text":"<pre><code>enum class Player {\n    White, Black, None;\n\n    fun opponent(): Player = when (this) {\n        White -&gt; Black\n        Black -&gt; White\n        None -&gt; throw IllegalArgumentException(\"No opponent for None player\")\n    }\n}\n\nenum class PieceType(val id: Char) {\n    King('K'), Queen('Q'), Rook('R'),\n    Bishop('B'), Knight('N'), Pawn('P'), None('*');\n\n    companion object {\n        fun fromId(id: String): PieceType {\n            require(id.length == 1) { \"Invalid piece id: $id\" }\n\n            return entries.first { it.id == id.single() }\n        }\n    }\n}\n\nenum class Side {\n    King, Queen\n}\n</code></pre> <p>The <code>Player</code> enum represents the two sides in chess, with an <code>opponent()</code> method for easy switching between players. The <code>PieceType</code> enum maps each chess piece to its standard notation character, enabling easy parsing of chess moves.</p> <p>The <code>Side</code> enum helps distinguish between kingside and queenside castling moves.</p>"},{"location":"examples/Chess/#piece-and-position-modeling","title":"Piece and Position Modeling","text":"<pre><code>data class Piece(val pieceType: PieceType, val player: Player) {\n    init {\n        require((pieceType == PieceType.None) == (player == Player.None)) {\n            \"Invalid piece: $pieceType $player\"\n        }\n    }\n\n    fun toChar(): Char = when (player) {\n        Player.White -&gt; pieceType.id.uppercaseChar()\n        Player.Black -&gt; pieceType.id.lowercaseChar()\n        Player.None -&gt; pieceType.id\n    }\n\n    fun isNone(): Boolean = pieceType == PieceType.None\n\n    companion object {\n        val None = Piece(PieceType.None, Player.None)\n    }\n}\n\ndata class Position(val row: Int, val col: Char) {\n    init {\n        require(row in 1..8 &amp;&amp; col in 'a'..'h') { \"Invalid position: $col$row\" }\n    }\n\n    constructor(position: String) : this(\n        position[1].digitToIntOrNull() ?: throw IllegalArgumentException(\"Incorrect position: $position\"),\n        position[0],\n    ) {\n        require(position.length == 2) { \"Invalid position: $position\" }\n    }\n}\n\nclass ChessBoard {\n    private val backRow = listOf(\n        PieceType.Rook, PieceType.Knight, PieceType.Bishop,\n        PieceType.Queen, PieceType.King,\n        PieceType.Bishop, PieceType.Knight, PieceType.Rook\n    )\n\n    private val board: List&lt;MutableList&lt;Piece&gt;&gt; = listOf(\n        backRow.map { Piece(it, Player.Black) }.toMutableList(),\n        List(8) { Piece(PieceType.Pawn, Player.Black) }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece.None }.toMutableList(),\n        List(8) { Piece(PieceType.Pawn, Player.White) }.toMutableList(),\n        backRow.map { Piece(it, Player.White) }.toMutableList()\n    )\n\n    override fun toString(): String = board\n        .withIndex().joinToString(\"\\n\") { (index, row) -&gt;\n            \"${8 - index} ${row.map { it.toChar() }.joinToString(\" \")}\"\n        } + \"\\n  a b c d e f g h\"\n\n    fun getPiece(position: Position): Piece = board[8 - position.row][position.col - 'a']\n    fun setPiece(position: Position, piece: Piece) {\n        board[8 - position.row][position.col - 'a'] = piece\n    }\n}\n</code></pre> <p>The <code>Piece</code> data class combines a piece type with its owner, using uppercase letters for white pieces and lowercase for black pieces in the visual representation. The <code>Position</code> class encapsulates chess coordinates (e.g., \"e4\") with built-in validation.</p>"},{"location":"examples/Chess/#game-state-management","title":"Game State Management","text":""},{"location":"examples/Chess/#chessboard-implementation","title":"ChessBoard Implementation","text":"<p>The <code>ChessBoard</code> class manages the 8\u00d78 grid and piece positions. Key design decisions include:</p> <ul> <li>Internal Representation: Uses a list of mutable lists for efficient access and modification</li> <li>Visual Display: The <code>toString()</code> method provides a clear ASCII representation with rank numbers and file letters</li> <li>Position Mapping: Converts between chess notation (a1-h8) and internal array indices</li> </ul>"},{"location":"examples/Chess/#chessgame-logic","title":"ChessGame Logic","text":"<pre><code>/**\n * Simple chess game without checks for valid moves.\n * Stores a correct state of the board if the entered moves are valid\n */\nclass ChessGame {\n    private val board: ChessBoard = ChessBoard()\n    private var currentPlayer: Player = Player.White\n    val moveNotation: String = \"\"\"\n        0-0 - short castle\n        0-0-0 - long castle\n        &lt;piece&gt;-&lt;from&gt;-&lt;to&gt; - usual move. e.g. p-e2-e4\n        &lt;piece&gt;-&lt;from&gt;-&lt;to&gt;-&lt;promotion&gt; - promotion move. e.g. p-e7-e8-q.\n        Piece names:\n            p - pawn\n            n - knight\n            b - bishop\n            r - rook\n            q - queen\n            k - king\n    \"\"\".trimIndent()\n\n    fun move(move: String) {\n        when {\n            move == \"0-0\" -&gt; castleMove(Side.King)\n            move == \"0-0-0\" -&gt; castleMove(Side.Queen)\n            move.split(\"-\").size == 3 -&gt; {\n                val (_, from, to) = move.split(\"-\")\n                usualMove(Position(from), Position(to))\n            }\n\n            move.split(\"-\").size == 4 -&gt; {\n                val (piece, from, to, promotion) = move.split(\"-\")\n\n                require(PieceType.fromId(piece) == PieceType.Pawn) { \"Only pawn can be promoted\" }\n\n                usualMove(Position(from), Position(to))\n                board.setPiece(Position(to), Piece(PieceType.fromId(promotion), currentPlayer))\n            }\n\n            else -&gt; throw IllegalArgumentException(\"Invalid move: $move\")\n        }\n\n        updateCurrentPlayer()\n    }\n\n    fun getBoard(): String = board.toString()\n    fun currentPlayer(): String = currentPlayer.name.lowercase()\n\n    private fun updateCurrentPlayer() {\n        currentPlayer = currentPlayer.opponent()\n    }\n\n    private fun usualMove(from: Position, to: Position) {\n        if (board.getPiece(from).pieceType == PieceType.Pawn &amp;&amp; from.col != to.col &amp;&amp; board.getPiece(to).isNone()) {\n            // the move is en passant\n            board.setPiece(Position(from.row, to.col), Piece.None)\n        }\n\n        movePiece(from, to)\n    }\n\n    private fun castleMove(side: Side) {\n        val row = if (currentPlayer == Player.White) 1 else 8\n        val kingFrom = Position(row, 'e')\n        val (rookFrom, kingTo, rookTo) = if (side == Side.King) {\n            Triple(Position(row, 'h'), Position(row, 'g'), Position(row, 'f'))\n        } else {\n            Triple(Position(row, 'a'), Position(row, 'c'), Position(row, 'd'))\n        }\n\n        movePiece(kingFrom, kingTo)\n        movePiece(rookFrom, rookTo)\n    }\n\n    private fun movePiece(from: Position, to: Position) {\n        board.setPiece(to, board.getPiece(from))\n        board.setPiece(from, Piece.None)\n    }\n}\n</code></pre> <p>The <code>ChessGame</code> class orchestrates the game logic and maintains state. Notable features include:</p> <ul> <li>Move Notation Support: Accepts standard chess notation for regular moves, castling (0-0, 0-0-0), and pawn promotion</li> <li>Special Move Handling: Implements en passant capture and castling logic</li> <li>Turn Management: Automatically alternates between players after each move</li> <li>Validation: While it doesn't validate move legality (trusting the AI to make valid moves), it handles move parsing and state updates correctly</li> </ul> <p>The <code>moveNotation</code> string provides clear documentation for the AI agent on acceptable move formats.</p>"},{"location":"examples/Chess/#integrating-with-koog-framework","title":"Integrating with Koog Framework","text":""},{"location":"examples/Chess/#creating-custom-tools","title":"Creating Custom Tools","text":"<pre><code>import kotlinx.serialization.Serializable\n\nclass Move(val game: ChessGame) : SimpleTool&lt;Move.Args&gt;() {\n    @Serializable\n    data class Args(val notation: String) : ToolArgs\n\n    override val argsSerializer = Args.serializer()\n\n    override val descriptor = ToolDescriptor(\n        name = \"move\",\n        description = \"Moves a piece according to the notation:\\n${game.moveNotation}\",\n        requiredParameters = listOf(\n            ToolParameterDescriptor(\n                name = \"notation\",\n                description = \"The notation of the piece to move\",\n                type = ToolParameterType.String,\n            )\n        )\n    )\n\n    override suspend fun doExecute(args: Args): String {\n        game.move(args.notation)\n        println(game.getBoard())\n        println(\"-----------------\")\n        return \"Current state of the game:\\n${game.getBoard()}\\n${game.currentPlayer()} to move! Make the move!\"\n    }\n}\n</code></pre> <p>The <code>Move</code> tool demonstrates the Koog framework's tool integration pattern:</p> <ol> <li>Extends SimpleTool: Inherits the basic tool functionality with type-safe argument handling</li> <li>Serializable Arguments: Uses Kotlin serialization to define the tool's input parameters</li> <li>Rich Documentation: The <code>ToolDescriptor</code> provides the LLM with detailed information about the tool's purpose and parameters</li> <li>Execution Logic: The <code>doExecute</code> method handles the actual move execution and provides formatted feedback</li> </ol> <p>Key design aspects: - Context Injection: The tool receives the <code>ChessGame</code> instance, allowing it to modify game state - Feedback Loop: Returns the current board state and prompts the next player, maintaining conversational flow - Error Handling: Relies on the game class for move validation and error reporting</p>"},{"location":"examples/Chess/#agent-strategy-design","title":"Agent Strategy Design","text":""},{"location":"examples/Chess/#memory-optimization-technique","title":"Memory Optimization Technique","text":"<pre><code>import ai.koog.agents.core.environment.ReceivedToolResult\n\n/**\n * Chess position is (almost) completely defined by the board state,\n * So we can trim the history of the LLM to only contain the system prompt and the last move.\n */\ninline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.nodeTrimHistory(\n    name: String? = null\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { result -&gt;\n    llm.writeSession {\n        rewritePrompt { prompt -&gt;\n            val messages = prompt.messages\n\n            prompt.copy(messages = listOf(messages.first(), messages.last()))\n        }\n    }\n\n    result\n}\n\nval strategy = strategy&lt;String, String&gt;(\"chess_strategy\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendToolResult(\"nodeSendToolResult\")\n    val nodeTrimHistory by nodeTrimHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeTrimHistory)\n    edge(nodeTrimHistory forwardTo nodeSendToolResult)\n    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n}\n</code></pre> <p>The <code>nodeTrimHistory</code> function implements a crucial optimization for chess games. Since chess positions are largely determined by the current board state rather than the full move history, we can significantly reduce token usage by keeping only:</p> <ol> <li>System Prompt: Contains the agent's core instructions and behavior guidelines</li> <li>Latest Message: The most recent board state and game context</li> </ol> <p>This approach: - Reduces Token Consumption: Prevents exponential growth of conversation history - Maintains Context: Preserves essential game state information - Improves Performance: Faster processing with shorter prompts - Enables Long Games: Allows for extended gameplay without hitting token limits</p> <p>The chess strategy demonstrates Koog's graph-based agent architecture:</p> <p>Node Types: - <code>nodeCallLLM</code>: Processes input and generates responses/tool calls - <code>nodeExecuteTool</code>: Executes the Move tool with the provided parameters - <code>nodeTrimHistory</code>: Optimizes conversation memory as described above - <code>nodeSendToolResult</code>: Sends tool execution results back to the LLM</p> <p>Control Flow: - Linear Path: Start \u2192 LLM Request \u2192 Tool Execution \u2192 History Trim \u2192 Send Result - Decision Points: LLM responses can either finish the conversation or trigger another tool call - Memory Management: History trimming occurs after each tool execution</p> <p>This strategy ensures efficient, stateful gameplay while maintaining conversational coherence.</p>"},{"location":"examples/Chess/#setting-up-the-ai-agent","title":"Setting up the AI Agent","text":"<pre><code>val baseExecutor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\"))\n</code></pre> <p>This section initializes our OpenAI executor. The <code>simpleOpenAIExecutor</code> creates a connection to OpenAI's API using your API key from environment variables.</p> <p>Configuration Notes: - Store your OpenAI API key in the <code>OPENAI_API_KEY</code> environment variable - The executor handles authentication and API communication automatically - Different executor types are available for various LLM providers</p>"},{"location":"examples/Chess/#agent-assembly","title":"Agent Assembly","text":"<pre><code>val game = ChessGame()\nval toolRegistry = ToolRegistry { tools(listOf(Move(game))) }\n\n// Create a chat agent with a system prompt and the tool registry\nval agent = AIAgent(\n    executor = baseExecutor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Reasoning.O3Mini,\n    systemPrompt = \"\"\"\n            You are an agent who plays chess.\n            You should always propose a move in response to the \"Your move!\" message.\n\n            DO NOT HALLUCINATE!!!\n            DO NOT PLAY ILLEGAL MOVES!!!\n            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!\n        \"\"\".trimMargin(),\n    temperature = 0.0,\n    toolRegistry = toolRegistry,\n    maxIterations = 200,\n)\n</code></pre> <p>Here we assemble all components into a functional chess-playing agent:</p> <p>Key Configuration: - Model Choice: Using <code>OpenAIModels.Reasoning.O3Mini</code> for high-quality chess play - Temperature: Set to 0.0 for deterministic, strategic moves - System Prompt: Carefully crafted instructions emphasizing legal moves and proper behavior - Tool Registry: Provides the agent access to the Move tool - Max Iterations: Set to 200 to allow for complete games</p> <p>System Prompt Design: - Emphasizes move proposal responsibility - Prohibits hallucination and illegal moves - Restricts messaging to only resignations or checkmate declarations - Creates focused, game-oriented behavior</p>"},{"location":"examples/Chess/#running-the-basic-agent","title":"Running the Basic Agent","text":"<pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Chess Game started!\")\n\nval initialMessage = \"Starting position is ${game.getBoard()}. White to move!\"\n\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre> <pre><code>Chess Game started!\n8 r n b q k b n r\n7 p p p p p p p p\n6 * * * * * * * *\n5 * * * * * * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n8 r n b q k b * r\n7 p p p p * p p p\n6 * * * * * n * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n8 r n b q k b * r\n7 p p p p * p p p\n6 * * * * * n * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * N * * N * *\n2 P P P P * P P P\n1 R * B Q K B * R\n  a b c d e f g h\n-----------------\n\n\n\nThe execution was interrupted\n</code></pre> <p>This basic agent plays autonomously, making moves automatically. The game output shows the sequence of moves and board states as the AI plays against itself.</p>"},{"location":"examples/Chess/#advanced-feature-interactive-choice-selection","title":"Advanced Feature: Interactive Choice Selection","text":"<p>The next sections demonstrate a more sophisticated approach where users can participate in the AI's decision-making process by choosing from multiple AI-generated moves.</p>"},{"location":"examples/Chess/#custom-choice-selection-strategy","title":"Custom Choice Selection Strategy","text":"<pre><code>import ai.koog.agents.core.feature.choice.ChoiceSelectionStrategy\n\n/**\n * `AskUserChoiceStrategy` allows users to interactively select a choice from a list of options\n * presented by a language model. The strategy uses customizable methods to display the prompt\n * and choices and read user input to determine the selected choice.\n *\n * @property promptShowToUser A function that formats and displays a given `Prompt` to the user.\n * @property choiceShowToUser A function that formats and represents a given `LLMChoice` to the user.\n * @property print A function responsible for displaying messages to the user, e.g., for showing prompts or feedback.\n * @property read A function to capture user input.\n */\nclass AskUserChoiceSelectionStrategy(\n    private val promptShowToUser: (Prompt) -&gt; String = { \"Current prompt: $it\" },\n    private val choiceShowToUser: (LLMChoice) -&gt; String = { \"$it\" },\n    private val print: (String) -&gt; Unit = ::println,\n    private val read: () -&gt; String? = ::readlnOrNull\n) : ChoiceSelectionStrategy {\n    override suspend fun choose(prompt: Prompt, choices: List&lt;LLMChoice&gt;): LLMChoice {\n        print(promptShowToUser(prompt))\n\n        print(\"Available LLM choices\")\n\n        choices.withIndex().forEach { (index, choice) -&gt;\n            print(\"Choice number ${index + 1}: ${choiceShowToUser(choice)}\")\n        }\n\n        var choiceNumber = ask(choices.size)\n        while (choiceNumber == null) {\n            print(\"Invalid response.\")\n            choiceNumber = ask(choices.size)\n        }\n\n        return choices[choiceNumber - 1]\n    }\n\n    private fun ask(numChoices: Int): Int? {\n        print(\"Please choose a choice. Enter a number between 1 and $numChoices: \")\n\n        return read()?.toIntOrNull()?.takeIf { it in 1..numChoices }\n    }\n}\n</code></pre> <p>The <code>AskUserChoiceSelectionStrategy</code> implements Koog's <code>ChoiceSelectionStrategy</code> interface to enable human participation in AI decision-making:</p> <p>Key Features: - Customizable Display: Functions for formatting prompts and choices - Interactive Input: Uses standard input/output for user interaction - Validation: Ensures user input is within valid range - Flexible I/O: Configurable print and read functions for different environments</p> <p>Use Cases: - Human-AI collaboration in gameplay - AI decision transparency and explainability - Training and debugging scenarios - Educational demonstrations</p>"},{"location":"examples/Chess/#enhanced-strategy-with-choice-selection","title":"Enhanced Strategy with Choice Selection","text":"<pre><code>inline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.nodeTrimHistory(\n    name: String? = null\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { result -&gt;\n    llm.writeSession {\n        rewritePrompt { prompt -&gt;\n            val messages = prompt.messages\n\n            prompt.copy(messages = listOf(messages.first(), messages.last()))\n        }\n    }\n\n    result\n}\n\nval strategy = strategy&lt;String, String&gt;(\"chess_strategy\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendToolResult(\"nodeSendToolResult\")\n    val nodeTrimHistory by nodeTrimHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeTrimHistory)\n    edge(nodeTrimHistory forwardTo nodeSendToolResult)\n    edge(nodeSendToolResult forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeSendToolResult forwardTo nodeExecuteTool onToolCall { true })\n}\n\nval askChoiceStrategy = AskUserChoiceSelectionStrategy(promptShowToUser = { prompt -&gt;\n    val lastMessage = prompt.messages.last()\n    if (lastMessage is Message.Tool.Call) {\n        lastMessage.content\n    } else {\n        \"\"\n    }\n})\n</code></pre> <pre><code>val promptExecutor = PromptExecutorWithChoiceSelection(baseExecutor, askChoiceStrategy)\n</code></pre> <p>The first interactive approach uses <code>PromptExecutorWithChoiceSelection</code>, which wraps the base executor with choice selection capability. The custom display function extracts move information from tool calls to show users what the AI wants to do.</p> <p>Architecture Changes: - Wrapped Executor: <code>PromptExecutorWithChoiceSelection</code> adds choice functionality to any base executor - Context-Aware Display: Shows the last tool call content instead of the full prompt - Higher Temperature: Increased to 1.0 for more diverse move options</p>"},{"location":"examples/Chess/#advanced-strategy-manual-choice-selection","title":"Advanced Strategy: Manual Choice Selection","text":"<pre><code>val game = ChessGame()\nval toolRegistry = ToolRegistry { tools(listOf(Move(game))) }\n\nval agent = AIAgent(\n    executor = promptExecutor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Reasoning.O3Mini,\n    systemPrompt = \"\"\"\n            You are an agent who plays chess.\n            You should always propose a move in response to the \"Your move!\" message.\n\n            DO NOT HALLUCINATE!!!\n            DO NOT PLAY ILLEGAL MOVES!!!\n            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!\n        \"\"\".trimMargin(),\n    temperature = 1.0,\n    toolRegistry = toolRegistry,\n    maxIterations = 200,\n    numberOfChoices = 3,\n)\n</code></pre> <p>The advanced strategy integrates choice selection directly into the agent's execution graph:</p> <p>New Nodes: - <code>nodeLLMSendResultsMultipleChoices</code>: Handles multiple LLM choices simultaneously - <code>nodeSelectLLMChoice</code>: Integrates the choice selection strategy into the workflow</p> <p>Enhanced Control Flow: - Tool results are wrapped in lists to support multiple choices - User selection occurs before continuing with the chosen path - The selected choice is unwrapped and continues through the normal flow</p> <p>Benefits: - Greater Control: Fine-grained integration with agent workflow - Flexibility: Can be combined with other agent features - Transparency: Users see exactly what the AI is considering</p>"},{"location":"examples/Chess/#running-interactive-agents","title":"Running Interactive Agents","text":"<pre><code>println(\"Chess Game started!\")\n\nval initialMessage = \"Starting position is ${game.getBoard()}. White to move!\"\n\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre> <pre><code>Chess Game started!\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_K46Upz7XoBIG5RchDh7bZE8F, tool=move, content={\"notation\": \"p-e2-e4\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]\nChoice number 2: [Call(id=call_zJ6OhoCHrVHUNnKaxZkOhwoU, tool=move, content={\"notation\": \"p-e2-e4\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]\nChoice number 3: [Call(id=call_nwX6ZMJ3F5AxiNUypYlI4BH4, tool=move, content={\"notation\": \"p-e2-e4\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:40.368252Z, totalTokensCount=773, inputTokensCount=315, outputTokensCount=458, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p p p p p\n6 * * * * * * * *\n5 * * * * * * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_2V93GXOcIe0fAjUAIFEk9h5S, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]\nChoice number 2: [Call(id=call_INM59xRzKMFC1w8UAV74l9e1, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]\nChoice number 3: [Call(id=call_r4QoiTwn0F3jizepHH5ia8BU, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:47.949303Z, totalTokensCount=1301, inputTokensCount=341, outputTokensCount=960, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_f9XTizn41svcrtvnmkCfpSUQ, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 2: [Call(id=call_c0Dfce5RcSbN3cOOm5ESYriK, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 3: [Call(id=call_Lr4Mdro1iolh0fDyAwZsutrW, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:17:55.467712Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n\n\n\nThe execution was interrupted\n</code></pre> <pre><code>import ai.koog.agents.core.feature.choice.nodeLLMSendResultsMultipleChoices\nimport ai.koog.agents.core.feature.choice.nodeSelectLLMChoice\n\ninline fun &lt;reified T&gt; AIAgentSubgraphBuilderBase&lt;*, *&gt;.nodeTrimHistory(\n    name: String? = null\n): AIAgentNodeDelegate&lt;T, T&gt; = node(name) { result -&gt;\n    llm.writeSession {\n        rewritePrompt { prompt -&gt;\n            val messages = prompt.messages\n\n            prompt.copy(messages = listOf(messages.first(), messages.last()))\n        }\n    }\n\n    result\n}\n\nval strategy = strategy&lt;String, String&gt;(\"chess_strategy\") {\n    val nodeCallLLM by nodeLLMRequest(\"sendInput\")\n    val nodeExecuteTool by nodeExecuteTool(\"nodeExecuteTool\")\n    val nodeSendToolResult by nodeLLMSendResultsMultipleChoices(\"nodeSendToolResult\")\n    val nodeSelectLLMChoice by nodeSelectLLMChoice(askChoiceStrategy, \"chooseLLMChoice\")\n    val nodeTrimHistory by nodeTrimHistory&lt;ReceivedToolResult&gt;()\n\n    edge(nodeStart forwardTo nodeCallLLM)\n    edge(nodeCallLLM forwardTo nodeExecuteTool onToolCall { true })\n    edge(nodeCallLLM forwardTo nodeFinish onAssistantMessage { true })\n    edge(nodeExecuteTool forwardTo nodeTrimHistory)\n    edge(nodeTrimHistory forwardTo nodeSendToolResult transformed { listOf(it) })\n    edge(nodeSendToolResult forwardTo nodeSelectLLMChoice)\n    edge(nodeSelectLLMChoice forwardTo nodeFinish transformed { it.first() } onAssistantMessage { true })\n    edge(nodeSelectLLMChoice forwardTo nodeExecuteTool transformed { it.first() } onToolCall { true })\n}\n</code></pre> <pre><code>val game = ChessGame()\nval toolRegistry = ToolRegistry { tools(listOf(Move(game))) }\n\nval agent = AIAgent(\n    executor = baseExecutor,\n    strategy = strategy,\n    llmModel = OpenAIModels.Reasoning.O3Mini,\n    systemPrompt = \"\"\"\n            You are an agent who plays chess.\n            You should always propose a move in response to the \"Your move!\" message.\n\n            DO NOT HALLUCINATE!!!\n            DO NOT PLAY ILLEGAL MOVES!!!\n            YOU CAN SEND A MESSAGE ONLY IF IT IS A RESIGNATION OR A CHECKMATE!!!\n        \"\"\".trimMargin(),\n    temperature = 1.0,\n    toolRegistry = toolRegistry,\n    maxIterations = 200,\n    numberOfChoices = 3,\n)\n</code></pre> <pre><code>println(\"Chess Game started!\")\n\nval initialMessage = \"Starting position is ${game.getBoard()}. White to move!\"\n\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre> <pre><code>Chess Game started!\n8 r n b q k b n r\n7 p p p p p p p p\n6 * * * * * * * *\n5 * * * * * * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_gqMIar0z11CyUl5nup3zbutj, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 2: [Call(id=call_6niUGnZPPJILRFODIlJsCKax, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nChoice number 3: [Call(id=call_q1b8ZmIBph0EoVaU3Ic9A09j, tool=move, content={\"notation\": \"p-e7-e5\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:17.313548Z, totalTokensCount=917, inputTokensCount=341, outputTokensCount=576, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * * * *\n2 P P P P * P P P\n1 R N B Q K B N R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_pdBIX7MVi82MyWwawTm1Q2ef, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]\nChoice number 2: [Call(id=call_oygsPHaiAW5OM6pxhXhtazgp, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]\nChoice number 3: [Call(id=call_GJTEsZ8J8cqOKZW4Tx54RqCh, tool=move, content={\"notation\": \"n-g1-f3\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:24.505344Z, totalTokensCount=1237, inputTokensCount=341, outputTokensCount=896, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b n r\n7 p p p p * p p p\n6 * * * * * * * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n\nAvailable LLM choices\nChoice number 1: [Call(id=call_5C7HdlTU4n3KdXcyNogE4rGb, tool=move, content={\"notation\": \"n-g8-f6\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]\nChoice number 2: [Call(id=call_EjCcyeMLQ88wMa5yh3vmeJ2w, tool=move, content={\"notation\": \"n-g8-f6\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]\nChoice number 3: [Call(id=call_NBMMSwmFIa8M6zvfbPw85NKh, tool=move, content={\"notation\": \"n-g8-f6\"}, metaInfo=ResponseMetaInfo(timestamp=2025-08-18T21:18:34.646667Z, totalTokensCount=1621, inputTokensCount=341, outputTokensCount=1280, additionalInfo={}))]\nPlease choose a choice. Enter a number between 1 and 3: \n8 r n b q k b * r\n7 p p p p * p p p\n6 * * * * * n * *\n5 * * * * p * * *\n4 * * * * P * * *\n3 * * * * * N * *\n2 P P P P * P P P\n1 R N B Q K B * R\n  a b c d e f g h\n-----------------\n\n\n\nThe execution was interrupted\n</code></pre> <p>The interactive examples show how users can guide the AI's decision-making process. In the output, you can see:</p> <ol> <li>Multiple Choices: The AI generates 3 different move options</li> <li>User Selection: Users input numbers 1-3 to choose their preferred move</li> <li>Game Continuation: The selected move is executed and the game continues</li> </ol>"},{"location":"examples/Chess/#conclusion","title":"Conclusion","text":"<p>This tutorial demonstrates several key aspects of building intelligent agents with the Koog framework:</p>"},{"location":"examples/Chess/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Domain Modeling: Well-structured data models are crucial for complex applications</li> <li>Tool Integration: Custom tools enable agents to interact with external systems effectively</li> <li>Memory Management: Strategic history trimming optimizes performance for long interactions</li> <li>Strategy Graphs: Koog's graph-based approach provides flexible control flow</li> <li>Interactive AI: Choice selection enables human-AI collaboration and transparency</li> </ol>"},{"location":"examples/Chess/#framework-features-explored","title":"Framework Features Explored","text":"<ul> <li>\u2705 Custom tool creation and integration</li> <li>\u2705 Agent strategy design and graph-based control flow</li> <li>\u2705 Memory optimization techniques</li> <li>\u2705 Interactive choice selection</li> <li>\u2705 Multiple LLM response handling</li> <li>\u2705 Stateful game management</li> </ul> <p>The Koog framework provides the foundation for building sophisticated AI agents that can handle complex, multi-turn interactions while maintaining efficiency and transparency.</p>"},{"location":"examples/GoogleMapsMcp/","title":"Google Maps MCP with Koog: From Zero to Elevation in a Kotlin Notebook","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this short, blog-style walkthrough, we\u2019ll connect Koog to a Model Context Protocol (MCP) server for Google Maps. We\u2019ll spin up the server with Docker, discover the available tools, and let an AI agent geocode an address and fetch its elevation \u2014 all from a Kotlin Notebook.</p> <p>By the end, you\u2019ll have a reproducible, end\u2011to\u2011end example you can drop into your workflow or documentation.</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#prerequisites","title":"Prerequisites","text":"<p>Before you run the cells below, make sure you have:</p> <ul> <li>Docker installed and running</li> <li>A valid Google Maps API key exported as an environment variable: <code>GOOGLE_MAPS_API_KEY</code></li> <li>An OpenAI API key exported as <code>OPENAI_API_KEY</code></li> </ul> <p>You can set them in your shell like this (macOS/Linux example):</p> <pre><code>export GOOGLE_MAPS_API_KEY=\"&lt;your-key&gt;\"\nexport OPENAI_API_KEY=\"&lt;your-openai-key&gt;\"\n</code></pre> <pre><code>// Get the API key from environment variables\nval googleMapsApiKey = System.getenv(\"GOOGLE_MAPS_API_KEY\") ?: error(\"GOOGLE_MAPS_API_KEY environment variable not set\")\nval openAIApiToken = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#start-the-google-maps-mcp-server-docker","title":"Start the Google Maps MCP server (Docker)","text":"<p>We\u2019ll use the official <code>mcp/google-maps</code> image. The container will expose tools such as <code>maps_geocode</code> and <code>maps_elevation</code> over MCP. We pass the API key via environment variables and launch it attached so the notebook can talk to it over stdio.</p> <pre><code>// Start the Docker container with the Google Maps MCP server\nval process = ProcessBuilder(\n    \"docker\",\n    \"run\",\n    \"-i\",\n    \"-e\",\n    \"GOOGLE_MAPS_API_KEY=$googleMapsApiKey\",\n    \"mcp/google-maps\"\n).start()\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#discover-tools-via-mcptoolregistry","title":"Discover tools via McpToolRegistry","text":"<p>Koog can connect to an MCP server over stdio. Here, we create a tool registry from the running process and print out the discovered tools and their descriptors.</p> <pre><code>val toolRegistry = McpToolRegistryProvider.fromTransport(\n    transport = McpToolRegistryProvider.defaultStdioTransport(process)\n)\ntoolRegistry.tools.forEach {\n    println(it.name)\n    println(it.descriptor)\n}\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#build-an-ai-agent-with-openai","title":"Build an AI Agent with OpenAI","text":"<p>Next we assemble a simple agent backed by the OpenAI executor and model. The agent will be able to call tools exposed by the MCP server through the registry we just created.</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(openAIApiToken),\n    llmModel = OpenAIModels.Chat.GPT4o,\n    toolRegistry = toolRegistry,\n)\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#ask-for-elevation-geocode-first-then-elevation","title":"Ask for elevation: geocode first, then elevation","text":"<p>We prompt the agent to find the elevation of the JetBrains office in Munich. The instruction explicitly tells the agent to use only the available tools and which ones to prefer for the task.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nval request = \"Get elevation of the Jetbrains Office in Munich, Germany?\"\nrunBlocking {\n    agent.run(\n        request +\n            \"You can only call tools. Get it by calling maps_geocode and maps_elevation tools.\"\n    )\n}\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#clean-up","title":"Clean up","text":"<p>When you\u2019re done, stop the Docker process so you don\u2019t leave anything running in the background.</p> <pre><code>process.destroy()\n</code></pre>"},{"location":"examples/GoogleMapsMcp/#troubleshooting-and-next-steps","title":"Troubleshooting and next steps","text":"<ul> <li>If the container fails to start, check that Docker is running and your <code>GOOGLE_MAPS_API_KEY</code> is valid.</li> <li>If the agent can\u2019t call tools, re-run the discovery cell to ensure the tool registry is populated.</li> <li>Try other prompts like route planning or place searches using the available Google Maps tools.</li> </ul> <p>Next, consider composing multiple MCP servers (e.g., Playwright for web automation + Google Maps) and let Koog orchestrate tool usage for richer tasks.</p>"},{"location":"examples/Guesser/","title":"Building a Number\u2011Guessing Agent with Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>Let\u2019s build a small but fun agent that guesses a number you\u2019re thinking of. We\u2019ll lean on Koog\u2019s tool-calling to ask targeted questions and converge using a classic binary search strategy. The result is an idiomatic Kotlin Notebook that you can drop straight into docs.</p> <p>We\u2019ll keep the code minimal and the flow transparent: a few tiny tools, a compact prompt, and an interactive CLI loop.</p>"},{"location":"examples/Guesser/#setup","title":"Setup","text":"<p>This notebook assumes: - You\u2019re running in a Kotlin Notebook with Koog available. - The environment variable <code>OPENAI_API_KEY</code> is set. The agent uses it via <code>simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\"))</code>.</p> <p>Load the Koog kernel:</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/Guesser/#tools-asking-targeted-questions","title":"Tools: asking targeted questions","text":"<p>Tools are small, well-described functions the LLM can call. We\u2019ll provide three: - <code>lessThan(value)</code>: \u201cIs your number less than value?\u201d - <code>greaterThan(value)</code>: \u201cIs your number greater than value?\u201d - <code>proposeNumber(value)</code>: \u201cIs your number equal to value?\u201d (used once the range is tight)</p> <p>Each tool returns a simple \"YES\"/\"NO\" string. The helper <code>ask</code> implements a minimal Y/n loop and validates input. Descriptions via <code>@LLMDescription</code> help the model select tools correctly.</p> <pre><code>import ai.koog.agents.core.tools.annotations.Tool\n\nclass GuesserTool : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Asks the user if his number is STRICTLY less than a given value.\")\n    fun lessThan(\n        @LLMDescription(\"A value to compare the guessed number with.\") value: Int\n    ): String = ask(\"Is your number less than $value?\", value)\n\n    @Tool\n    @LLMDescription(\"Asks the user if his number is STRICTLY greater than a given value.\")\n    fun greaterThan(\n        @LLMDescription(\"A value to compare the guessed number with.\") value: Int\n    ): String = ask(\"Is your number greater than $value?\", value)\n\n    @Tool\n    @LLMDescription(\"Asks the user if his number is EXACTLY equal to the given number. Only use this tool once you've narrowed down your answer.\")\n    fun proposeNumber(\n        @LLMDescription(\"A value to compare the guessed number with.\") value: Int\n    ): String = ask(\"Is your number equal to $value?\", value)\n\n    fun ask(question: String, value: Int): String {\n        print(\"$question [Y/n]: \")\n        val input = readln()\n        println(input)\n\n        return when (input.lowercase()) {\n            \"\", \"y\", \"yes\" -&gt; \"YES\"\n            \"n\", \"no\" -&gt; \"NO\"\n            else -&gt; {\n                println(\"Invalid input! Please, try again.\")\n                ask(question, value)\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"examples/Guesser/#tool-registry","title":"Tool Registry","text":"<p>Expose your tools to the agent. We also add a built\u2011in <code>SayToUser</code> tool so the agent can surface messages directly to the user.</p> <pre><code>val toolRegistry = ToolRegistry {\n    tool(SayToUser)\n    tools(GuesserTool())\n}\n</code></pre>"},{"location":"examples/Guesser/#agent-configuration","title":"Agent configuration","text":"<p>A short, tool\u2011forward system prompt is all we need. We\u2019ll suggest a binary search strategy and keep <code>temperature = 0.0</code> for stable, deterministic behavior. Here we use OpenAI\u2019s reasoning model <code>GPT4oMini</code> for crisp planning.</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Reasoning.GPT4oMini,\n    systemPrompt = \"\"\"\n            You are a number guessing agent. Your goal is to guess a number that the user is thinking of.\n\n            Follow these steps:\n            1. Start by asking the user to think of a number between 1 and 100.\n            2. Use the less_than and greater_than tools to narrow down the range.\n                a. If it's neither greater nor smaller, use the propose_number tool.\n            3. Once you're confident about the number, use the propose_number tool to check if your guess is correct.\n            4. If your guess is correct, congratulate the user. If not, continue guessing.\n\n            Be efficient with your guessing strategy. A binary search approach works well.\n        \"\"\".trimIndent(),\n    temperature = 0.0,\n    toolRegistry = toolRegistry\n)\n</code></pre>"},{"location":"examples/Guesser/#run-it","title":"Run it","text":"<ul> <li>Think of a number between 1 and 100.</li> <li>Type <code>start</code> to begin.</li> <li>Answer the agent\u2019s questions with <code>Y</code>/<code>Enter</code> for yes or <code>n</code> for no. The agent should zero in on your number in ~7 steps.</li> </ul> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Number Guessing Game started!\")\nprintln(\"Think of a number between 1 and 100, and I'll try to guess it.\")\nprintln(\"Type 'start' to begin the game.\")\n\nval initialMessage = readln()\nrunBlocking {\n    agent.run(initialMessage)\n}\n</code></pre>"},{"location":"examples/Guesser/#how-it-works","title":"How it works","text":"<ul> <li>The agent reads the system prompt and plans a binary search.</li> <li>On each iteration it calls one of your tools: <code>lessThan</code>, <code>greaterThan</code>, or (when certain) <code>proposeNumber</code>.</li> <li>The helper <code>ask</code> collects your Y/n input and returns a clean \"YES\"/\"NO\" signal back to the model.</li> <li>When it gets confirmation, it congratulates you via <code>SayToUser</code>.</li> </ul>"},{"location":"examples/Guesser/#extend-it","title":"Extend it","text":"<ul> <li>Change the range (e.g., 1..1000) by tweaking the system prompt.</li> <li>Add a <code>between(low, high)</code> tool to reduce calls further.</li> <li>Swap models or executors (e.g., use an Ollama executor and a local model) while keeping the same tools.</li> <li>Persist guesses or outcomes to a store for analytics.</li> </ul>"},{"location":"examples/Guesser/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Missing key: ensure <code>OPENAI_API_KEY</code> is set in your environment.</li> <li>Kernel not found: make sure <code>%useLatestDescriptors</code> and <code>%use koog</code> executed successfully.</li> <li>Tool not called: confirm the <code>ToolRegistry</code> includes <code>GuesserTool()</code> and the names in the prompt match your tool functions.</li> </ul>"},{"location":"examples/Langfuse/","title":"Tracing Koog Agents to Langfuse with OpenTelemetry","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook shows how to export Koog agent traces to your Langfuse instance using OpenTelemetry. You'll set up environment variables, run a simple agent, and then inspect spans and traces in Langfuse.</p>"},{"location":"examples/Langfuse/#what-youll-learn","title":"What you'll learn","text":"<ul> <li>How Koog integrates with OpenTelemetry to emit traces</li> <li>How to configure the Langfuse exporter via environment variables</li> <li>How to run an agent and view its trace in Langfuse</li> </ul>"},{"location":"examples/Langfuse/#prerequisites","title":"Prerequisites","text":"<ul> <li>A Langfuse project (host URL, public key, secret key)</li> <li>An OpenAI API key for the LLM executor</li> <li>Environment variables set in your shell:</li> </ul> <pre><code>export OPENAI_API_KEY=sk-...\nexport LANGFUSE_HOST=https://cloud.langfuse.com # or your self-hosted URL\nexport LANGFUSE_PUBLIC_KEY=pk_...\nexport LANGFUSE_SECRET_KEY=sk_...\n</code></pre> <pre><code>%useLatestDescriptors\n//%use koog\n</code></pre> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.features.opentelemetry.feature.OpenTelemetry\nimport ai.koog.agents.features.opentelemetry.integration.langfuse.addLangfuseExporter\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\n\n/**\n * Example of Koog agents tracing to [Langfuse](https://langfuse.com/)\n *\n * Agent traces are exported to:\n * - Langfuse OTLP endpoint instance using [OtlpHttpSpanExporter]\n *\n * To run this example:\n *  1. Set up a Langfuse project and credentials as described [here](https://langfuse.com/docs/get-started#create-new-project-in-langfuse)\n *  2. Get Langfuse credentials as described [here](https://langfuse.com/faq/all/where-are-langfuse-api-keys)\n *  3. Set `LANGFUSE_HOST`, `LANGFUSE_PUBLIC_KEY`, and `LANGFUSE_SECRET_KEY` environment variables\n *\n * @see &lt;a href=\"https://langfuse.com/docs/opentelemetry/get-started#opentelemetry-endpoint\"&gt;Langfuse OpenTelemetry Docs&lt;/a&gt;\n */\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Reasoning.GPT4oMini,\n    systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n) {\n    install(OpenTelemetry) {\n        addLangfuseExporter()\n    }\n}\n</code></pre>"},{"location":"examples/Langfuse/#configure-the-agent-and-langfuse-exporter","title":"Configure the agent and Langfuse exporter","text":"<p>In the next cell, we:</p> <ul> <li>Create an AIAgent that uses OpenAI as the LLM executor</li> <li>Install the OpenTelemetry feature and add the Langfuse exporter</li> <li>Rely on environment variables for Langfuse configuration</li> </ul> <p>Under the hood, Koog emits spans for agent lifecycle, LLM calls, and tool execution (if any). The Langfuse exporter ships those spans to your Langfuse instance via the OpenTelemetry endpoint.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Running agent with Langfuse tracing\")\n\nrunBlocking {\n    val result = agent.run(\"Tell me a joke about programming\")\n    \"Result: $result\\nSee traces on the Langfuse instance\"\n}\n</code></pre>"},{"location":"examples/Langfuse/#run-the-agent-and-view-traces","title":"Run the agent and view traces","text":"<p>Execute the next cell to trigger a simple prompt. This will generate spans that are exported to your Langfuse project.</p>"},{"location":"examples/Langfuse/#where-to-look-in-langfuse","title":"Where to look in Langfuse","text":"<ol> <li>Open your Langfuse dashboard and select your project</li> <li>Navigate to the Traces/Spans view</li> <li>Look for recent entries around the time you ran this cell</li> <li>Drill down into spans to see:</li> <li>Agent lifecycle events</li> <li>LLM request/response metadata</li> <li>Errors (if any)</li> </ol>"},{"location":"examples/Langfuse/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>No traces showing up?</li> <li>Double-check LANGFUSE_HOST, LANGFUSE_PUBLIC_KEY, LANGFUSE_SECRET_KEY</li> <li>Ensure your network allows outbound HTTPS to the Langfuse endpoint</li> <li>Verify your Langfuse project is active and keys belong to the correct project</li> <li>Authentication errors</li> <li>Regenerate keys in Langfuse and update env vars</li> <li>OpenAI issues</li> <li>Confirm OPENAI_API_KEY is set and valid</li> </ul>"},{"location":"examples/OpenTelemetry/","title":"OpenTelemetry with Koog: Tracing your AI agent","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook demonstrates how to add OpenTelemetry-based tracing to a Koog AI agent. We will: - Emit spans to the console for quick local debugging. - Export spans to an OpenTelemetry Collector and view them in Jaeger.</p> <p>Prerequisites: - Docker/Docker Compose installed - An OpenAI API key available in environment variable <code>OPENAI_API_KEY</code></p> <p>Start the local OpenTelemetry stack (Collector + Jaeger) before running the notebook: <pre><code>./docker-compose up -d\n</code></pre> After the agent runs, open Jaeger UI: - http://localhost:16686</p> <p>To stop the services later: <pre><code>docker-compose down\n</code></pre></p> <pre><code>%useLatestDescriptors\n// %use koog\n</code></pre> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.features.opentelemetry.feature.OpenTelemetry\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\nimport io.opentelemetry.exporter.logging.LoggingSpanExporter\nimport io.opentelemetry.exporter.otlp.trace.OtlpGrpcSpanExporter\n</code></pre>"},{"location":"examples/OpenTelemetry/#configure-opentelemetry-exporters","title":"Configure OpenTelemetry exporters","text":"<p>In the next cell, we: - Create a Koog AIAgent - Install the OpenTelemetry feature - Add two span exporters:   - LoggingSpanExporter for console logs   - OTLP gRPC exporter to http://localhost:4317 (Collector)</p> <p>This mirrors the example description: console logs for local debugging and OTLP for viewing traces in Jaeger.</p> <pre><code>val agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Reasoning.GPT4oMini,\n    systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n) {\n    install(OpenTelemetry) {\n        // Add a console logger for local debugging\n        addSpanExporter(LoggingSpanExporter.create())\n\n        // Send traces to OpenTelemetry collector\n        addSpanExporter(\n            OtlpGrpcSpanExporter.builder()\n                .setEndpoint(\"http://localhost:4317\")\n                .build()\n        )\n    }\n}\n</code></pre>"},{"location":"examples/OpenTelemetry/#run-the-agent-and-view-traces-in-jaeger","title":"Run the agent and view traces in Jaeger","text":"<p>Execute the next cell to trigger a simple prompt. You should see: - Console span logs from the LoggingSpanExporter - Traces exported to your local OpenTelemetry Collector and visible in Jaeger at http://localhost:16686</p> <p>Tip: Use the Jaeger search to find recent traces after you run the cell.</p> <pre><code>import ai.koog.agents.utils.use\nimport kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    agent.use { agent -&gt;\n        println(\"Running agent with OpenTelemetry tracing...\")\n\n        val result = agent.run(\"Tell me a joke about programming\")\n\n        \"Agent run completed with result: '$result'.\\nCheck Jaeger UI at http://localhost:16686 to view traces\"\n    }\n}\n</code></pre>"},{"location":"examples/OpenTelemetry/#cleanup-and-troubleshooting","title":"Cleanup and troubleshooting","text":"<p>When you're done:</p> <ul> <li> <p>Stop services:   <pre><code>docker-compose down\n</code></pre></p> </li> <li> <p>If you don't see traces in Jaeger:</p> </li> <li>Ensure the stack is running: <code>./docker-compose up -d</code> and give it a few seconds to start.</li> <li>Verify ports:<ul> <li>Collector (OTLP gRPC): http://localhost:4317</li> <li>Jaeger UI: http://localhost:16686</li> </ul> </li> <li>Check container logs: <code>docker-compose logs --tail=200</code></li> <li>Confirm your <code>OPENAI_API_KEY</code> is set in the environment where the notebook runs.</li> <li> <p>Make sure the endpoint in the exporter matches the collector: <code>http://localhost:4317</code>.</p> </li> <li> <p>What spans to expect:</p> </li> <li>Koog agent lifecycle</li> <li>LLM request/response metadata</li> <li>Any tool execution spans (if you add tools)</li> </ul> <p>You can now iterate on your agent and observe changes in your tracing pipeline.</p>"},{"location":"examples/PlaywrightMcp/","title":"Drive the browser with Playwright MCP and Koog","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this notebook, you'll connect a Koog agent to Playwright's Model Context Protocol (MCP) server and let it drive a real browser to complete a task: open jetbrains.com, accept cookies, and click the AI section in the toolbar.</p> <p>We'll keep things simple and reproducible, focusing on a minimal but realistic agent + tools setup you can publish and reuse.</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre>"},{"location":"examples/PlaywrightMcp/#prerequisites","title":"Prerequisites","text":"<ul> <li>An OpenAI API key exported as an environment variable: <code>OPENAI_API_KEY</code></li> <li>Node.js and npx available on your PATH</li> <li>Kotlin Jupyter notebook environment with Koog available via <code>%use koog</code></li> </ul> <p>Tip: Run the Playwright MCP server in headful mode to watch the browser automate the steps.</p>"},{"location":"examples/PlaywrightMcp/#1-provide-your-openai-api-key","title":"1) Provide your OpenAI API key","text":"<p>We read the API key from the <code>OPENAI_API_KEY</code> environment variable. This keeps secrets out of the notebook.</p> <pre><code>// Get the API key from environment variables\nval openAIApiToken = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\n</code></pre>"},{"location":"examples/PlaywrightMcp/#2-start-the-playwright-mcp-server","title":"2) Start the Playwright MCP server","text":"<p>We'll launch Playwright's MCP server locally using <code>npx</code>. By default, it will expose an SSE endpoint we can connect to from Koog.</p> <pre><code>// Start the Playwright MCP server via npx\nval process = ProcessBuilder(\n    \"npx\",\n    \"@playwright/mcp@latest\",\n    \"--port\",\n    \"8931\"\n).start()\n</code></pre>"},{"location":"examples/PlaywrightMcp/#3-connect-from-koog-and-run-the-agent","title":"3) Connect from Koog and run the agent","text":"<p>We build a minimal Koog <code>AIAgent</code> with an OpenAI executor and point its tool registry to the MCP server over SSE. Then we ask it to complete the browser task strictly via tools.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    println(\"Connecting to Playwright MCP server...\")\n    val toolRegistry = McpToolRegistryProvider.fromTransport(\n        transport = McpToolRegistryProvider.defaultSseTransport(\"http://localhost:8931\")\n    )\n    println(\"Successfully connected to Playwright MCP server\")\n\n    // Create the agent\n    val agent = AIAgent(\n        executor = simpleOpenAIExecutor(openAIApiToken),\n        llmModel = OpenAIModels.Chat.GPT4o,\n        toolRegistry = toolRegistry,\n    )\n\n    val request = \"Open a browser, navigate to jetbrains.com, accept all cookies, click AI in toolbar\"\n    println(\"Sending request: $request\")\n\n    agent.run(\n        request + \". \" +\n            \"You can only call tools. Use the Playwright tools to complete this task.\"\n    )\n}\n</code></pre>"},{"location":"examples/PlaywrightMcp/#4-shut-down-the-mcp-process","title":"4) Shut down the MCP process","text":"<p>Always clean up the external process at the end of your run.</p> <pre><code>// Shutdown the Playwright MCP process\nprintln(\"Closing connection to Playwright MCP server\")\nprocess.destroy()\n</code></pre>"},{"location":"examples/PlaywrightMcp/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If the agent can't connect, make sure the MCP server is running on <code>http://localhost:8931</code>.</li> <li>If you don't see the browser, ensure Playwright is installed and able to launch a browser on your system.</li> <li>If you get authentication errors from OpenAI, double-check the <code>OPENAI_API_KEY</code> environment variable.</li> </ul>"},{"location":"examples/PlaywrightMcp/#next-steps","title":"Next steps","text":"<ul> <li>Try different websites or flows. The MCP server exposes a rich set of Playwright tools.</li> <li>Swap the LLM model, or add more tools to the Koog agent.</li> <li>Integrate this flow into your app, or publish the notebook as documentation.</li> </ul>"},{"location":"examples/UnityMcp/","title":"Unity + Koog: Drive your game from a Kotlin Agent","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook walks you through building a Unity-savvy AI agent with Koog using the Model Context Protocol (MCP). We'll connect to a Unity MCP server, discover tools, plan with an LLM, and execute actions against your open scene.</p> <p>Prerequisites - A Unity project with the Unity-MCP server plugin installed - JDK 17+ - An OpenAI API key in the OPENAI_API_KEY environment variable</p> <pre><code>%useLatestDescriptors\n%use koog\n</code></pre> <pre><code>lateinit var process: Process\n</code></pre>"},{"location":"examples/UnityMcp/#1-provide-your-openai-api-key","title":"1) Provide your OpenAI API key","text":"<p>We read the API key from the <code>OPENAI_API_KEY</code> environment variable so you can keep secrets out of the notebook.</p> <pre><code>val token = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\nval executor = simpleOpenAIExecutor(token)\n</code></pre>"},{"location":"examples/UnityMcp/#2-configure-the-unity-agent","title":"2) Configure the Unity agent","text":"<p>We define a compact system prompt and agent settings for Unity.</p> <pre><code>val agentConfig = AIAgentConfig(\n    prompt = prompt(\"cook_agent_system_prompt\") {\n        system {\n            \"You are a Unity assistant. You can execute different tasks by interacting with tools from the Unity engine.\"\n        }\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 1000\n)\n</code></pre> <pre><code>\n</code></pre>"},{"location":"examples/UnityMcp/#3-start-the-unity-mcp-server","title":"3) Start the Unity MCP server","text":"<p>We'll launch the Unity MCP server from your Unity project directory and connect over stdio.</p> <pre><code>// https://github.com/IvanMurzak/Unity-MCP\nval pathToUnityProject = \"path/to/unity/project\"\nval process = ProcessBuilder(\n    \"$pathToUnityProject/com.ivanmurzak.unity.mcp.server/bin~/Release/net9.0/com.IvanMurzak.Unity.MCP.Server\",\n    \"60606\"\n).start()\n</code></pre>"},{"location":"examples/UnityMcp/#4-connect-from-koog-and-run-the-agent","title":"4) Connect from Koog and run the agent","text":"<p>We discover tools from the Unity MCP server, build a small plan-first strategy, and run an agent that uses only tools to modify your open scene.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    // Create the ToolRegistry with tools from the MCP server\n    val toolRegistry = McpToolRegistryProvider.fromTransport(\n        transport = McpToolRegistryProvider.defaultStdioTransport(process)\n    ) + ToolRegistry {\n        tool(ProvideStringSubgraphResult)\n    }\n\n    toolRegistry.tools.forEach {\n        println(it.name)\n        println(it.descriptor)\n    }\n\n    val strategy = strategy&lt;String, String&gt;(\"unity_interaction\") {\n        val nodePlanIngredients by nodeLLMRequest(allowToolCalls = false)\n        val interactionWithUnity by subgraphWithTask&lt;String&gt;(\n            // work with plan\n            tools = toolRegistry.tools,\n        ) { input -&gt;\n            \"Start interacting with Unity according to the plan: $input\"\n        }\n\n        edge(\n            nodeStart forwardTo nodePlanIngredients transformed {\n                \"Create detailed plan for \" + agentInput + \"\" +\n                    \"using the following tools: ${toolRegistry.tools.joinToString(\"\\n\") {\n                        it.name + \"\\ndescription:\" + it.descriptor\n                    }}\"\n            }\n        )\n        edge(nodePlanIngredients forwardTo interactionWithUnity onAssistantMessage { true })\n        edge(interactionWithUnity forwardTo nodeFinish transformed { it.result })\n    }\n\n    val agent = AIAgent(\n        promptExecutor = executor,\n        strategy = strategy,\n        agentConfig = agentConfig,\n        toolRegistry = toolRegistry,\n        installFeatures = {\n            install(Tracing)\n\n            install(EventHandler) {\n                onBeforeAgentStarted { eventContext -&gt;\n                    println(\"OnBeforeAgentStarted first (strategy: ${strategy.name})\")\n                }\n\n                onBeforeAgentStarted { eventContext -&gt;\n                    println(\"OnBeforeAgentStarted second (strategy: ${strategy.name})\")\n                }\n\n                onAgentFinished { eventContext -&gt;\n                    println(\n                        \"OnAgentFinished (agent id: ${eventContext.agentId}, result: ${eventContext.result})\"\n                    )\n                }\n            }\n        }\n    )\n\n    val result = agent.run(\n        \" extend current opened scene for the towerdefence game. \" +\n            \"Add more placements for the towers, change the path for the enemies\"\n    )\n\n    result\n}\n</code></pre>"},{"location":"examples/UnityMcp/#5-shut-down-the-mcp-process","title":"5) Shut down the MCP process","text":"<p>Always clean up the external Unity MCP server process at the end of your run.</p> <pre><code>// Shutdown the Unity MCP process\nprocess.destroy()\n</code></pre>"},{"location":"examples/VaccumAgent/","title":"Build a Simple Vacuum Cleaner Agent","text":"<p> Open on GitHub  Download .ipynb</p> <p>In this notebook, we'll explore how to implement a basic reflex agent using the new Kotlin agents framework. Our example will be the classic \"vacuum world\" problem \u2014 a simple environment with two locations that can be clean or dirty, and an agent that needs to clean them.</p> <p>First, let's understand our environment model:</p> <pre><code>import kotlin.random.Random\n\n/**\n * Represents a simple vacuum world with two locations (A and B).\n *\n * The environment tracks:\n * - The current location of the vacuum agent ('A' or 'B')\n * - The cleanliness status of each location (true = dirty, false = clean)\n */\nclass VacuumEnv {\n    var location: Char = 'A'\n        private set\n\n    private val status = mutableMapOf(\n        'A' to Random.nextBoolean(),\n        'B' to Random.nextBoolean()\n    )\n\n    fun percept(): Pair&lt;Char, Boolean&gt; = location to status.getValue(location)\n\n    fun clean(): String {\n        status[location] = false\n        return \"cleaned\"\n    }\n\n    fun moveLeft(): String {\n        location = 'A'\n        return \"move to A\"\n    }\n\n    fun moveRight(): String {\n        location = 'B'\n        return \"move to B\"\n    }\n\n    fun isClean(): Boolean = status.values.all { it }\n\n    fun worldLayout(): String = \"${status.keys}\"\n\n    override fun toString(): String = \"location=$location, dirtyA=${status['A']}, dirtyB=${status['B']}\"\n}\n</code></pre> <p>The VacuumEnv class models our simple world: - Two locations are represented by characters 'A' and 'B' - Each location can be either clean or dirty (randomly initialized) - The agent can be at either location at any given time - The agent can perceive its current location and whether it's dirty - The agent can take actions: move to a specific location or clean the current location</p>"},{"location":"examples/VaccumAgent/#creating-tools-for-vacuum-agent","title":"Creating Tools for Vacuum Agent","text":"<p>Now, let's define the tools our AI agent will use to interact with the environment:</p> <pre><code>import ai.koog.agents.core.tools.annotations.LLMDescription\nimport ai.koog.agents.core.tools.annotations.Tool\nimport ai.koog.agents.core.tools.reflect.ToolSet\n\n\n/**\n * Provides tools for the LLM agent to control the vacuum robot.\n * All methods either mutate or read from the VacuumEnv passed to the constructor.\n */\n@LLMDescription(\"Tools for controlling a two-cell vacuum world\")\nclass VacuumTools(private val env: VacuumEnv) : ToolSet {\n\n    @Tool\n    @LLMDescription(\"Returns current location and whether it is dirty\")\n    fun sense(): String {\n        val (loc, dirty) = env.percept()\n        return \"location=$loc, dirty=$dirty, locations=${env.worldLayout()}\"\n    }\n\n    @Tool\n    @LLMDescription(\"Cleans the current cell\")\n    fun clean(): String = env.clean()\n\n    @Tool\n    @LLMDescription(\"Moves the agent to cell A\")\n    fun moveLeft(): String = env.moveLeft()\n\n    @Tool\n    @LLMDescription(\"Moves the agent to cell B\")\n    fun moveRight(): String = env.moveRight()\n}\n</code></pre> <p>The <code>VacuumTools</code> class creates an interface between our LLM agent and the environment:</p> <ul> <li>It implements <code>ToolSet</code> from the Kotlin AI Agents framework</li> <li>Each tool is annotated with <code>@Tool</code> and has a description for the LLM</li> <li>The tools allow the agent to sense its environment and take actions</li> <li>Each method returns a string that describes the outcome of the action</li> </ul>"},{"location":"examples/VaccumAgent/#setting-up-the-agent","title":"Setting Up the Agent","text":"<p>Next, we'll configure and create our AI agent:</p> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.core.agent.config.AIAgentConfig\nimport ai.koog.agents.core.tools.ToolRegistry\nimport ai.koog.agents.core.tools.reflect.asTools\nimport ai.koog.agents.ext.agent.chatAgentStrategy\nimport ai.koog.agents.ext.tool.AskUser\nimport ai.koog.agents.ext.tool.SayToUser\nimport ai.koog.prompt.dsl.prompt\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\nimport ai.koog.prompt.params.LLMParams\n\n\nval env = VacuumEnv()\nval apiToken = System.getenv(\"OPENAI_API_KEY\") ?: error(\"OPENAI_API_KEY environment variable not set\")\nval executor = simpleOpenAIExecutor(apiToken = apiToken)\n\nval toolRegistry = ToolRegistry {\n    tool(SayToUser)\n    tool(AskUser)\n    tools(VacuumTools(env).asTools())\n}\n\nval systemVacuumPrompt = \"\"\"\n    You are a reflex vacuum-cleaner agent living in a two-cell world labelled A and B.\n    Your goal: make both cells clean, using the provided tools.\n    First, call sense() to inspect where you are. Then decide: if dirty \u2192 clean(); else moveLeft()/moveRight().\n    Continue until both cells are clean, then tell the user \"done\".\n    Use sayToUser to inform the user about each step.\n\"\"\".trimIndent()\n\nval agentConfig = AIAgentConfig(\n    prompt = prompt(\"chat\", params = LLMParams(temperature = 1.0)) {\n        system(systemVacuumPrompt)\n    },\n    model = OpenAIModels.Chat.GPT4o,\n    maxAgentIterations = 50,\n)\n\nval agent = AIAgent(\n    promptExecutor = executor,\n    strategy = chatAgentStrategy(),\n    agentConfig = agentConfig,\n    toolRegistry = toolRegistry\n)\n</code></pre> <p>In this setup:</p> <ol> <li>We create an instance of our environment</li> <li>We set up a connection to OpenAI's GPT-4o model</li> <li>We register the tools our agent can use</li> <li>We define a system prompt that gives the agent its goal and behavior rules</li> <li>We create the agent using the <code>AIAgent</code> constructor with a chat strategy</li> </ol>"},{"location":"examples/VaccumAgent/#running-the-agent","title":"Running the Agent","text":"<p>Finally, let's run our agent:</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nrunBlocking {\n    agent.run(\"Start cleaning, please\")\n}\n</code></pre> <pre><code>Agent says: Currently in cell A. It's already clean.\nAgent says: Moved to cell B. It's already clean.\n</code></pre> <p>When we run this code:</p> <ol> <li>The agent receives the initial prompt to start cleaning</li> <li>It uses its tools to sense the environment and make decisions</li> <li>It continues cleaning until both cells are clean</li> <li>Throughout the process, it keeps the user informed about what it's doing</li> </ol> <pre><code>// Finally we can validate that the work is finished by printing the env state\n\nenv\n</code></pre> <pre><code>location=B, dirtyA=false, dirtyB=false\n</code></pre>"},{"location":"examples/Weave/","title":"Weave tracing for Koog agents","text":"<p> Open on GitHub  Download .ipynb</p> <p>This notebook demonstrates how to trace Koog agents to W&amp;B Weave using OpenTelemetry (OTLP). You will create a simple Koog <code>AIAgent</code>, enable the Weave exporter, run a prompt, and view rich traces in the Weave UI.</p> <p>For background, see Weave OpenTelemetry docs: https://weave-docs.wandb.ai/guides/tracking/otel/</p>"},{"location":"examples/Weave/#prerequisites","title":"Prerequisites","text":"<p>Before running the example, make sure you have:</p> <ul> <li>A Weave/W&amp;B account: https://wandb.ai</li> <li>Your API key from https://wandb.ai/authorize exposed as an environment variable: <code>WEAVE_API_KEY</code></li> <li>Your Weave entity (team or user) name exposed as <code>WEAVE_ENTITY</code></li> <li>Find it on your W&amp;B dashboard: https://wandb.ai/home (left sidebar \"Teams\")</li> <li>A project name exposed as <code>WEAVE_PROJECT_NAME</code> (if not set, this example uses <code>koog-tracing</code>)</li> <li>An OpenAI API key exposed as <code>OPENAI_API_KEY</code> to run the Koog agent</li> </ul> <p>Example (macOS/Linux): <pre><code>export WEAVE_API_KEY=...  # required by Weave\nexport WEAVE_ENTITY=your-team-or-username\nexport WEAVE_PROJECT_NAME=koog-tracing\nexport OPENAI_API_KEY=...\n</code></pre></p>"},{"location":"examples/Weave/#notebook-setup","title":"Notebook setup","text":"<p>We use the latest Kotlin Jupyter descriptors. If you have Koog preconfigured as a <code>%use</code> plugin, you can uncomment the line below.</p> <pre><code>%useLatestDescriptors\n//%use koog\n</code></pre>"},{"location":"examples/Weave/#create-an-agent-and-enable-weave-tracing","title":"Create an agent and enable Weave tracing","text":"<p>We construct a minimal <code>AIAgent</code> and install the <code>OpenTelemetry</code> feature with the Weave exporter. The exporter sends OTLP spans to Weave using your environment configuration: - <code>WEAVE_API_KEY</code> \u2014 authentication to Weave - <code>WEAVE_ENTITY</code> \u2014 which team/user owns the traces - <code>WEAVE_PROJECT_NAME</code> \u2014 the Weave project to store traces in</p> <pre><code>import ai.koog.agents.core.agent.AIAgent\nimport ai.koog.agents.features.opentelemetry.feature.OpenTelemetry\nimport ai.koog.agents.features.opentelemetry.integration.weave.addWeaveExporter\nimport ai.koog.prompt.executor.clients.openai.OpenAIModels\nimport ai.koog.prompt.executor.llms.all.simpleOpenAIExecutor\n\nval entity = System.getenv()[\"WEAVE_ENTITY\"] ?: throw IllegalArgumentException(\"WEAVE_ENTITY is not set\")\nval projectName = System.getenv()[\"WEAVE_PROJECT_NAME\"] ?: \"koog-tracing\"\n\nval agent = AIAgent(\n    executor = simpleOpenAIExecutor(System.getenv(\"OPENAI_API_KEY\")),\n    llmModel = OpenAIModels.Reasoning.GPT4oMini,\n    systemPrompt = \"You are a code assistant. Provide concise code examples.\"\n) {\n    install(OpenTelemetry) {\n        addWeaveExporter(\n            weaveEntity = entity,\n            weaveProjectName = projectName\n        )\n    }\n}\n</code></pre>"},{"location":"examples/Weave/#run-the-agent-and-view-traces-in-weave","title":"Run the agent and view traces in Weave","text":"<p>Execute a simple prompt. After completion, open the printed link to view the trace in Weave. You should see spans for the agent\u2019s run, model calls, and other instrumented operations.</p> <pre><code>import kotlinx.coroutines.runBlocking\n\nprintln(\"Running agent with Weave tracing\")\n\nrunBlocking {\n    val result = agent.run(\"Tell me a joke about programming\")\n    \"Result: $result\\nSee traces on https://wandb.ai/$entity/$projectName/weave/traces\"\n}\n</code></pre>"},{"location":"examples/Weave/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If you don't see traces, verify <code>WEAVE_API_KEY</code>, <code>WEAVE_ENTITY</code>, and <code>WEAVE_PROJECT_NAME</code> are set in your environment.</li> <li>Ensure your network allows outbound HTTPS to Weave's OTLP endpoint.</li> <li>Confirm your OpenAI key is valid and the selected model is accessible from your account.</li> </ul>"}]}