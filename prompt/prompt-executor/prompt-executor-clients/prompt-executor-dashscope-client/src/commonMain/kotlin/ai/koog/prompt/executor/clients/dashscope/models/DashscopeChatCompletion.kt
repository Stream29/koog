package ai.koog.prompt.executor.clients.dashscope.models

import ai.koog.prompt.executor.clients.openai.models.CompletionTokensDetails
import ai.koog.prompt.executor.clients.openai.models.OpenAIBaseLLMRequest
import ai.koog.prompt.executor.clients.openai.models.OpenAIBaseLLMResponse
import ai.koog.prompt.executor.clients.openai.models.OpenAIBaseLLMStreamResponse
import ai.koog.prompt.executor.clients.openai.models.OpenAIChoice
import ai.koog.prompt.executor.clients.openai.models.OpenAIMessage
import ai.koog.prompt.executor.clients.openai.models.OpenAIResponseFormat
import ai.koog.prompt.executor.clients.openai.models.OpenAIStreamChoice
import ai.koog.prompt.executor.clients.openai.models.OpenAIStreamOptions
import ai.koog.prompt.executor.clients.openai.models.OpenAITool
import ai.koog.prompt.executor.clients.openai.models.OpenAIToolChoice
import kotlinx.serialization.SerialName
import kotlinx.serialization.Serializable

/**
 * DashScope Chat Completions API Request using OpenAI-compatible format
 *
 * @property messages A list of messages comprising the conversation so far.
 * @property model ID of the model to use (e.g., qwen-turbo, qwen-plus, qwen-max, qwen-long).
 * @property stream If set, partial message deltas will be sent.
 * @property temperature What sampling temperature to use, between 0 and 2.
 * @property tools A list of tools the model may call.
 * @property toolChoice Controls which (if any) tool is called by the model.
 * @property topP An alternative to sampling with temperature, called nucleus sampling.
 * @property maxTokens The maximum number of tokens that can be generated.
 * @property frequencyPenalty Number between -2.0 and 2.0. Penalizes new tokens based on frequency.
 * @property presencePenalty Number between -2.0 and 2.0. Penalizes new tokens based on presence.
 * @property responseFormat An object specifying the format that the model must output.
 * @property stop Up to 16 sequences where the API will stop generating further tokens.
 * @property streamOptions Options for streaming response. Only set when stream is true.
 */
@Serializable
internal class DashscopeChatCompletionRequest(
    val messages: List<OpenAIMessage>,
    override val model: String,
    val enableThinking: Boolean? = null,
    override val stream: Boolean? = null,
    override val temperature: Double? = null,
    val tools: List<OpenAITool>? = null,
    val toolChoice: OpenAIToolChoice? = null,
    override val topP: Double? = null,
    override val topLogprobs: Int? = null,
    val maxTokens: Int? = null,
    val frequencyPenalty: Double? = null,
    val presencePenalty: Double? = null,
    val responseFormat: OpenAIResponseFormat? = null,
    val stop: List<String>? = null,
    val streamOptions: OpenAIStreamOptions? = null,
    val parallelToolCalls: Boolean? = null,
    val enableSearch: Boolean? = null,
) : OpenAIBaseLLMRequest

/**
 * DashScope Chat Completion Response using OpenAI-compatible format
 */
@Serializable
public class DashscopeChatCompletionResponse(
    public val choices: List<OpenAIChoice>,
    override val created: Long,
    override val id: String,
    override val model: String,
    @SerialName("object")
    public val objectType: String = "chat.completion",
    public val usage: DashscopeUsage? = null,
) : OpenAIBaseLLMResponse

/**
 * DashScope Chat Completion Streaming Response using OpenAI-compatible format
 */
@Serializable
public class DashscopeChatCompletionStreamResponse(
    public val choices: List<OpenAIStreamChoice>,
    override val created: Long,
    override val id: String,
    override val model: String,
    @SerialName("object")
    public val objectType: String = "chat.completion.chunk",
    public val usage: DashscopeUsage? = null,
) : OpenAIBaseLLMStreamResponse

/**
 * @property completionTokens Number of tokens in the generated completion.
 * @property promptTokens Number of tokens in the prompt.
 * @property totalTokens Total number of tokens used in the request (prompt + completion).
 * @property completionTokensDetails Breakdown of tokens used in a completion.
 * @property promptTokensDetails Breakdown of tokens used in the prompt.
 */
@Serializable
public class DashscopeUsage(
    public val promptTokens: Int? = null,
    public val completionTokens: Int? = null,
    public val totalTokens: Int,
    public val completionTokensDetails: CompletionTokensDetails? = null,
    public val promptTokensDetails: PromptTokensDetails? = null
)

/**
 * @property audioTokens Audio input tokens generated by the model.
 * @property cachedTokens Cached tokens present in the prompt.
 */
@Serializable
public class PromptTokensDetails(
    public val audioTokens: Int? = null,
    public val cachedTokens: Int,
)
